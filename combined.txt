Overview
Andy Haldane looks at the impact Covid-19 has had on the way people work in the UK. And he considers what that might mean for people’s wellbeing and the health of the economy.

Speech
Introduction
I am delighted to be speaking at this “Engaging Business” Summit, at such a critical time for business, for workers and for the wider economy. The focus of today, and the excellent background report, is happiness in the workplace. This is an issue in which everyone has a stake. It is particularly pertinent with many people having had to adapt their ways of working as a result of the Covid crisis. Indeed, this year may well have seen the largest shift in working practices ever seen, certainly the largest in modern times.

That begs a host of questions about the impact of these changes in working practices on workers, businesses, communities and the wider economy. For economists like me, it raises questions about the impact on productivity and output in the workplace. As arid as these concepts can sometimes sound, they are crucial for shaping how this crisis will affect incomes and living standards over the medium-term.

Equally important are issues of well-being, not least given understandable concerns about how the virus and lockdown are affecting our mental health. The background report for this event is timely in providing some early answers to these questions.footnote[1] Taken at face value, its conclusions are encouraging. Workplace happiness is, in general, higher and many are feeling a greater sense of workplace empowerment.

At the same time, it is too early to be reaching definitive conclusions on what the long-term effects of these seismic shifts in how we work will be. It is also crucial to recognise that these changes have affected individuals in very different ways. For many frontline workers - from health and social care, to public transport and police – home-working has simply not been an option. Those jobs, and many others like them, have become both harder and more hazardous as a result of the Covid crisis.

The report captures some of those important distributional differences, with happiness lowest among young people, black people, females and those in the worst-affected sectors whose jobs and incomes are most at risk. Even for those who are currently content home-working, there is a question about whether the benefits will persist. Some of the potential costs of home-working, including the loss of social engagement, are only now being felt and may grow with time, in ways which affect both our well-being and productivity at work.

In what follows, I will try to navigate through some of these issues, drawing on evidence where possible. There are both positives and negatives from the shift in working practices that has taken place this year and the balance of these is likely itself to shift over time. If you’ll forgive the indulgence, I’ll try and weave in some of my own work experiences to add some personal colour.

The Changing World of Work
Even before the Covid crisis struck, there was evidence of a secular shift towards more flexible forms of working. Analysis of working trends by the Association of Professional Staffing Companies in 2019 found that, over the past two decades, the number of people working ‘flexible hours’ has increased five-fold, from less than 10% to more than half the workforce.footnote[2]

As with many other things, the Covid crisis brought about an overnight transformation and acceleration of those trends. ONS data suggests that, prior to the pandemic, around 5% of people worked from home as their main location. Around 12% had worked from home in the previous week and a little more than a quarter said they had worked from home at some point in the recent past.footnote[3]

At the peak of lockdown in April, almost half of the workforce was working from home in any given week, either exclusively or partially. Over the summer that fraction began falling as restrictions were loosened, before beginning to rise again gradually following government announcements in September. The fraction of the workforce home-working currently stands at around a third, multiples of its pre-Covid level.

Of course, these averages obscure some sharp differences across sectors and occupations. Even before the pandemic, there was a high incidence of home-working in sectors such as information and communication (over 50%), professional and scientific (around 45%) and real estate and finance (about 40%). During lockdown, all of these sectors headed towards a 100% model of home-working.

That was not the case, either pre or post-Covid, among a range of other occupations representing between half and two-thirds of the workforce. Many of those were key workers, including in health and social care, dealing with the effects of the pandemic. In this respect, Covid has not only accelerated pre-existing trends towards home-working; it has also widened pre-existing occupational divides between those who can and cannot exercise the option of home-working.footnote[4]

Surveys of workers and businesses suggest increased home-working is likely to persist, albeit not on the same scale. Around a fifth of businesses say they intend using home-working as a permanent business model. Interestingly, the main reason cited is improved staff well-being. Among workers the picture is much the same, with surveys suggesting more than a quarter expect to spend more time home-working after the pandemic has abated.

My own experience since March has mirrored trends in the wider economy. I am one of the lucky ones who has been able to work from home, as have virtually all other Bank of England staff. I have been back into the office only twice in the past six months. Full-time home-working has, for me, been a radical shift. For the past 30, my working week has been 5-0, office versus home. Nonetheless, like many others, if you asked me how my future working week might look, I think it unlikely I will revert back to the 5-0 model.

The Effects on Productivity and Output
Given these shifts, some of which seem likely to prove durable, what impact might they have had on workers’ and businesses’ economic contribution – that is to say, their productivity (the amount done per hour worked) and their overall output (productivity multiplied by working hours)? In short, how has this shift in working practices affected working capacity of the economy? A number of empirical studies have looked at the effects of home working on productivity.footnote[5]

Pre-Covid, the lion’s share of these studies suggested home-working improved productivity. As one example, Bloom et al (2015) conducted a randomised control trial in a call centre of a Chinese travel company, randomly allocating some workers to work from home and others to work in the office, before measuring their respective performance. Home-working improved worker productivity by around 13% - almost a day’s extra output, a massive gain. Interestingly, after the trial the company allowed everyone who wanted to work from home to do so. That improved company performance by 22%.

These productivity effects do not appear to be fixed across different demographics, firms, tasks and sectors.footnote[6] For example, studies suggest productivity gains from home-working are largest for creative tasks, perhaps reflecting the benefits of a quieter, less distracting home-working environment.footnote[7] It is also worth saying that some of the evidence of productivity gains may be biased upwards by self-selection, with those opting to work from home more likely to be able to do so productively.

When it comes to the productivity effects of home-working during the Covid crisis, the evidence – while thinner – points in a different direction. Survey evidence for Japan suggests around a 7% hit to labour productivity from home-working.footnote[8] Similar evidence can be found in studies of the UK.footnote[9] Survey data from the ONS paints a similar picture, with almost a quarter of workers believing their productivity has been negatively affected by home-working, compared with only 12% saying it has improved.footnote[10]

These differences in the productivity effects of home-working, pre and post-Covid, are perhaps unsurprising. Mandatory home-working thrust large numbers of workers into an alien working environment – their kitchens, bedrooms and attics. There was no option of self-selecting. That meant, for many, not only a worse working environment but a steep learning curve as they adapted to new ways of working. Productivity, predictably, was hardest hit among those with least prior experience of home-working.footnote[11]

At the same time, we should be cautious about jumping to too negative a conclusion when assessing the economic costs of the shift to home-working. Even if the amount workers produce each hour has fallen, as evidence and anecdote tentatively suggests, this need not imply workers’ overall economic contribution has fallen. Compensating factors may have been at work, one of which is longer working hours.

One of the clear benefits of home-working is reduced commuting time, some of which might have been allocated to work. Studies point to daily savings in commuting time of almost an hour. They also suggest around a third of that saved time has been spent working.footnote[12] Assuming an eight-hour working day, that represents a 4% increase in working hours.

Studies of the length of the working day – measured by the time between the first and last email being sent – point to a larger effect still, with close to an 8% rise in working hours as a result of home-working.footnote[13] If that is roughly right, this suggests most workers’ overall economic contribution may not have been much affected by the switch to home-working, with extra hours compensating for reduced productivity.

Another relevant factor is that some of initial losses in productivity from home-working might be expected to lessen over time: as the kitchen chair is replaced by an office chair (office equipment sales have soared); as the video-conferencing facilities become less daunting to navigate; and as children return to school. Consistent with the last of those, there is evidence both of women assuming more of the childcare responsibilities during lockdown and of their productivity being harder hit.footnote[14]

Indeed, one of the beneficial long-term side-effects of home-working is that it has brought about a forced improvement in both our digital kit and our digital skills. Old dogs, like me, have been required to learn new tricks. Prior to the Covid crisis, an estimated extra 5 million people in the UK were expected to have a digital skills deficit by 2030.footnote[15] The switch to home-working is likely to have made inroads into this deficit.

This means there are good grounds for thinking any hit to productivity from home-working may lessen over time and could plausibly even reverse sign. Studies prior to the Covid crisis estimated that between a third and a half of the tasks we do in the office could effectively be done at home.footnote[16] With between a third and a half of the workforce home-working, the Covid crisis has taken us to that point, involuntarily, at warp speed.

These research findings broadly chime with my own experience. I do not know whether working from home has affected my productivity, which is never easy to measure at the best of times. Early on, as I juggled new ways of working and home-schooling, my personal productivity probably suffered. For what it is worth, self-reported surveys of Bank of England staff suggest, overall, they believe their productivity has not been much affected by home-working.

Not commuting has given me back two hours of my day, a chunk of which I spend working to offset the time spent answering the door for Amazon deliveries. Like many others, I also felt a productivity surge when the kids went back to school, although in my case that might well have just been relief. As for my digital skills, they have improved to the point where I am now (almost) competent on around 10 different video-conferencing platforms, although I am aware I am tempting fate by even mentioning that.

The Effects on Well-Being and Happiness
Productivity and output are an objective measure of the economic contribution of workers. These measures need not, however, map into subjective measures of the well-being felt by workers. Well-being matters, both in its own right but also because there is a fairly well-established link between peoples’ sense of satisfaction and their productivity in the workplace.footnote[17] Happier people tend also to be more productive.

The survey study for this Summit is timely in speaking directly to this well-being question. On the face of it, its conclusions are rather encouraging. Many people appear to be happier, indeed materially happier, from having shifted to home-working. They also feel more empowered in the workplace. The result is that, when surveyed, most seem to prefer working at home to working in the office.

There are some important caveats in saying all that. These benefits are clearly not evenly spread, even for those who can work from home. In general, they have been felt less by workers than by their managers; less by women than by men; less by Black people than by White or Asian people; and less by people working in sectors worst-affected by the Covid crisis – for example, retail and hospitality. In this respect, these results echo other findings about the highly uneven economic effects of the Covid crisis.footnote[18]

Despite these important qualifications, I was pleasantly surprised by the survey findings. If home-working has led to longer working hours for many people, you might have expected that to have dented well-being, not to have raised it. The WorkL survey also points to rising levels of workplace anxiety since Covid, understandably so, which would also be expected to harm happiness. Yet happiness at work seems, despite all that, to have risen. What is going on?

Weighing against these effects appear to be at least two key factors: commuting and empowerment. In well-being studies, commuting ranks just after death and divorce in the unhappiness stakes. A study by Chatterjee and Clarke (2017) showed that an additional 20 minute commute reduced people’s well-being as much as a 19% pay cut. On that basis, home-working would be expected to have boosted well-being, with losses on the swings of increased working hours more than made up for on the roundabouts of reduced time spent commuting.

The second potentially potent effect, brought out in today’s study, is improved worker empowerment. This could arise in part from the increased flex home-working gives us, with our working days now tailored more to our personal needs and less to organisational requirements. It might also reflect on increased ability to “be ourselves” at work, when work is at home – the pyjamas, the unkempt hair, the Bermuda shorts - none of which, I hasten to add, I am sporting today.

Well-being studies suggest these agency and empowerment effects are important for well-being. For example, studies have found that working from home is positively associated with perceived autonomy and this, in turn, has positive spillover effects to our job satisfaction and levels of stress.footnote[19] Both boost workplace productivity over the medium term.

If home-working is broadly neutral in its impact on our economic contribution, but positive for our welfare, this suggests it is win-win, or at worst draw-win. The Covid crisis may inadvertently have fast-forwarded us to a better-way-of-working, perhaps even back-to-a-future where most work takes place in and around the house rather than in an office or factory, as was the case prior to the Industrial Revolution.

I would hesitate before jumping to that conclusion. Just as the short-term productivity cost of an abrupt shift to home-working might be overstated, so too might the short-term happiness benefits. In particular, let me discuss two longer-term consequences of the shift to home-working whose effects may not yet be fully visible but which could, over time, detract from both our productivity and our happiness at work.

The Effects on Creativity and Relationships
These two factors are working relationships (or what economists sometimes call social capital) and workplace creativity (or what economists call intellectual capital). Modern economic theory, and a growing body of empirical evidence, gives both factors a prominent role in explaining why individuals thrive, economies grow and societies succeed.footnote[20]

It is well-established that creativity fosters innovation and that in turn fuels economic growth. That was the story of the Industrial Revolution. What is new is our understanding of how the work environment plays a crucial role in nurturing creativity.footnote[21] For example, the absence of distraction and noise is one important factor fostering creativity. Being in “flow” requires a degree of cognitive tunnelling.footnote[22] For many people, home-working provides that quieter, less distracting environment where tunnelling is easier and creativity fostered.

But lack of distraction and noise is not always and everywhere a good thing, including the creativity. It is also well-established that exposure to new and different experiences - sounds, smells, environments, ideas, people – is a key source of creative spark. These external stimuli are fuel for our imaginations and the imagined, made real, is what we typically mean by creativity.

To give one example, walking has been found in studies to be positive for creativity, by freeing our minds from its office constraints and exposing our senses to different stimuli.footnote[23] So too are random conversations and chance meetings, serendipity being the cradle of creativity. Research shows that face-to-face meetings, like serendipity, also foster ideas and innovation, as does music and other of the creative arts.footnote[24]

The key point here is that home-working can starve us of many of these creative raw ingredients - the chance conversation, the new person or idea or environment. Home-working means serendipity is supplanted by scheduling, face-to-face by Zoom-to-Zoom. What creativity is gained in improved tunnelling is lost in the darkness of the tunnel itself. I imagine some people will have used lockdown to write that creative novel they always knew was in them. I doubt many will become modern-day classics.

For me personally, a large part of my job pre-Covid was spent meeting as wide a range of people as possible in as wide a range of settings as possible - companies, charities, community and faith groups, from all walks of life and right around the UK. It also meant me bringing into the Bank people from very different backgrounds – musicians, sport-stars, poets, authors, philosophers. As well as giving me a different window on the world, these experiences were fuel for the imagination.

That links to the second factor – social capital, the relationships we grow as individuals. “Grow” is the right word here because these working relationships, like any relationship, need to be fed and watered. Remote working inhibits our ability to cultivate and grow these working relationships. It has meant social capital has been another casualty of the crisis, with existing capital run down and new capital not built to replace it.

Virtual meetings can be an efficient way of getting things done, indeed often more effective than the physical alternative. The Monetary Policy Committee (MPC) on which I sit and which sets interest rates has taken place virtually since March, having never taken place virtually in the more than 20 years prior to March. MPC deliberations have been no less effective, and MPC decisions no less expeditious, for this shift.

What virtual meetings risk losing, however, is the capacity to explore uncharted territory, to share tacit knowledge and personal information. Those informal between-meetings conversations are, in my experience, the bedrock of relationship-building and the key to trust–building. It is the loss of those informal moments that has resulted in many of us running down our past stock of social capital for the past six months. This cannot be done indefinitely.

I always knew that I picked up a lot of information from the unscheduled time between meetings, when informal and sometimes chance conversations take place. Having now lived without them for 6 months, I now realise these informal non-meetings were often my main source of information. The informal chat in the 5 minute walk from the lift to my office often contained more useful knowledge than the subsequent one-hour meeting in my office.

The other week I spoke to the Bank’s new crop of graduates. When I was in their shoes, almost all of my knowledge came from informal conversations in the pub with my graduate cohort on their jobs, bosses and experiences, rather than from listening to talking heads like me. This year all of that is lost and, with it, a significant down-payment of social capital. Those losses are being replicated among organisations right around the world. This social capital, once lost, will be difficult to reacquire.

Whether it is creative sparks being dampened, existing social capital being depleted or new social capital being lost, these are real costs and costs which would be expected to grow, silently but steadily, over time. They weigh on the other side of the ledger when it comes to assessing the case for home-working. They cast doubt on whether it will lead to the promised land of improved productivity and greater happiness.

If you asked me if I am happier working from home, I genuinely would not know. I do not miss the commute. But I feel acutely the loss of working relationships and external stimuli – the chance conversations, listening to very different people with very different lived experiences, the exposure to new ideas and experiences. These losses will grow with time. At some point, they will offset the benefits of avoiding South-West trains.

Although it is hard to measure, I feel home-working probably has reduced my capacity for creative thought. No modern-day classic novels lie unfinished in my office drawer – or at least not ones penned by me. And the past six months have certainly depleted my social capital: I do not feel I know anyone at work better than six months ago and most a little less well. Everyone I know would I think say the same.

Conclusion
Let me conclude. Covid has re-shaped our working lives, our economic contributions and our well-being, certainly in the short-term but probably in the longer-term too. Whether this change is for the better is one of the key questions of our time, as workers, businesses, policymakers and citizens.

The evidence so far on these issues cannot at this stage be more than illustrative. There is a balance to be struck between events which distract and events which fire the imagination. For me, the 0-5 model of home-working strikes this balance in the wrong place, as with hindsight did my pre-pandemic 5-0 model.

Thirty years of hurt, never
stopped me dreaming -
speech by Andy Haldane
Given at the Institute for Government
Page 1
Published on 30 June 2021
On his last day as our Chief Economist, Andy Haldane shares his experience of
working at the Bank of England for over 30 years.
He covers inflation, the stability of the UK’s financial system, and how we
communicate with the public.
Speech
At the end of September I leave the Bank of England, 32 years almost to the day since I joined.
Most would say that is a decent stint, but in Bank terms it is just really getting started. My personal
assistant, the brilliant Sandra Mills, has just completed 44 years. The Bank’s company secretary,
John Footman, is just about to clock 52 years. I never was a completer-finisher.
It has not, of course, been 30 years of hurt.[1] I have loved almost all of my time at the Bank. I
promised myself one thing when I joined - that I would only stay as long as it was interesting. It has
been interesting for 32 years. Events made it so. And it remains no less interesting now. I tell the
Bank’s new entrants I can promise them only one thing: they will be telling their families and friends
about this moment in history and, uniquely, they can write its next chapter. As a public servant, this
is as good as it gets.
Any lengthy career in public policy will inevitably be punctuated by crisis. In my case, crises have
provided not just the punctuation marks, but most of the words, sentences and paragraphs too. In
public policy, crises are the ultimate learning experience. They are also the moments when the
Overton window of opportunity is widest and the opportunity for change greatest. Crises are
moments of challenge and opportunity in equal measure. I am very fortunate to have experienced
plenty of both.
My time of the Bank has been evenly split between its twin statutory functions, monetary and
financial stability. These functions, embedded in the Bank’s Royal Charter in 1694, remain its
statutory centrepiece today. (The Bank’s third objective, fighting the French, has by contrast
tended to be downplayed.) Over the intervening 327 years, both monetary and financial stability
have had their fair share of challenges and opportunities. These have perhaps come thicker and
faster over the past 27 years than the preceding 300.
I want to offer a few retrospective thoughts on the evolution of monetary and financial stability over
the past 30 years before turning to central bank communications, a crucial ingredient of both.
Bank of England policy frameworks and practices have undergone an astonishing transformation
Page 2
over that period. I do not think it is an exaggeration to say there has been a revolution in how the
Bank goes about securing monetary and financial stability and how it communicates about both.
The catalyst for this revolution has been crisis.
Having discussed this historical evolution-cum-revolution, I discuss some of the key issues facing
central banks today. This is the “dreaming” bit - looking around corners to judge not only what is
coming but how to reshape it, seeking out the biggest issues not just of today but tomorrow. It is
the Wayne Gretsky approach to public policy - skating to where the puck is going, not where it is.[2]
That can be unconventional and sometimes misunderstood. But it is, for me, the essence of
effective policymaking.
Imagining a different future is not sufficient for policy success. It is the imagined made real that
matters. The Bank is blessed with having the capacity to both think and do, both brain and hands.
When former Governor Cobbold said “the Bank is a bank and not a study group” he was wrong.
The Bank is both. And the magic happens when the two are combined, the brains and hands coordinated. Nothing illustrates this better than the revolution in the UK’s monetary and financial
stability frameworks during my tenure.
Monetary policy
I joined the Economics Division of the Bank in 1989, following the well-trodden path from
Sunderland council estate to Threadneedle Street. I hoped to redesign the UK’s monetary policy
framework. My first task fell a fraction short of those ambitions. It was to forecast the non-resident
(“externals”) component of the asset counterparts to M4 (a measure of the money supply). Like the
Schleswig-Holstein problem, only three people understood the external counterparts to M4. One
was dead, the other mad and the third was not me.
The external counterparts of M4 are as close to a random walk as any time-series on the planet.
That makes forecasting them a mug’s game. I was that mug. Almost as thankless was the sixmonthly forecasting exercise we undertook at the time. This involved every economist forecasting
a component of the National Accounts in microscopic detail. My job was to forecast the Interest,
Profits and Dividends component of the UK current account, another lofty task, another random
walk, another game of mugs.
At the end of this exhaustive process, the forecasts were sent around the Bank, as well as to HM
Treasury.
There, I have it on good authority, they quickly became landfill (as recycling wasn’t an option at the
time). Like the UK’s entry at Eurovision, the Bank economists’ contribution was spirited but
ultimately pointless. The Bank’s analytical brain did not connect to any hands. John Kenneth
Galbraith said that economics was extremely useful as a form of employment for economists. At
the time, that was the Bank’s view too.
Page 3
The Bank’s forecasting process was a fitting metaphor for the UK’s monetary policy experience at
that point. From the early 1970s onwards, many monetary policy frameworks had been tried. All of
them had ended up in the wheelie bin. In the late 1980s, the UK had no clearly defined nominal
anchor for monetary policy at all. The best predictor of interest rate movements was not GDP or
inflation. It was whether Mrs Thatcher (the then-Prime Minister) had recently suffered a bad byelection result. Policy played second fiddle to politics.
At the point I joined, the search for another new nominal anchor for the UK was well underway. One
of my early tasks was to become an expert on the European Exchange Rate Mechanism (ERM), a
framework that was seen as offering a route to monetary redemption for the UK, effectively by
outsourcing monetary policy to Germany’s Bundesbank. Within a year, the UK had joined the
ERM. And two years later - Black Wednesday, 16 September 1992 - it was forcibly, and
ingloriously, ejected.
That day is etched on my memory. I had the incredible good fortune to be sitting on the Bank’s
foreign exchange dealing desk that day, watching agog as we lost around £20 billion in foreign
exchange reserves defending the pound – at the time, real money – despite announcing interest
rate rises of 5 percentage points in a single day. How the world has changed. Currently, financial
markets expect UK interest rates to rise by an average 5 basis points each six months for the
next 10 years.
The UK’s exit from the ERM led to a new nominal anchor being needed. And almost immediately,
one was adopted – an inflation target. It had one obvious merit: it was the only monetary
framework not to have already been tried in the UK. But the track record of inflation-targeting was
close to non-existent. At the point the new target was announced, expectations for UK inflation
were high (over 5%) and expectations for the framework lasting were low. The collapse in sterling
following sterling’s ERM exit, and the expected sharp rise in inflation, meant the wheelie bin
beckoned for inflation-targeting.
In the event, inflation failed to pick up as much as expected after the ERM debacle. And, behind
the scenes at the Bank, the machinery of monetary policy was changing. Data, analysis and
models suddenly became more central to judgements on inflation and the appropriate monetary
stance. Accompanying this, economics and economists began playing a more central role in
formulating the Bank’s judgements.
Although decisions on interest rates still resided with politicians, the Bank now had a betterinformed voice.
That voice became louder as a result of two great leaps forward in monetary policy transparency:
the publication by the Bank, from 1993 onwards, of a quarterly Inflation Report; and the publication
of monthly minutes of the meetings between the Chancellor and Governor at which monetary
policy was decided – the “Ken and Eddie Show”. Both put the Bank’s analysis and judgements on
Page 4
the economy and monetary policy in the public domain, for the first time ever.
This did not give the Bank a vote on monetary policy, but did give it a public voice. That voice
became increasingly influential in shaping external debate on policy through the 1990s,
constraining somewhat the Chancellor’s hand. It also re-shaped the Bank’s own processes, which
became more rigorous and resource-intensive. Transparency plus a clear target imposed
discipline on the Bank as well as the Chancellor. Whereas before Bank forecasts went into the
bin, now they went into the quarterly Inflation Report.
During this time, I was lucky enough to work on the issue I had joined the Bank to pursue – design
of the UK’s monetary framework, inflation-targeting. Having been introduced in haste, there were a
wide range of design issues to be researched and agreed. To that end, the Bank hosted an
international conference of central banks in 1995, the fruits of which became the first (and my first)
book on inflation-targeting.[3] Despite being the first of its kind, the book was not an instant bestseller. Nor was it subsequently.
With hindsight, most of the important scaffolding of inflation-targeting, its design and processes,
was erected in the early-to-mid-1990s. As much as what followed, this was when the UK’s new
monetary policy framework was forged for good. The Bank’s analytical brain was being rewired to
connect with its operational arms. And the outside world was beginning to take notice. Mediumterm inflation expectations in the UK began to edge down, as policy credibility grew.
As the Bank’s voice on monetary policy grew louder, so too did debate on taking the next step -
granting the Bank formal independence for the setting of monetary policy, vote as well as voice. In
1995, on reflection rather presumptuously, I wrote a paper for a Bank of Japan conference called
“Independence and Accountability”, making the case for Bank independence and greater degrees
of accountability. It was written with the Bank’s Chief Economist at the time, Mervyn King.[4] I often
wonder what Mervyn did next.
In 1997, as its first act, the incoming Labour Government took the momentous step of granting the
Bank operational independence for monetary policy. At a stroke, politics was taken out of policy.
The Bank’s brain and hands were now fully and durably connected in statute. UK inflation
expectations ratcheted down by a further 50 basis points on announcement, as credibility
stepped-up.[5] The Monetary Policy Committee (MPC) was soon formed and met for the first time
in June 1997.
Internally, a new machine and procedures were developed to service and support the MPC. The
centrepiece of this support was a meeting called “Pre-MPC”. This was a full day of briefing
presentations by Bank staff to the MPC. It is, and remains, the closest you will ever get to
economic theatre at the Bank of England. Bank Staff sit on one side of the amphitheatre armed
with Powerpoint presentations, the MPC sit on the other armed with tricky questions.
As luck would have it, I gave the first-ever pre-MPC presentation at the first-ever pre-MPC. The
Page 5
room was rammed with banks of screens, crowds of people and clouds of cigarette smoke –
three of the MPC were smokers, including the Chair Eddie George. I was a bag of nerves. Eddie
gave me the nod, I pressed the button on my PC to start the presentation – and all the screens
went blank and an ear-piecing noise let out around the room, the type of which instantly causes
teeth to grind.
With my bag of nerves now full to overflowing, I peered through the fug of Rothmans to see Eddie
staring back at me. Both eyebrows were raised. This was bad news. In central bank circles at the
time, the double eyebrows was career-defining for all of the wrong reasons. Most of my
subsequent 25-years has been managing that decline as gracefully as possible.
Fifteen year later, I returned to pre-MPC on the other side of the table. There was now no cigarette
smoke, but otherwise the format of structured presentations remained the same. Indeed, most of
the main MPC processes remain much the same as at inception. That, I would say, is a sign of
success. While MPC procedures have evolved for the better, and the standard of presentations I
now receive is far-higher than those I used to give, the foundations of the MPC have remained
essentially unmoved.
It is no coincidence that durability in monetary policy processes has been accompanied by
improved
macro-economic outcomes. Since 1992, inflation has averaged 2% - exactly in line with target (to
one decimal place) and 6 percentage points lower than in the preceding 25 years. The volatility of
output has fallen by around half over the same period.[6] Contrary to everyone’s expectations,
inflation-targeting has lasted and delivered a twin-win, with greater stability on both the nominal
and real sides of the economy.[7] Given the UK’s previously chequered monetary history, this truly
is a transformation.
Of course, there is a question about how much of this improved performance reflects good luck
rather than good monetary management. In the years running up to the Global Financial Crisis, a
period known as the Great Moderation, these explanations were difficult to disentangle.[8] Shocks
to the economy were modest and unthreatening over this period, at least relative to earlier
episodes. Governor Mervyn King called this the NICE era – Non Inflationary Continuous
Expansion.[9]
But the macro-economic period since 2007 has been naughty, not NICE. The past 15 years have
presented as many macro-economic challenges as any in recent history: first the collapse of the
global economy associated with the Global Financial Crisis; then its localised after-shock, the
Euro-area crisis; then the run-up to, and aftermath of, the Brexit referendum; and most recently, of
course, the Covid crisis. We have gone from a NICE era to a VILE one – Volatile Inflation Limited
Expansion.
Yet despite those challenges, UK inflation expectations have remained anchored to the 2%
Page 6
inflation target. In the 1970s, 1980s and 1990s, volatility in near-term inflation and output was
mirrored in medium-term inflation expectations, as credibility swung with the wind. Inflationtargeting has broken decisively that link. That nominal anchoring, courtesy of inflation-targeting,
has in turn eliminated one crucial source of past macro-economic instability in the UK.
And so to the present day. While the essentials of inflation-targeting are as strong as ever, the
regime is being tested as never before. The Covid crisis has seen interest rates fall to their
effective lower bound (ELB) and Quantitative Easing (QE) restarted on a scale that may yet match
the decade-long period after the Global Financial Crisis. By the end of this year, the Bank will hold
close to £1 trillion of nominal Government assets, around half of their total and 40% of GDP. Other
central banks find themselves in a similar spot.
Truth be told, this is for me an uncomfortable spot. It is uncomfortable for two distinct but related
reasons. The first, nearer-term, is discomfort at whether continuing monetary stimulus is consistent
with central banks hitting their inflation targets on a sustainable basis. A second, more mediumterm, discomfort is whether the monetary policy strategies being pursued by central banks are at
risk of time-inconsistency, fiscal dominance and an erosion of central bank independence.
The restrictions imposed as a result of the Covid crisis caused an extra-ordinarily large and sharp
contraction in activity, almost without precedent. As these restrictions are lifted, we would expect
an equally large and sharp recovery. That bounce-back is now well underway in the UK. Indeed,
current data suggest it is occurring faster and sooner than expected, with the economy already
within statistical spitting distance of its pre-Covid level. This rapid bounce-back has been quasiautomatic, as restrictions have eased, businesses have reopened and people have returned to
working, shopping and socialising.
But this reflex response is not the only macro-economic force at work. Two further large and
powerful sources of economic energy are also fuelling growth, one public, one private. The public
source is the extra-ordinary degree of additional stimulus that has been provided to the UK and
global economies by monetary and fiscal policy. In the UK, the quantum of additional QE and fiscal
easing both presently stand at around 15-20% of GDP, adding significant further momentum to an
already rapidly bouncing-back economy.
The private source of economic energy comes courtesy of the large pool of involuntary savings
amassed as a result of restrictions on spending. For UK households these amount to over £200
billion and for UK companies around £100 billion. Leakages from this saving lake are already
fuelling spending on goods and assets. And lower than expected unemployment is encouraging
more of these savings to be spent in a virtuous cycle. The size and depth of this saving lake
means it could finance demand at scale for some period to come.
With public and private financial fuel being injected into a macro-economic engine already running
hot, the result could well be macro-economic overheating. When resurgent, and probably
Page 7
persistent, demand bumps up against slowly-emerging, and possibly static, supply, the laws of
economic gravity mean the prices of goods, services and assets tend to rise, at first in a localised
and seemingly temporary fashion, but increasingly in a generalised and persistent fashion.
This we are now seeing, with price surges across a widening array of goods, services and asset
markets. At present, this is showing itself as pockets of excess demand. But as aggregate
excess demand emerges in the second half of the year, I would expect inflation to rise, significantly
and persistently. There are already some signs of this risk being priced in financial markets.
Longer-term financial market measures of UK inflation expectations have picked up to stand
around 30 basis points above levels over the past decade.
There is no evidence so far of inflation expectations, in the UK or elsewhere, becoming durably or
significantly de-anchored from target – for example, among households or businesses. But it is
early days. Overall, inflation expectations and monetary policy credibility feel more fragile at
present than at any time since inflation-targeting was introduced in 1992. Why do I say that?
By the end of this year, I expect UK inflation to be nearer 4% than 3%. This increases the chances
of a high inflation narrative becoming the dominant one, a central expectation rather than a risk. If
that happened, inflation expectations at all maturities would shift upwards, not only in financial
markets but among households and businesses too. We would experience a Minsky Moment for
monetary policy, a taper tantrum without the taper.[10] This would leave monetary policy needing to
play catch-up to re-anchor inflation expectations through materially larger and/or faster interest
rate rises than are currently expected.
Even if this scenario is a risk rather than a central view, it is a risk that is rising fast and which is
best managed ex-ante rather than responded to ex-post. If this risk were to be realised, everyone
would lose – central banks with missed mandates needing to execute an economic hand-brake
turn, businesses and households facing a higher cost of borrowing and living, and governments
facing rising debt-servicing costs. As in the past, avoiding that inflation surprise is one of the
central tasks of central banks.
These near-term inflation concerns also have a bearing on the medium-term risks facing central
banks. After the Global Financial Crisis, central banks went in large and fast with QE, I believe
rightly, to support the economy. They then withdrew that stimulus slowly, if at all, to protect the
fragile recovery – again, I believe rightly. As a result of these actions, central banks’ balance
sheets, including the Bank’s, inflated quickly and never subsequently deflated.
When the Covid crisis struck, central banks again went in large and fast to protect a collapsing
economy, I believe rightly. Balance sheets ratcheted higher. On current expectations, central bank
balance sheets are unlikely to deflate any faster than after the Global Financial Crisis. But this time
that policy script feels stretched. The pace of recovery is significantly faster now than then,
bouncing rather than edging back.
Page 8
More fundamentally, a slow exit risks putting central bank balance sheets on an unsustainable
footing.
Entering fast and large, and exiting slow and small, puts a ratchet into central bank balance
sheets.[11] With large or frequent enough future shocks, this strategy is not time-consistent: either
the stock of Government assets to buy is exhausted or, more likely before that, debt-serving
concerns begin to contaminate perceptions of the future monetary stance. The latter is what
academics call fiscal dominance.[12] An asymmetric QE response function nudges us towards that
fiscal danger zone and adds to concerns about the erosion of monetary policy independence. Or
that, at least, is the risk.
It is these two points, taken together, that lead me to believe that this is the most dangerous
moment inflation-targeting has so far faced. The answer is not to change the regime itself. Indeed,
I can think of few poorer times to do so. In my view it does, however, call for immediate thought,
and action, on unwinding the QE currently being provided, given the state of the economy and
central banks’ balance sheets. The Bank’s on-going review into the process and sequencing of
QE unwind is a welcome opportunity to do so.
A dependency culture around cheap money has emerged over the past decade. Only a minority of
those with mortgages have ever experienced a rise in borrowing costs. Fewer still have significant
inflation in their lived experience. Easy money is always an easier decision than tight money. But
an asymmetric monetary policy reaction function is a recipe for a Minsky mistake. Having followed
the right script on the way in, central banks now need to follow a different script on the way out to
avoid putting 30 years of progress at risk.
Financial Stability
The Bank has been bedevilled by bouts of financial instability since its inception. After I joined,
these continued with the failures of BCCI and Barings. Each met with a ratchet response - more
regulation, more regulators. At root, what these failures illustrated was a structural flaw in policy:
the Bank was being asked to look in two directions at once, as both regulator of, and promoter of,
the City. Both are useful roles. But giving one institution both roles invites failure when, as
inevitably happens, the wrong balance is struck.
Looked at now, firm failures in the 1980s and 1990s, and the cluster of small banks in the 1970s,
were not close to being financial crises of the scale and severity we would recognise today.
Certainly, none of them were systemic in their impact, either for the financial system as a whole or
the wider economy. Contagion was short-lived and limited. And few losses of job or GDP resulted.
The Bank was left with egg on its face, but neither the City nor the economy were scrambled.
That was probably as much by good luck as design. At the time, the macro-monetary and financial
stability arms of the Bank rarely crossed or co-ordinated. The monetary policy side dealt with the
Page 9
economy, at a macro level. The financial stability side dealt with financial firms, at a micro level.
One had a bird’s eye view, the other a worm’s eye view. But the bird and the worm rarely
socialised. The Bank’s macro-economic brain was largely detached from its micro-supervisory
hands.
You needed only to visit the Bank’s restaurant building in the early 1990s to spot this difference.
On the ground floor was both a wine bar and a pub. If you visited the pub on a Friday you would
spot the economists, probably in knitwear. If you visited the wine bar you would see the bank
supervisors, probably in tweed. Beer and wine do not mix and nor did the beer and wine-drinkers.
It was probably just as well there were no systemic crises at the time requiring these two tribes to
co-ordinate their analysis and actions.
The bank failures of the 1980s and 1990s contributed to the decision to strip the Bank of
responsibility for supervising financial firms, soon after monetary policy independence. The two
decisions were linked. Separation, it was said, avoided reputational contamination of monetary
policy from financial firm failure.
This, however, came at one obvious cost: it severed, institutionally, any link between the micro and
macro, the brain and the hands. This would come back to haunt, not just the Bank but the world, a
decade later.
After the separation of the Bank from the Financial Services Authority (FSA), the new financial
services regulator, the Bank found itself with too little international resource. This was unfortunate
because the Asian financial crisis was raging. Two new international divisions were set up to plug
this gap – one on the monetary policy side headed by Andrew Bailey (another whose progress I
have lost track of), the other on the financial stability side headed by me.
I moved to set up the new division on Monday 10 August 1998. The following Monday, Russia
defaulted on its debts and devalued the rouble, triggering another emerging market-cum-global
crisis. The next day I briefed Eddie George on the Russian crisis. Eddie spoke Russian, had
worked in Russia and had an astute working knowledge of the Russian economy. I was in week
two. I wasn’t long into my briefing before eyebrows were raised.
The next few years brought crises aplenty as they spread contagiously through emerging markets.
Spotting these crises, their unique dynamics, the way markets turn, the blend of politics and
economics, is part instinct, part science. The emerging market crises at the time enabled me to
cut my teeth on understanding financial crises. Yet at that time, financial crises were a
phenomenon felt to be confined to emerging markets. Advanced economies, it was thought, knew
better. This, too, would come back to haunt us.
Resolving emerging market crises was one thing, redesigning the international financial system to
forestall them quite another. Working alongside HM Treasury colleagues (including Sir Jon Cunliffe
– another whose path I have lost track of), an intensive programme of international reform began
Page 10
with the aim of preventing a repetition of these emerging market crises. As part of the Bank’s
contribution, under Eddie and Mervyn’s guidance, I worked with the Bank of Canada to produce a
blueprint for international monetary reform.[13]
This blueprint proved controversial, as it proposed restrictions on the IMF’s ability to extend credit
to countries in crisis. That jarred with IMF orthodoxy, including the then-Deputy Managing Director
Stan Fischer, who at the time was proposing the IMF become an international lender of last
resort.[14] It also jarred with US orthodoxy. Under Larry Summers and Tim Geithner, the US
Treasury had overseen the emerging market bailouts from Mexico in 1994 onwards.
In 2002, I was asked by Tim Geithner (now at the IMF) to go to Washington to work on international
financial system design, alongside two former US Treasury colleagues, Nouriel Roubini (now
better known as Dr Doom) and Brad Setser. There were no prizes for spotting the odd one out. My
time at the IMF gave me an insight into the fantastic quality of its staff and the political constraints
they often operated under. As for Tim, I often wonder what he went on to do next.
On return to London I produced another book, on the design of the international financial
system.[15] This too failed to make the bestsellers’ list. In the period since, fault-lines in the
international financial system have continued, periodically, to cause global tremors. They will
continue to do so until the international financial architecture is strengthened. With hindsight, I think
Stan Fischer was right on the need for a better-resourced IMF, alongside the international
equivalent of a resolution authority for nation states. Alas, it may take another bout of emerging
market crises to instigate this change.
Back in London, I was reassigned to lead a Division working on domestic rather than international
financial stability issues. This was something of an anti-climax. The domestic financial system
was, by comparison with the violent seas of emerging markets, a millpond of tranquillity at the
time. Moreover, the prevailing consensus was that this tranquillity was set to last. In response, the
Bank slimmed the resources it devoted to financial stability by around half.
The timing was unfortunate. The financial system was riding a credit wave as banks’ balance
sheets and leverage ballooned. Every major financial crisis in the past has been presaged by
credit waves of this type.[16] But this time, it was said, was different. Risk had been dispersed and
diversified to the four winds, courtesy of an ever-more interconnected global financial system and
ever-more sophisticated financial instruments.[17] The Great Moderation in the economy also
meant finance faced fewer shocks than in the past.
When it came to assessing risks to the financial system, unlike with monetary policy, there was no
off-the-shelf model. In pursuit of one, I started looking to other disciplines for inspiration. From the
mid-2000s, I began discussion with a set of scientists – physicists, evolutionary biologists,
epidemiologists – on the models they used to understand complex, adaptive systems. I was
hoping they might provide some analytical clues when it came to modelling the complex, adaptive
Page 11
world of finance.
I was in luck. There was a well-developed field of complexity science, with applications in most of
the natural sciences and some of the social sciences. Economics and finance was a notable
exception. Once I had retro-fitted these models to the financial system, I wrote a note and sent it to
the Governors in 2005. It was titled “Public Policy in an Era of Super-Systemic Risk”. It made
some bold claims about financial system resilience, most of which jarred with the prevailing
orthodoxy.[18]
Financial integration, it argued, was a double-edged sword. It was fantastic for risk-dispersal
when the good times rolled. But interconnections could switch from friend to foe when shocks
were large. Connectivity then amplified, rather than dispersed, risk; it spread contagion. The more
connected the system’s nodes – the larger the number of “super-spreaders” - the greater this
fragility. This “robust-yet-fragile” property of complex webs struck a cautionary note about the true
stability of modern finance.[19]
When it came to managing systemic risk, complexity science was rich in answers too. In avoiding
fragility, one effective solution was to ring-fence activities, the financial equivalent of fire-breaks, to
contain contagion. A second solution was to focus on inoculating, or risk-proofing, the superspreaders to prevent them serving as a conduit for contagion. And a third was to manage
emergent aggregate risks to the system by explicitly leaning against the risk cycle, moderating its
emerging excesses.
I am still waiting for comments on my 2005 memo. With hindsight, one of my career regrets was
not to make more of the results until it was too late. This framework did, nonetheless, prove useful
after the global financial system went into meltdown in 2008. The robust-yet-fragile property of
modern finance was then laid bare. The double-edged sword of financial integration did then cut
through the financial system. And super-spreaders did suddenly appear on our high streets, as
people queued in the streets for their money.
None of this is to suggest that me or anyone else foresaw the true horror of the Global Financial
Crisis. As best I can tell, no-one got the crisis completely right, despite a number of people
subsequently exhibiting supernatural powers of hindsight. Rather, the crisis illustrated the limits of
our collective knowledge, our collective lack of imagination. It demonstrated that, in a world of
uncertainty as distinct from risk, it is better to be super-safe ex-ante than super-sorry ex-post,
better to be roughly right than precisely wrong.[20]
The latter comes to mind when reflecting on the initial rescue response to the crisis. For me, one
of the decisive moments came early in 2008 when, led by Mervyn King, the Bank judged that the
problems facing the UK and global banking systems were ones of solvency, not illiquidity. Righting
the system meant restoring the solvency of banks through capital injections. This was a big call.
But how much more capital was needed, given the degree of uncertainty (not risk) about the true
Page 12
value of UK banks’ assets?
This was the problem Mervyn set for my Division. One approach to answering it was to assess
capital shortfalls bottom-up, asset by asset. As UK banks had an asset base of £6 trillion, much of
which they themselves could not value, this was practically impossible. In even trying, we would
probably have been precisely wrong. We decided instead to use top-down assessments of
solvency based on market valuations.
That pointed to a capital deficit for UK banks of around £100 billion, well in excess of others’
estimates.
When UK banks were finally recapitalised that Autumn, around £65 billion was injected into them
by the UK Government. Our calculations had been roughly right, good enough to save the ship. To
this day, I believe that if greater amounts had been injected then – perhaps £100 billion? - UK
banks would have been more willing to lend and the recovery would have been less anaemic. It
would have been better to be super-safe ex-ante than super-sorry ex-post.
With the ship stabilised, in the UK and globally, the task now was to make it seaworthy. The
Overton window of opportunity to affect radical regulatory reform was ajar. But what reform?
Having helped explain the dynamics of a complex web during the financial crisis, complexity
science could also help in its redesign. Although these regulatory reforms eventually fell short of
my up-front ambitions in their scale and scope, they nonetheless moved the system to a decisively
better place.
The centrepiece of these banking reforms was so-called Basel 3, overseen by the Basel
Committee on which I sat.[21] The Basel 3 reforms eventually resulted in significant increases in the
amounts of capital banks held; the introduction of an international regime for leverage and
liquidity; a capital surcharge for the world’s “super-spreader” banks; and a system of countercyclical capital regulation to modulate credit cycles. These reforms bore more than a passing
resemblance to the solutions complexity scientists might have proposed.
A new word emerged to capture these reforms - macro-prudential regulation. The macro
signalled two important ideological shifts from the past. First, banking needed to be managed at
the level of the system as a whole, like any other eco-system. Second, as important as the
resilience of the financial system was its interaction with the macro-economy to avoid adverse
feedback effects between the two, such as credit crunches. Finance was to be servant of the
economy, not master. This, truly, was a regulatory revolution.
Not all proposals for international regulatory reform found universal favour. In a paper prepared for
the Jackson Hole central banking shindig in 2012, I questioned whether the very complexity of
financial regulation might have contributed to the increasing fragility of the financial system. As you
did not fight fire with fire, you did not fight financial complexity with regulatory complexity. That
risked making a bad situation worse, a complexity problem squared rather than halved.
Page 13
To be honest, had I not called the paper “The Dog and the Frisbee” I doubt anyone would have
read it. But I did, they did and the result was a sharp intake of collective breath among my
colleagues around the Basel Committee table - and beyond. On the other side of the Atlantic, one
of those drawing breath was the then-Governor of the Bank of Canada, Mark Carney. I am not sure
what Mark went on to do next.
But it was not just internationally where the regulatory reform bandwagon was rolling. In the UK, the
Vickers Commission reforms created a fire-break between banks’ services to the domestic
economy and their other activities. Complexity scientists would have approved. And sweeping
institutional reforms were also underway in the UK, with the FSA’s prudential responsibilities
merged into the Bank and a new consumer protection agency, the Financial Conduct Authority
(FCA), created.
The return of prudential regulation to the Bank was not, fortunately, a reversion to the pre-1997
orthodoxy. The Global Financial Crisis had laid bare the costs of separating finance and the
economy, the micro and macro – a separation that had also been a feature of the Bank in the
past. Crisis needed to be the catalyst for change, forging a link between the Bank’s analytical
brain and its regulatory hands.
And so it was, with the creation of a new policy body, the Financial Policy Committee (FPC). The
FPC was charged with safeguarding systemic risk in the UK using new macro-prudential tools. An
interim FPC was set up in 2011 ahead of a new statutory framework being put in place. And the
interim FPC gave way to a statutory FPC in 2013 when the new Financial Services Act came into
place. I was fortunate enough to be a member of both the interim and inaugural FPC.
As with monetary policy after the ERM debacle, it was fascinating to erect the scaffolding of an
almost entirely new policymaking edifice. Indeed, even less of this scaffolding was in place than in
1992. There were big issues to address about how to operationalise the objectives of macroprudential policy, the transmission of these policies and which were the most effective macroprudential tools.[22] This new macro-prudential building was being constructed from ground zero,
at the same time as being fully occupied.
It has been fascinating, too, to watch the evolution of the FPC in the period since. The FPC has
established for itself a clear and well-defined role, clear and well-defined tools and clear and welldefined transparency and accountability mechanisms, including the six-monthly Financial Stability
Report and periodic stress-tests, conducted alongside the Prudential Regulation Committee
(PRC) – the third leg of the Bank’s policymaking stool. Though they will never (and should never)
be complete, the FPC and PRC are essential additions to the UK’s policymaking skyline.
Crucial for the success of both the FPC and PRC is operational independence of decisionmaking, set in statute. Independence for financial regulation and supervision has received far less
attention, analytically and practically, than on the monetary policy side. But, for me, the case for
Page 14
independence is at least as strong as for monetary policy.[23] If anything, decisions on withdrawing
the punchbowl are harder, and even more important, during raucous credit parties.
Thirty years on, the transformation in the policy-making structures and technologies for financial
stability are every bit as great as those for monetary policy. An entirely new system of macroprudential regulation is now in place, fusing together the micro and macro, the economic and the
financial. Through the FPC and PRC, the Bank’s brain and hands are now synchronous. Beer and
wine now mix just fine and the jumper-with-tweed-jacket combination is the height of policymaking
fashion.
In 2012, the Queen and Prince Philip visited the Bank. My colleague Sujit Kapadia used the
opportunity to answer the Queen’s question soon after the crisis – “Why hadn’t anyone seen it
coming?” Sujit set out the reform steps taken to avoid a repetition. The Royal couple took, I hope,
a degree of reassurance. On the way out, the late Prince Philip turned and said: “Oh, just one last
thing – don’t do it again”. I think the institutional framework now in place gives us a realistic hope
of making good on his request.
More than a decade on from the crisis, the financial system is a fundamentally different animal -
leverage far lower, liquidity far higher.[24] The UK’s largest banks’ activities are protected,
additionally, by a ring-fence and systemic surcharges. While I still doubt big banks can fail safely,
they are far less likely to inflict collateral damage on depositors and the wider financial system. In
all of these respects, the regulatory reform agenda of the past decade has been strikingly
successful.
And the benefits of this have already been felt. During the Covid crisis, the global banking system
has lived up to the expectations set for them by Mark Carney at its start: they have been part of the
solution, not the problem.[25] That is far from saying, however, that the financial stability job is done.
In a complex, adaptive web, it can never be done. So from a potentially long list, let me discuss
two areas of unfinished business: lending to small and medium-sized enterprises (SMEs) and the
future of payments.
In 1929, Hugh Macmillan led a Government Commission into lending to SMEs in the UK. It
included in its ranks John Maynard Keynes and Ernest Bevin.[26] It concluded that many UK SMEs
were unable to access adequate finance, restraining growth and job creation. The “Macmillan
Gaps” were born. In the period since, there are few signs these gaps have closed. Indeed, with the
retreat by UK banks from business lending and many high-streets, there are reasons to think the
Macmillan gaps may have widened.
Those SME financing fault-lines have been clearest at times of financial stress. During the Global
Financial Crisis, many UK SMEs struggled to access bank credit on reasonable terms or, in some
cases, at all.
Constrained credit to companies was, in turn, a potent factor behind the UK’s anaemic
Page 15
subsequent recovery.
These same fault-lines were re-exposed during the Covid crisis. The good news, this time around,
was that large numbers of loans – in excess of one and a half million of them – were made to UK
businesses by UK banks in the space of a few months. The bad news is that the vast majority of
these loans would not have been made at that speed without a 100% guarantee from
Government. Only by effectively nationalising SME lending were the Macmillan gaps bridged in
crisis.
Over the years, several initiatives and institutional fixes have been attempted to close the
Macmillan gaps. These include the British Growth Fund (BGF), the British Business Bank (BBB)
and Innovate UK. While individually helpful, none of these has had either the scale financially, nor
the scope regionally, to close the Macmillan gaps. Fintechs, using new data and new
technologies, have sought to bridge these gaps with some success but, realistically, are likely to
do so only slowly.
To my mind, what is needed to bridge the Macmillan gaps, durably and comprehensively, is the
equivalent of a UK Development Bank, operating on a decentralised basis. As other countries
have found, the scale and scope created by a Development Bank is necessary to reach SME
start-ups and scale-ups across all sectors and all regions. The best time to have put in place a UK
Development Bank would have been 1929. The second best time is now.
Until recently one of the great financial puzzles was that, despite waves of innovation, there was
little evidence of its effects in measures of the efficiency of financial intermediation. At the macro
level, the work of Thomas Phillippon suggests measures of banking efficiency in the UK and US
have flat-lined.[27] At the micro level, this lack of progress was well captured by the late, great Paul
Volcker in 2009 when we suggested the only useful piece of financial innovation over the
preceding 20 years had been the ATM.
Today, the winds of technological change are blowing a gale through finance, led by payments.
New technological barbarians are appearing at the gates, transferring monies cheaper, faster and
easier than ever. Many private sector initiatives are underway currently to develop so-called
“stablecoins” – digital currencies backed by pools of safe assets. And debates are advancing
rapidly too on the possibility of central banks themselves issuing a digital form of cash, so-called
Central Bank Digital Currencies (CBDC).[28]
So far, these debates have tended to focus on the payment benefits of these new technologies,
often led by payment technicians. This is an approach I remember well from working on payments
myself in the 1990s.
Back then, monetary policy and banking stability were often out of scope when designing payment
schemes. At the time, I thought this was a mistake: the supply and distribution of money is central
to monetary policy and banking stability too. We published another book – another resounding
Page 16
failure – making this point.[29]
I see a similar pattern now in the debate over stablecoins and CBDC. Resilience of this new
payments medium is, of course, crucial. But the case for adopting, and the means of designing,
these new payment instruments needs to weigh a richer array of considerations to harvest the full
fruits of this innovation.
Specifically, far greater focus needs I think to be placed on the longer-term monetary and financial
stability benefits of these new monetary technologies, as the Bank’s recent work has argued.[30]
On financial stability, a widely-used digital currency could change the topology of banking
fundamentally. It could result in something akin to narrow banking, with safe, payments-based
activities segregated from banks’ riskier credit-provision activities. In other words, the traditional
model of banking familiar for over 800 years could be disrupted. While the focus of debate so far
has been on the costs of this disruption, largely in the form of disintermediation of existing agents,
there are significant potential benefits to be had too.
Specifically, this could lead to a closer alignment of risk for those institutions, new and old, offering
these services - narrow banking for payments (money backed by safe assets) and limited
purpose banking for lending (risky assets backed by risky liabilities). This radically different
topology, while not costless, would reduce at source the fragilities in the banking model that have
been causing financial crises for over 800 years. Given the costs of those crises – large and rising
– this is a benefit that needs to be weighed.
On monetary policy, the most important constraint facing policymakers today is the (close to) zero
lower bound (ZLB) on interest rates. At root, the ZLB arises from a technological constraint – the
inability to pay or receive interest on physical cash. This is a technological constraint that every
form of money, other than cash, has long since side-stepped. Even if you accept cash has other
benefits that mean it is the preferred payment method for some, the inability to pay interest on
public money is a relic of a bygone era.
In principle, a widely-used digital currency could mitigate, perhaps even eliminate, this
technological constraint. Specifically, CBDC would enable interest to be levied on central bank
issued monetary assets or digital cash. The extent to which this relaxed the ZLB constraint
depends, in addition, on the elasticity with which physical cash is provided to the public alongside
CBDC. Access to physical cash is an issue well above the pay grade of central bank technicians;
it is a political-cum-social issue.
Nonetheless, the potential macro-economic benefits of easing the ZLB constraint are large and
have grown over time. Studies suggest the ZLB constraint can result in significant shortfalls in
output relative to potential (of around 2%) and inflation relative to target (of as much as 2
percentage points).[31] These are potentially enormous gains in macro-economic terms. To those
Page 17
benefits needs to be added the gains to digital cash users of holding a remunerated instrument,
helping protect their purchasing power.
These financial stability and monetary-macro benefits should be at the centre of the debate about
the desirability and design of digital currencies. To give an example, the design of the
remuneration schedule for CDBC will in my view be one of the most significant decisions made by
central banks in the next half-century. Yet, to date, central banks have scarcely touched the surface
of the complexities this issue raises. This needs to change if the potentially transformative benefits
of CBDC are to be unlocked.[32]
Within the next year or so, the UK will reach a decision on CBDC. It will be pivotal. An earlier
pivotal moment was the Bank Charter Act of 1844, conferring on the Bank monopoly rights over
paper money. At that point, central banking came of age in the setting monetary and financial
stability policy. Tomorrow’s decisions on CDBC rival the 1844 Act in their significance for central
banks over the medium term. And that is why a deeper consideration of monetary and financial
stability implications is paramount today.
Communications and Engagement
At the point I joined the Bank, central banks lived by a mantra of “monetary mystique”. This was
more than just cultural. Secrecy was seen as one of the essential tools in central banks’
armoury.[33] Opacity imparted power, secrecy conferred influence. And the Bank of England was
seen as the grand-master of these Delphic arts. These were well-exemplified by the utterances of
its most famous former Governor, Montagu Norman, whose “never apologise, never explain” and “I
don’t have reasons, I have instincts” have gone down in the annals of central bank folklore.
For much of the first few hundred years of the Bank’s history, its public communication largely took
the form of an annual speech by the Governor of the day to the bankers and merchants of the City
of London at the Lord Major’s Mansion House Banquet, literally a stone’s throw from the Bank.
The public audience for this singular act of public communication was about as diverse a set of
white, male, middle-aged, slightly-pissed financiers as it is possible to assemble.
Body language can sometimes substitute for the spoken word. So it was at the Bank of England in
the 1920s, when the Governor’s “eyebrows” famously became one of the Bank’s means of
externally communicating. The Governor’s eyebrows were, in a way, a primitive form of emoji:
sterling crisis – sad face, bad pre-MPC presentation – very sad face. Nonetheless, for even the
most malleable-faced Governor, the eyebrows were an imperfect communications medium.
Beginning in the 1960s, a sea-change began in central bank communication practices, at first
slowly. Speeches by the Governor and other officials increased, by the 1960s averaging around
five per year. They were matched by other communication innovations. December 1960 saw the
first edition of the Bank of England Quarterly Bulletin, which is still going strong. There was roughly
a doubling in the number of speeches by Bank officials in each subsequent decade.
Page 18
With the arrival of the Monetary Policy Committee (MPC) in 1997, the number of published
speeches trebled. And with the advent of the FPC and PRC, Bank publications have kept going
through the gears. In 2020, the Bank issued 62 speeches, 56 working papers, over 100
consultation documents, 74 blogs and around 100 statistical releases - in total, around 500
publications. That is around four million words - a genuine revolution in transparency practices.
Transparency is necessary for central bank success, but not sufficient. The “twin deficits” of public
understanding and public trust need also to be tackled.[34] These deficits are closely linked as the
public are unlikely to trust something they do not understand. Alongside a greater quantity of
communication, tackling these deficits requires a shift in the nature of central bank
communications to boost understanding and in the degree of central bank engagement to build
trust. Both of those shifts are now underway.
On public understanding, in surveys around 60% of the general public believe the Bank of England
has a good understanding of the economy. That is the good news. The bad is that the same
surveys suggest only around a quarter of the public believe the Bank explains its actions and
decisions in ways they understand. Given the transparency revolution that has taken place, how
can that be? Which of those four million words are people not understanding?
The answer is most of them. If you study the linguistic complexity of central bank publications, you
find they are readily accessible to only between 5-10% of the population.[35] Others have made
this point in plainer English. A few years ago the Campaign for Plain English, a militant band of
grammarians, described the MPC’s Monetary Policy Statement as “worthless, impenetrable
waffle” and “gobbledygook”. Reading between the lines, I am not sure they liked it.
In response, the Bank has over recent years sought to simplify and diversify its stable of
publications, with a view to reaching a wider audience with simpler messaging. The Bank now
“layers” the policy messages in its regular reports, including through a simple “one line, one
graphic” version of the top-line message. Research suggests this simplification can have a
dramatic impact in improving not only the digestibility of messages, but also peoples’ degree of
confidence in them.[36]
As a second example, since 2015 the Bank has published a staff blog, “Bank Underground”,
offering shorter, punchier access to Bank analysis, including when it deviates from established
Bank policy positions. We fretted at the beginning that the outside world may fail to recognise the
separation of Staff opinion from Bank policy. We need not have worried. So far, the blogs have
received over one and a half million views.
When it comes to building trust among the public, speaking in plain words and sentences is not
enough.
Trust is built on engagement, a two-way flow of information not a one-way door. Trust-building calls
Page 19
for public conversation, not public lectures. While communications land, trust is built. This is true to
an increasing degree, as the model of trust-building among the public of the past – anonymised
and centralised – has been replaced by one which is instead personalised and distributed.[37]
As institutions used to communicating in a centralised, anonymised way, this shift in trust-building
poses new challenges for central banks. Like many public institutions, central banks have
historically been better at talking than listening, better at public understanding than understanding
the public. Building trust has required central banks, counter-culturally, to engage with far more
diverse audiences using different media.
Prior to Covid, I spent several years visiting left-behind parts of the UK listening to groups with
whom the Bank had not traditionally engaged – workers, charities, community and faith groups.
This is the economics of wandering around or “deep hanging out”.[38] These visits were one of the
most rewarding things I have ever done professionally, a unique source of intelligence on the
economy or “folk wisdom”.[39] My only regret was committing to visit every county in the UK before
having first counted them (there are loads).
These visits have subsequently become an established part of the Bank’s engagement
programme.[40] We now have a well-developed network of Citizens Panels across all regions of
the UK, as well as a Youth Forum. These meet regularly, hosted by senior Bank staff with an
independent chair, and comprise a diverse spectrum of the public. The issues raised at these fora
are published annually. We also have a programme of Community Forums, which engage with
local charities and community groups on local issues.
Complementing these engagement initiatives, I have helped oversee the Bank’s educational
engagement programme over the past few years, including the development of curriculum
materials for 11-16 year olds (EconoMe) and 8-11 year olds (Money and Me), the latter in
partnership with the Times Educational Supplement and the Beano. This has given the Bank a
face and a footprint among millions of early learners. That, I hope, will bear longer-term fruit for
both those learners and the Bank.
Finally, I set up and ran the Bank’s Flagship seminar programme. This has brought in a diverse
set of inspirational speakers – from Grayson Perry to Michael Holding, from Prue Leith to Doreen
Lawrence, from Trevor MacDonald to Billy Bragg. It has provided Bank staff, and the wider public,
with a wider angle lens on the most pressing societal issues of the day. Like the educational and
Citizen Panel programmes, it has allowed the Bank to engage with an entirely new cohort of
people with new perspectives on new issues.
Establishing these initiatives as central planks of the Bank’s engagement strategy is one of things
I am proudest about from my time at the Bank. I believe they hold the key to closing the twin
deficits. The economy affects everyone and everyone affects the economy. Central banks’ actions
also affect everyone. So the Bank needs to seek to engage everyone if it is to meet the needs of
Page 20
its stakeholders. Doing so is one of the most effective ways to ensure the Bank’s social contract
with the public is not breached.
To my mind, the revolution in central bank communication and engagement practices has been as
great as with monetary and financial stability frameworks - and crucial in cementing the success of
both. I have done my bit, leading the Bank’s outreach programmes and through 32 working
papers, 75 published speeches, over 100 regional visits, around 150 published articles in books
and academic journals, and many hundreds more unpublished speeches and schools talks. To
say nothing of several spectacularly unsuccessful books.
Looking ahead, the transparency revolution has yet to run its course. There is more the Bank can
do to deepen and broaden its engagement with a wider audience. There is further for the Bank to
go in harvesting the stories from this audience to inform its view of the economy. And there is
more the Bank can do to widen and deepen its educational offering in schools. On each of those
fronts, however, the Bank has over the course of the past ten years established robust foundations
on which to build.
When it comes to one particular aspect of transparency - forward guidance - I think a more
fundamental rethink may be needed, however. Forward guidance was introduced by central banks
after the Global Financial Crisis. In principle, it offered real promise. Offering a soft precommitment to a given policy path could bring forward some of the effects of future policy, giving it
extra bang for its buck.[41]
In practice, these benefits are only achievable if the policy guidance issued is hard enough to offer
some credible pre-commitment, at the same time as being soft enough not to lock central banks
in too tightly should circumstances change. This is a classic Three Bears problem: how to choose
a form of words that is neither too hard, nor too soft, but just right.
Getting guidance just right is a problem central banks, a decade on, have not solved satisfactorily.
In my view, with few exceptions forward guidance has ended up either being too vague and
ambiguous to offer an effective source of assurance to the outside world, or a source of regret
among central banks who have over-committed to something ex-post undesirable. Sometimes, it
has been a bit of both. As I see it, the structural problems with forward guidance come in two
flavours.
First, the type of forward guidance that financial market participants crave is precise, time-specific
guidance. This makes their job of pricing assets and inferring central bank signals easiest. But
this is also the type of forward guidance central banks are, rightly, least willing to provide. The path
of policy ought to depend on the path of the economy, not the passage of time. And that path is
uncertain. So from a policymaker perspective, imprecise, state-dependent guidance is the
preferred form.
This difference in requirements gives rise to an inherent tension. Either guidance ends up being
Page 21
ex-ante over-precise and ex-post unreliable. Or it is ex-ante imprecise but then serves little or no
ex-post signalling role. We have seen examples of both types in the UK, with early forward
guidance erring towards the former and recent forms towards the latter. So-called “dot plots”
combine, for me, the worst of both worlds.
There is a second problem with forward guidance, first articulated by Stephen Morris and Hyun
Shin several years ago. [42] The provision of public policy signals may dampen incentives among
market participants to invest themselves in understanding the economy. These risks have I think
been realised in practice, with forward guidance encouraging too much poring over central banks’
words and too little poring over the data on which monetary policy decisions are based. That is the
wrong way around.
I do not conclude from this that forward guidance has no role. I do, however, think this role, its
audience and its content needs to be rethought fairly fundamentally. This could build on the
experience of what was, in my view, the one example so far of successful forward guidance in the
UK.
In 2014, the MPC used three little words, “limited and gradual”, to describe the future trajectory of
interest rates. This language was neither precise nor time-specific, but did offer a clear and
simple description of the broad direction and destination of interest rates. It offered next to no
guidance to those pricing short sterling assets - that was not its purpose. But it did help
households when planning taking out a mortgage and businesses when contemplating taking out
a loan.
We know that from surveys the Bank conducted at the time. The MPC’s message got through to
around 75% of companies and around a fifth of households. In other words, the words were both
fairly widely understood and helped most those that mattered most to decision-making in the
economy. This guidance did not dis-incentivise anyone from personal budgeting, though it did
provide some basis for that budgeting.
My takeaway for forward guidance from this experience echoes my takeaway for the Bank’s
approach to communications generally. Where possible, keep it short and simple. And focus the
message on the needs of those shaping our economy, companies and households, not those
trading financial instruments. This is the direction the forward guidance puck, in my view, needs
now to travel.
Whatever Next?
From September I am moving a mile west from one roughly 270-year old building to another – the
Royal Society of Arts (RSA). The RSA is an 18th century Enlightenment institution. Like the Bank,
it has delivered large and lasting social change by combining brains and hands. Although no
longer in the public sector I will remain a public servant, seeking social change on some of the
Page 22
signature issues of the day - good work, fair education, lifelong skills, natural capital, place and
belonging, good governance.
Before I start, I shall be writing a book for students on Why Economics Matters. If history is any
guide, it is unlikely to trouble the bestseller list. I have written extensively about the failures of
economics, which are well captured by Robert Heilbroner’s “mathematics has brought rigour to
economic, but it has also brought mortis.” Yet, more than at any time in my professional life,
economics really does matter, especially to young people writing the next chapter.
The aim of any job is to leave the place slightly better than when you arrived. While I can claim no
credit, the Bank I leave is a far more transparent, powerful, analytical, agile, engaged, diverse and
meritocratic institution than the one I joined. The policy frameworks guiding its two founding
objectives – monetary and financial stability – have undergone a revolution. So too has the Bank’s
degree of external transparency and engagement. The Bank is unrecognisably better than the
institution I joined.
As a public servant, I consider myself fortunate to have worked on the signature monetary and
financial stability issues of the past 30 years, riding the waves of crisis that mark history and make
careers. Like those new Bank graduates, this old one could not have wished for a better
endowment. It has been an education to do so alongside several generations of incredible Bank
colleagues, including five exceptional Governors in Robin Leigh-Pemberton, Eddie George,
Mervyn King, Mark Carney and Andrew Bailey.
As ever, there are challenges aplenty for the Bank today – I have touched on one or two of them.
But its strong institutional foundations, and quite brilliant Staff, mean I have never had more faith in
the Bank rising to them. I leave with the UK economy surfing as high a wave as any in its history.
Inflation is bang in line with its target and the economy is growing at an annualised rate of over
20%. I wish Bank colleagues the best of luck in maintaining this performance after I have gone.
Eyebrows will be raised if not.
The views (more so than ever!) are not necessarily those of the Bank of England or Monetary
Policy Committee. I would like to thank Andrew Bailey, Nick Butt, Shiv Chowla and Mette Nielsen
for comments, Jack Meaning, Sandra Mills and Sarah Saunders for help in preparing the text and
hundreds of Bank colleagues over several decades for their support.

Avoiding Economic Anxiety
Speech given by
Andy Haldane
Chief Economist and Member of the Monetary Policy Committee (MPC)
Cheshire and Warrington LEP Economic Summit Webinar
30 September 2020
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
2
2
Thank you for the invitation to speak at this Cheshire and Warrington Local Enterprise Partnership (LEP)
Economic Summit. Through my colleague on the Industrial Strategy Council and former Chair of the LEP,
Christine Gaskell, I know the excellent work the LEP does in supporting the local economy and community.
That support has never been more important given the risks facing individuals, businesses and communities.
Given those risks, I thought I would focus today on the economic outlook in the UK, updated in the light of
events since the publication of the Bank’s Monetary Policy Report in August. Plenty has since happened.
But one factor has remained the same: the extra-ordinary degree of uncertainty about the economic outlook.
That makes monitoring the economy closely, and setting policy to support it, more important than ever.
At present, the largest clouds on the economic horizon in the UK come from: the effects of rising numbers of
Covid cases across the UK and the accompanying policy measures taken to contain them; risks to business
activity and jobs in the light of these public health developments; and the effects of moving to new trading
arrangements with the EU at year-end. This unholy trinity of risks give good grounds for caution.
Some degree of caution is desirable - in how we socialise, shop and work - to prevent the spread of this
awful disease. But we need at the same time to prevent healthy caution morphing into fear and fatalism.
Pessimism can be as contagious as the disease - and as damaging to our economic fortunes. Avoiding
economic anxiety is crucial to support the on-going recovery. This has important implications for how
businesses and policymakers act and communicate.
The Fall and Rise of the UK Economy
Let me start by running through the fortunes of the economy so far this year. The early months saw a
collapse in UK economic activity unprecedented in its speed and scale – a fall in GDP of around a quarter in
a matter of weeks. This recession was unique in its source as well as its speed and scale. An extreme
shock to public health required extreme public health measures, restricting the flow of goods, services and
people. Large parts of the economy were, in effect, put into a policy-induced coma.
The resulting collapse in aggregate demand would ordinarily have resulted in huge numbers of job losses.
Economists call the relationship between demand and jobs Okun’s Law.
1
 Historically, the Okun coefficient is
found to lie around 0.5-0.6. On that basis, the 25% fall in output would have been expected to lower
employment by perhaps 12-15% and to raise the unemployment rate by maybe 10 percentage points, taking
the pool of unemployed workers to around 5 million.
The Government’s job support packages, notably the Coronavirus Job Retention Scheme (CJRS), avoided
this catastrophic outcome. The CJRS provided support to around 9 million workers at its peak, in addition to
which income support was provided to around 2 ½ million self-employed workers. Currently, it is estimated

1
 See Okun (1962)
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
3
3
that around 2.75 million workers remain on furlough. The CJRS will end next month, to be replaced with the
Job Support Scheme (JSS) announced by the Chancellor last week.
Of course, not all jobs have or could have been protected. Data from the ONS’s Labour Force Survey (LFS)
is, at present, difficult to interpret when gauging employment trends. But based on HMRC data on paid
employees, around 700,000 workers may have been laid-off, in addition to around 280,000 self-employed
workers. That means around 1 million workers in total may so far have lost their jobs. This figure, more than
any other, underlines the gravity of the shock the UK economy has faced this year.
The better news is that the economy began its recovery from this dramatic fall earlier, and has since
recovered far-faster, than anyone expected. The speed and scale of the UK’s recovery has surprised to the
upside, persistently and significantly, for at least the past four months. Back in May, the Bank expected GDP
to be around 18% below its pre-Covid level on average during the third quarter. Consensus forecasts by
professional economists were, at the time, weaker still.
Four months on, we now expect GDP to be around 3-4% below its pre-Covid level by the end of the third
quarter. In other words, the economy has already recovered just under 90% of its earlier losses. Having
fallen precipitously by 20% in the second quarter, we expect UK GDP to have risen by a vertiginous 20% in
the third quarter – by some margin its largest-ever rise. Put differently, since May UK GDP has been rising,
on average, by around 1.5% per week.
The pace of recovery has varied, starting slowly in May, picking up pace rapidly during June and July and is
then expected to have slowed a little during August and September. Even if our GDP nowcasts for August
and September come to pass, there remains an average recession-sized gap between output and its
pre-Covid level. Nonetheless, had this economic outcome been offered as a forward contract in the early
summer months, absolutely everyone would have been a buyer.
So what explains this faster-than-expected recovery? And, crucially, will it persist in the face of the unholy
trinity of risks from Covid, unemployment and Brexit that potentially lie in store? These are related questions
because behaviours exhibited by households and businesses so far are likely to be revealing about what
impact future risks might have. Habits tend to persist. That means, for all the uncertainty about the outlook,
the UK’s recent economic performance does offer some signal about the future outlook.
The simplest explanation for the upside surprises to UK activity over recent months would be that lockdown
measures have been released sooner and faster than expected. But the pace of release from lockdown in
the UK has in fact been broadly in line with what the Bank had expected in May. The biggest surprise has
been the robustness of peoples’ spending behaviour in the face of lockdown constraints and other risks, not
the evolution of these constraints and risks per se.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
4
4
The behaviour of UK consumers has been most surprising. Based on our suite of fast indicators, UK
consumption has been rising by, on average, around 2% per week since May.
2
 As best we can tell,
consumer spending now stands at around pre-Covid levels. In other words, consumption has fully recovered
more than a year earlier than the Bank expected as recently as August. Large-ticket purchases, such as
cars and houses, are also back to around pre-Covid levels.
Against a backdrop of more than 40,000 Covid-related deaths, an extra 1 million people unemployed and
perhaps a quarter of the workforce having faced a cut in their incomes, the speed and scale of this recovery
in consumption is, I think, fairly remarkable. It suggests considerable resilience on the part of consumers in
the face of adversity. It also indicates considerable flexibility in both how and on what they spend.
On the how, UK consumers have (perhaps unsurprisingly) switched from the High Street online to meet their
spending needs. Online spending has jumped from 20% to almost 27% of overall spending during the
course of this year, peaking at a third of all spending in May. It seems probable some of this switch online
will endure into the future. Online habits tend to persist.
On the what, there have been notable switches in expenditure patterns in the face of new working and
socialising patterns (Table 1). According to data from Visa, on average this year spending on hotels and
accommodation is down by over 40% and on travel by over 35%. By contrast, spending on households
goods is up 8% and food spending is up by over 25%. Home production and home consumption has surged
to counterbalance the effects of reduced time and money spent away from the home.
Table 1: Average year-on-year changes in Visa Consumer Spending Index since March 2020
Food
etc
Clothing
& Foot
Household
Goods
Hea. &
Edu.
Trans. &
Comms
Rec. &
Cult
Hot. &
Rest.
Misc.
Goods
% 25.7 -18.2 8.0 -30.9 -35.4 -11.2 -41.8 -9.0
Source: Visa
Some of these spending habits are likely to persist. If more people work from home, travel spending is likely
to be permanently lower and office equipment spending permanently higher. Other spending switches are
likely to reflect pent-up demand and prove temporary – for example, on household goods, cars and houses.
Nonetheless, with cumulative spending this year still running significantly below last year’s levels – for
houses and cars, 20-40% below – considerable pent-up demand remains.
An interesting case study in consumer resilience comes from restaurant spending. Peak to trough, this fell
over 90% as restaurants closed. As they re-opened in July, restaurant spending picked-up, if initially slowly.

2
 See Haldane and Chowla (2020).
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
5
5
In surveys at the start of August, almost half of consumers said they felt very or quite uncomfortable about
visiting a restaurant. That suggested considerable caution about eating-out, with many consumers
seemingly putting a high price on personal safety.
The Government’s Eat Out to Help Out scheme provided a £10-capped subsidy to eating-out during August.
By the end of the month, it had supported 100 million meals across the UK. The scheme also provided an
interesting test-bed for answering the question: what price do people put on overcoming their caution about
visiting a restaurant? At £5.22, the answer was strikingly low.
The economic news has not all been positive. Job losses have continued to mount (though the Bank revised
down its estimate of job losses by around 720,000 between May and August). And the recovery in consumer
spending has not been matched among businesses. Business surveys suggest investment is still 20-30%
below its pre-Covid level and online job vacancies are around 45% lower. For the worst-affected sectors,
such as retail, hospitality and culture, the situation is weaker-still.
Where Next for the Economy?
The key question, at this juncture, is what happens next? Will the positive momentum of the past few
months continue or was this a false dawn? Will the resilience of the consumer, or the reticence of
companies, win out? Is the economic glass nine-tenths full or one tenth empty? Such is the uncertainty, it
would be imprudent to make confident predictions about the shape of the recovery from here - which is one
reason why, contrary to some commentary, I have not done so.
Recovering the final few percentage points of lost output was always certain to be the hardest. Adding to
that difficulty, storm clouds have recently begun re-gathering over the recovery. The three darkest of these
clouds come from the rising number of Covid infections and the accompanying re-tightening of some
lockdown restrictions across the UK; the threat from further job losses, including from the closure of the
CJRS; and the risks from the transition to the UK’s new trading arrangements with the EU at year-end.
All three of these risks - the unholy trinity - are clear and present dangers to the UK’s recovery. They are the
reason why, in its August projections, the MPC included a significant downside skew to demand. At the
same time, these risks need to be put in proportion and in context. While recognising their gravity, my
concern is that perceptions of these risks among household and businesses are, at present, exaggerated.
By creating excess caution, this has the potential to restrain unnecessarily the recovery.
Levels of anxiety among the general public ratcheted-up dramatically in March and April, by a factor of twothirds. Despite falling gradually, anxiety has remained at elevated levels, a third above pre-Covid levels
(Chart 1). Measures of confidence among businesses and households have followed a similar pattern,
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
6
6
spiking sharply downwards and largely remaining at those low levels, despite the sharp economic recovery
(Chart 2). This suggests a persistent, and perhaps puzzling, degree of pessimism.
Chart 1: Anxiety score (mean response) Chart 2: HH and business consumer confidence
Source: ONS
Note: Quarterly data to March 2020 and weekly data
thereafter
Sources: GfK/EC Consumer Confidence Survey, Lloyds
Business Barometer
Note: Headline consumer confidence is based on the
average of five survey balances: general macroeconomic
situation over the past 12 months and expectations for the
next 12
The failure of consumer confidence to recover is particularly striking, given the complete recovery in
consumption by households over the same period. Put differently, the historical correlation between
consumer confidence and spending appears to have broken down (Chart 3). A wedge has emerged
between peoples’ expectations and their spending, between their risk perceptions and economic reality. The
same is true, to a somewhat lesser extent, among businesses.
For households, that wedge might have arisen because of fears about future unemployment or inflation.
Past experience suggests peoples’ perceptions are sensitive to these macro-economic factors which is why
economists sometimes combine the two in a so-called “misery index”.3
 (Economics is not called the dismal
science for nothing.) If you plot this index over time, and project it into the future using the Bank’s most
recent forecasts, two features stand out (Chart 4).
First, by the end of this year the misery index will have risen to around 8%, over two percentage points
higher than at the start of the year. Economically, this is a significant rise. But, second, in the historical
scheme of things this move is nonetheless fairly modest and the level of misery, comparatively-speaking,

3
 See Nessen (2008)
0
1
2
3
4
5
6
2011
2012
2013
2014
2015
2016
2017
2018
2019
09-Apr
07-May
04-Jun
02-Jul
05-Aug
-40
-30
-20
-10
0
10
20
30
40
-35
-30
-25
-20
-15
-10
-5
0
Jan-18
Apr-18
Jul-18
Oct-18
Jan-19
Apr-19
Jul-19
Oct-19
Jan-20
Apr-20
Jul-20
Headline consumer
confidence (RHS)
Overall business
confidence (LHS)
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
7
7
remains fairly low. The misery index also suggests some wedge has opened up between public perceptions
and policymaker expectations for the economy.
Chart 3: Consumer confidence and spending Chart 4: UK Misery Index
Sources: GfK/EC, ONS.
Notes: Headline consumer confidence is based on the
average of five survey balances: general macroeconomic
situation over the past 12 months and expectations for the
next 12
Sources: ONS and Bank calculations
Notes: Index is summation of the unemployment rate and
inflation rate. Forecast is based on projections in Aug 2020
Monetary Policy Report.
If the public do have an exaggerated sense of the risks they face, what might be its source? Psychological
studies suggest it may reflect behavioural biases inherited from our hunter-gatherer past. Humans tend to
over-estimate systematically risks that are systemic or existential to lives and livelihoods.
4
 This “dread risk”
causes excessively cautious behaviour, sometimes with harmful side-effects – as when the exaggerated fear
of flying after 9/11 caused more people to drive.5

These exaggerated risk perceptions are often amplified by others’ words and actions. Caution is contagious.
What often then emerges is a “popular narrative”. These narratives have been found to be an important
driver of collective behaviour in financial markets and the economy.6 Behavioural biases at times of
existential risk, spreading contagiously, can result in pessimistic popular narratives detached from reality. At
times of stress, a global game of Chinese Whispers can generate unduly negative expectations.

4
 See Haldane (2015)
5
 See Gigerenzer (2004)
6
 See Akerlof and Shiller (2009)
-25
-20
-15
-10
-5
0
5
10
-25
-20
-15
-10
-5
0
5
10 Percentage point
difference on 19Q4
Headline Consumer
Confidence (RHS)
Retail Sales (LHS)
Percentage change
on 19Q4
0
2
4
6
8
10
12
14
16
2000
2002
2004
2006
2008
2010
2012
2014
2016
2018
2020
2022
Misery index
Forecast
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
8
8
I think the prevailing popular economic narrative, among businesses and households currently, is unduly
negative. It has emphasized recession and risk over recovery and resilience. It has resulted in good
economic news (of which there has been plenty) being discounted too readily, and fearfulness about the
future being accentuated. Let me give a few simple examples.
Chart 5 plots a simple Google search of the relative incidence of two words – “recession” and “recovery”; it
is an economic pessimism ratio. Before the Covid crisis, the pessimism ratio was steady. As the crisis
struck there was a predictable and sizable spike upwards, by a factor of roughly 8. Since then the ratio has
fallen somewhat, as we might expect as the economy has moved from recession to recovery.
The dynamics of this sentiment indicator are nonetheless revealing. The drift down in the ratio has been
very gradual. It remained elevated well after economic recovery had commenced and recession ceased.
Even now, four or five months into recovery, recession is out-Googling recovery by a factor of 15 to one,
above its pre-Covid level. The prevailing popular narrative on the economy has remained recessionary.
Chart 5: Ratio of Google Trends topic searches for the term recession vs economic recovery
Source: Google Trends
A particularly revealing episode is associated with the notable spike in the pessimism ratio on the 12 August.
This was when the Office for National Statistics published second quarter GDP figures for the UK. These
showed a huge fall of over 20% in GDP, the largest quarterly fall on record by far. This, understandably, was
one of the top three new stories on the day. Here are some of the headlines that accompanied it:
0
20
40
60
80
100
120
140
160
180
200
Sep-19 Oct-19 Nov-19 Dec-19 Jan-20 Feb-20 Mar-20 Apr-20 May-20 Jun-20 Jul-20 Aug-20
Ratio of searches for recession:economic recovery
Economic growth
Monthly GDP
released
showing
growth
Q2 GDP
release
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
9
9
Yet the irony is that the only news in this release was GDP growth for the month of June, the final month of
the quarter. This saw an almost 9% rise in activity, by far the largest rise in any month ever and above
market expectations. Yet negative media headlines outnumbered positives by many multiples. Positive
economic news was media-filtered into an extreme negative event.
This filtering of good news, and accentuation of the bad, is a familiar pattern of human behaviour at times of
stress and uncertainty. Psychologists call it “catastrophizing” – discounting the best and fixating on the
worst, whatever the balance of risks. It is a well-known problem among people suffering anxiety or
depression. Economy-wide, the result has been collective dread risk, fanned by contagious pessimism.
There is some evidence of a detachment from fundamentals in the behaviour of financial markets too.
Chart 6 plot the responsiveness of various financial prices (interest rates, equities and the exchange rate) to
macro-economic news in the two years period to the Covid crisis and during this year. Pre-Covid, all three
market prices were sensitive to macro news. Since Covid, this relationship has disappeared. Financial
markets, recently, have detached from economic fundamentals.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
10
10
Charts 6 Financial market sensitivity to economic news
3 year sterling interest rates Sterling ERI
FTSE All-Share
Source: Bloomberg Finance L.P., Reuters Tick data from Refinitiv and Bank calculations.
The Role of Policy
There is another reason why people may, at present, have an exaggerated sense of anxiety. The risks they
face (to health, jobs and Brexit) are perceived to be beyond their control. It is well-known that events
perceived to be uncontrollable add to anxiety. Indeed, anxiety can itself generate perceptions that events
are beyond your control, contributing to fatalism about the future and excess levels of caution.
7
Yet the risks facing individuals, businesses and the economy at present are, at least to some degree, within
our control, collectively if not individually. All of these risks can be mitigated, if not eliminated entirely, by the
actions of individuals, businesses and policymakers.

7 See Grupe and Nitschke (2013).
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
11
11
One safeguard against these risks comes from our individual actions when we socialise, shop and work. A
further, collective, source of insurance comes from public policy – public health policy, fiscal policy and
monetary policy. Extra-ordinary action has been taken on all three fronts during the Covid crisis to contain
risks to the public, businesses and the economy. As importantly, it has been made clear that further action
would be taken on all three fronts where risks to re-escalate.
Taking the three largest risks in turn, on public health there has been a worrying rise in Covid cases across
the UK recently. In response, a sequence of local lockdown measures have been put in place and last week
these were accompanied by some tightening of national restrictions. While it is too early to judge what
economic impact these measures will have, they are likely to restrain economic activity and slow growth.
At the same time, it is important to put the likely impact of these measures in proportion. While the impact on
individual businesses may be large, their direct impact on aggregate activity is likely to be modest. The
measures affect only a sub-set of spending, notably hospitality (7% of total consumption) and work-related
travel (also 7%). If both categories were to fall to their levels at the start of summer, as lockdown began to
be eased, this would take a little over 3% off levels of consumption in the fourth quarter and 2% off GDP.
But there are good reasons to think this is likely to be a significant over-estimate. Measures announced so
far are nothing like as severe as earlier in the year. Even during that earlier period, we saw significant
substitution between spending categories, partially insulating aggregate spending. In this respect, it is
notable how quickly spending in the worst Covid-affected US States bounced back recently following their
second wave.

Of course, it is possible the indirect effects of lockdown measures hit spending harder. The most important
of those effects would arise if they caused a further significant dip in consumer and/or business confidence.
But that rather underscores my central point – the importance of avoiding over-pessimistic commentary on
the economic outlook which fuels anxiety, heightens caution and risks becoming self-fulfilling.
On the risks to jobs, this is clearly real and it is very likely further losses lie ahead. In its August projections,
the MPC expected unemployment to rise to around 7.5% (or 2 ½ million people) by the end of the year. On
that assumption, around two-thirds of the net job losses from the Covid crisis have already occurred, with a
further 500,000 losses expected in the remainder of the year.
That unemployment projection, while necessarily very uncertain, flowed from an assumption that UK output
would be around 5.4% below pre-Covid levels by year-end.
8
 With an Okun coefficient of around 0.6, that
translated into a rise of unemployment of 3-3.5 percentage points. In the August projections, this took
unemployment from its pre-Covid level of 4% to around 7-7.5% at its year-end peak.

8 Corrected 1 October 2020 from 7% in the initial publication.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
12
12
Since then, there have been three significant pieces of news about the jobs outlook. First, output has
continued to recover faster than expected. By the end of Q3, output is now expected to be only around 3-4%
below its pre-Covid level. Other things equal, this lowers risks to the MPC’s unemployment projections.
Second, acting in the opposite direction, there are downside risks to this output projection as we move into
the fourth quarter due to the new lockdown measures and any future measures.
The third piece of significant news is the announcement of the Government’s new JSS last week, which
provides wage subsidies to workers returning on reduced hours for a six month period. With details still to be
finalised, it is too early to reach quantitative conclusions on the impact of the JSS. The direction of its impact
is clear, however, reducing risks to unemployment relative to the Bank’s August projections which assumed
no successor scheme.
The balance of these effects on jobs is unclear. Surveys of employment expectations among households
and businesses are at low levels. For example, households’ expectations are consistent with a rise in
unemployment to around 8 or 9%, at least a percentage point higher than the MPC’s August projections.
This is another example of the wedge between public and policymaker expectations. It also means
households and businesses would be positively surprised if the MPC’s projections were to materialise.
The third of the unholy trinity of risks – Brexit – is in some ways the hardest of all to judge. It too, though, is a
controllable risk, at least to some degree, by businesses and policymakers. Whatever the final outcome of
the trade negotiations, it is clear the UK will be leaving the customs union with the EU at the end of the year.
Doing so in a way that reduces operational frictions in trade, and hence costs for the economy, will require
intensive preparatory work by the Government and businesses.
Existing surveys suggest many firms still have a distance to travel before they are fully prepared for leaving
the customs union with the EU, understandably so given the disruption caused by Covid. But there is still
time for this operational work to be done and it will be important businesses prioritise that in the weeks ahead
to minimise disruption to their businesses and the economy. I am confident UK companies will rise to this
challenge, as they have to the challenge of Covid.
Let me say a final word on monetary policy. The MPC has already taken extraordinary monetary policy
action in the face of the Covid crisis, reducing Bank Rate by 65 basis points and expanding QE by a further
£300 billion. Through its forward guidance, the MPC has made clear that it will not tighten monetary policy
until there is clear evidence of progress being made towards reducing unemployment and returning inflation
to target on a sustainable basis.
The MPC has also made clear that it stands ready to take whatever further action is needed to support the
economy and return inflation sustainably to target, should that prove necessary. Alongside public health and
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
13
13
fiscal measures, this forms part of the on-going economy-wide insurance policy put in place during the Covid
crisis. That insurance policy should be a source of confidence and comfort for companies and households
with understandable concerns about their businesses and jobs.
At its meeting earlier this month, MPC decided no further monetary policy action was needed to achieve its
statutory objectives. The MPC’s minutes also explained that the Bank would be taking forward operational
work to assess the feasibility of implementing negative interest rates, should this be required in future.
Commencing that operational work underlines the MPC’s commitment to having negative rates as a potential
tool in the monetary policy toolbox.
Some commentators have interpreted the start of this work as conveying a signal about the likelihood of the
MPC introducing negative rates in the near-term. The minutes contained no such signal. The operational
work necessary to assess the feasibility of negative rates is likely to take a number of months. After that
work is complete, judgements on negative rates will depend on the economic outlook at the time and in
particular on whether that necessitates further monetary stimulus. If that condition was satisfied, any
decision on negative rates would then depend on whether the balance of costs and benefits from using this
tool was positive and whether this cost/benefit balance favoured negative rates over other monetary tools.
All three of these conditions would need to be satisfied before negative rates became a reality. At present,
none of those conditions is in my view satisfied.
Conclusions
The economy faces uncertainties that are extraordinarily large and risks that are skewed to the downside. In
this environment, caution is natural and understandable. It is important policymakers are vigilant to these
risks and uncertainties and responsive to them with their policy actions. It is important they provide, as they
have so far, economy-wide insurance to support businesses and households during these troubled times.
That has been the approach of the MPC so far and it will continue to be its approach if new risks arise.
At the same time, it is important the unexpectedly positive progress the economy has made so far this year is
not overlooked. The economy has recovered further and faster, and has shown far-greater robustness and
resilience, than anyone expected. This positive news has received less attention than it deserves, both on
its own terms and because of what it may tell us about the economy’s resilience to future shocks.
My concern at present is that good news on the economy is being crowded-out by fears about the future.
This is human nature at times of stress. But it can also make for an overly-pessimistic popular narrative,
which fosters fear, fatalism and excess caution. This is unhealthy in itself but, if left unaddressed, also risks
becoming self-fulfilling. I have a Rooseveltian fear of fear itself.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
14
14
If the economy were sat on a psychiatrist’s sofa, the diagnosis would not be especially difficult. A propensity
to dismiss good news and dwell on bad? To catastrophize about the future? The sense of events being
beyond our control? These are the psychological symptoms of anxiety. And collective anxiety is as
contagious, and could be as damaging to our well-being, as this terrible disease.
Averting an economic anxiety attack calls for a balanced and flexible approach to the words and actions of
businesses and policymakers. Planning for the worst is important, but needs to be accompanied by hope for
the best. Encouraging news about the present needs not to be drowned out by fears for the future. Now is
not the time for the economics of Chicken Licken.9

That means balance in how the economic outlook is described, acknowledging good news as well as bad,
contemplating upside as well as downside scenarios, taking positive signals (as well as some comfort) from
the resilience shown so far. This is not boosterism; it is balance, at a time when behavioural biases and
pessimistic popular narratives offer an unbalanced lens on the economy. The policy authorities, including
the Bank, have a public responsibility to avoid economic catastrophizing.
Policymakers, including the MPC, have already demonstrated a willingness to act at speed and scale to
mitigate economic risks. They have put in place the UK’s largest-ever economic insurance policy. It is
important this insurance policy continues to flex as new risks arise, to build damaged confidence among
households and businesses. For its part, the MPC has committed to keeping borrowing costs at current
extraordinarily low levels to support jobs and incomes for as long as necessary to return inflation to target.

It is fantastic to be here at Glanford Park Stadium, home of Scunthorpe United Football Club. I remember as
a boy learning about how Liverpool legends Kevin Keegan and Ray Clemence started their careers here.
Cricketing legend Sir Ian Botham started (and ended) his professional football career here too. A couple of
weeks ago, my home town Guiseley entertained Scunthorpe in a pre-season friendly. Scunthorpe won 2–0.
Off the football pitch, it has not been all wins for Scunthorpe recently. Problems at the British Steel plant in
the town have put at risk the jobs of 5,000 workers and a further 20,000 in its supply chain.
1
 Hopefully, a
new buyer can be found and jobs in the town and beyond can be assured. This is the latest chapter in what
has been a long and sometimes painful story for the UK steel industry recently. It has been a recurrent
theme of my visits around the UK, including in Redcar and Port Talbot.
The loss of signature industries can leave lasting social and economic scars – jobs lost, families disrupted,
communities decimated. It is a pattern we see repeated across many industries in many advanced
economies. It has been a key cause of the disaffection and disconnection felt by many and a potent
polarising force in our politics, policies and societies. This polarisation is also a potent force shaping the
fortunes of our economies, as I will go on to discuss.
The travails of the steel industry lead me neatly to the issue I want to discuss today – jobs and pay. In the
UK recently, this has been a good news/bad news story. The good news has been the new jobs created –
the best jobs recovery in the UK since at least the Second World War. The bad news has been the subdued
performance of pay over much of that period – the weakest pay performance the UK has experienced since
at least the Boer War.
I will first discuss recent developments in jobs and pay, before providing an account of them centred on
movements up and down the jobs ladder. I will then turn to recent developments in the world and UK
economies and their potential implications for monetary policy.
The Jobs Boom
The past few years have been a jobs bonanza in the UK. Since 2012, 3.4 million net new jobs have been
created. Table 1 compares job creation across previous periods of UK economic recovery. No economic
recovery since the Second World War has seen as many new jobs created. This leaves the UK employment
rate at its highest (61.5%) and the unemployment rate at its lowest (3.8%) in half a century (Charts 1 and 2).

1
 https://www.bbc.co.uk/news/business-48365241
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
Table 1: Job creation in different UK recoveries
Recovery dates Job creation (millions)
2011 Q4 - present 3.4
1993 Q1 – 2003 Q2 2.9
1984 Q2 – 1990 Q1 2.7
1977 Q4 – 1979 Q2 0.3
1972 Q1 – 1973 Q4 0.5
1952 – 1955 0.7
Sources: ONS and Bank of England Millennium of Data.
Notes: Data taken at quarterly frequency from 1970s and measure change in whole economy employment period during periods of
sustained falling unemployment. Other definitions of recovery would of course suggest different amounts/periods of job creation, but it
remains true that the current phase of job creation is high by historical standards.
Chart 1: UK employment rate
Sources: ONS.
Notes: 16+ employment rate.
Chart 2: UK unemployment rate
Sources: ONS.
Notes: 16+ unemployment rate.
50
52
54
56
58
60
62
64
1971 1975 1979 1983 1987 1991 1995 1999 2003 2007 2011 2015 2019
Per cent
0
2
4
6
8
10
12
14
1971 1975 1979 1983 1987 1991 1995 1999 2003 2007 2011 2015 2019
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
This jobs bonanza has had a broad geographic base. Since 2012, unemployment has fallen by at least 4
percentage points in all corners of the UK (Chart 3). Unemployment here in Scunthorpe has also fallen
significantly from its peak of over 11% during the financial crisis but, at around 7 ½%, remains well above the
national average.
Chart 3: Fall in UK unemployment by region (latest vs. post-crisis peak)
Sources: ONS and Bank of England calculations.
Notes: Data for NUTS1 regions. Changes between post-crisis peak and latest data in Labour Force Statistics.
A high quantity of new jobs is no guarantee of quality. Yet recent job creation has also been encouraging on
that front. Of the jobs created since 2012, over 2.7 million or around 80% have been full-time. Around half
(over 1.7 million) have been taken by women. Employment rates among the youngest (aged 16-24) and
oldest (aged 55+) have both risen strongly, by around four to five percentage points, respectively.
The jobs bonanza has been fairly broadly-based across the occupational spectrum too (Chart 4). Unusually,
all of the main categories of occupation have seen a rise in employment rates since 2012. As elsewhere,
this boost has been largest at the higher-skill (such as managers and directors) and lower-skill (such as
elementary occupations) ends of the occupational spectrum, with the mid-skilled benefitting least.2

2
 For example, Haldane (2018), Autor (2014) and Autor (2017).
0 1 2 3 4 5 6 7
NE
NW
Y&H
E Mids
W Mids
East
London
SE
SW
Wales
Scotland
NI
Percentage points
Region
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
Chart 4: Change in UK employment by profession
Sources: ONS and Bank of England calculations.
Notes: Ordering defined by ONS 1-digit SOC 2012 classification.
Despite a slowing economy, this large and evenly-distributed jobs bonanza has continued apace. The stock
of available job vacancies – a measure of aggregate demand for workers – currently stands at around
830,000, close to its all-time high. After seven years of continuously creating jobs, and continuously falling
unemployment, the UK labour market remains in rude health.
It also appears to be tight. One way of gauging that comes from comparing the stock of vacancies (V) in the
economy (aggregate labour demand) with the stock of people seeking work (U, aggregate spare labour
supply). Chart 5 plots two measures of the V/U ratio in the UK.3
 They are at their highest since the 1960s.
Both ratios also well in excess of 100%, suggesting demand for labour exceeds available supply.

3 The broader measure is the vacancy-effective searcher ratio, where effective searchers are a weighted average of the employed,
unemployed and non-participants. The weights on the employed and non-participants are their job finding probabilities relative to the
job finding probability of the unemployed. The unemployed therefore have a weight of one.
0
5
10
15
20
25
Managers,
Directors and
Senior
Officials
Professional
Occupations
Associate
Prof. & Tech
Occupations
Administrative
and
Secretarial
Occupations
Skilled Trades
Occupations
Caring,
Leisure and
Other Service
Sales and
Customer
Service
Occupations
Process,
Plant and
Machine
Operatives
Elementary
Occupations
Percentage change in employment between 2011 and 2018
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
Chart 5: Vacancy to unemployment ratio
Sources: ONS and Bank of England calculations.
Other evidence confirms that picture. The UK unemployment rate (3.8%) is below the Bank’s estimate of its
long-run equilibrium rate (or NAIRU) of 4 ¼%. In business surveys, a majority of companies point to acute
skills shortages across a range of professions and industries. When I asked on a recent visit to Northern
Ireland about businesses’ top three concerns, the answer was revealing: skills, skills and skills.
Surveys of workers tell a similar tale. Households’ outlook for their financial situation is currently in line with
historical averages and well above troughs in previous economic downturns (Chart 6). It is easy to be blasé
and to forget just how big a societal shift this represents. Job insecurity was the scourge of the latter part of
the 20th century. During the 21st, and in particular over recent years, that scourge appears finally and
thankfully to have lifted.
0
20
40
60
80
100
120
140
160
2002 2004 2006 2008 2010 2012 2014 2016 2018
Vacancy-Unemployment ratio Vacancy-Effective searcher ratio
Ratio, percent
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
Chart 6: Household financial position
Sources: GfK and Bank of England calculations.
Notes: Question in survey asks, “How do you expect the financial position of your household to change over the next 12 months?” Net
balance refers to proportion of respondents expecting improvement minus proportion expecting worsening.
The Pay Bust
A tight labour market is usually a recipe for rising pay. Yet until recently, there was little sign of pay picking
up closer to rates seen before the crisis. Chart 7 plots inflation-adjusted pay in the UK over the past
70 years. For the first 60 of those years, real pay growth grew steadily at over 2 ¼% on average each year.
Over the past 10 years, real pay growth has averaged minus 0.4% – a “lost decade” for the average worker.
Despite some recovery recently, that leaves the inflation-adjusted pay of the average UK worker still over 5%
lower today than it was just prior to the crisis. You have to go back to the 19th century to find a similar period
of stasis in UK living standards. The UK’s jobs bonanza has been accompanied by a pay disaster.
Chart 7: Real pay
Sources: ONS, Bank of England Millennium of Data and Bank of England calculations.
Notes: Data at annual frequency using AWE deflated by CPI. Prior to AWE and CPI we use AEI and RPI, respectively, to extend the
series backwards.
-25
-20
-15
-10
-5
0
5
10
15
20
1985 1988 1991 1994 1997 2000 2003 2006 2009 2012 2015 2018
Net balance
0
20
40
60
80
100
120
1950 1955 1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010 2015
Index, 2007 = 100
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
These falls in real pay have been widely spread geographically. Chart 8 compares average real wage rises
across UK regions in the ten years either side of the global financial crisis. In the ten years prior to the crisis,
all regions experienced annual rises in real pay, averaging between 1.3% (Wales) and 2.2% (London).
These pay rises were roughly in line with pre-crisis economic growth rates.
Chart 8: Real pay by region
Sources: ONS and Bank of England calculations.
Notes: Data at annual frequency using median gross weekly pay in ASHE deflated by CPI. Regional calculations at NUTS1 level.
Over the past decade, average growth rates of real pay in every UK region have fallen into negative territory
and well below average GDP growth rates in the economy over the same period (1.3%). The worst-affected
regions have been the East Midlands and London. Scunthorpe has been towards the lower end of this pack,
with real pay falling by, on average, 0.8% per year over the past decade.
The distribution of real pay rises across the wage distribution tells a nuanced story (Chart 9). In the decade
prior to the crisis, real pay growth was fairly evenly distributed. The one exception was the top two wage
deciles, where real pay growth averaged 2 ¼% – 0.5 percentage points higher than in the lower deciles.
The global financial crisis (2008-11) saw a sharp and significant amplification of these pre-crisis trends. The
distribution of real pay rises fell below zero for all income groups. But while for the top two income deciles
real pay shrank by only 2%, for the bottom two deciles it fell by 4%. This widened pre-existing wage
inequalities. For the poor, this really was a Great Recession.
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
1998-2007 2009-2018 Average annual real pay growth (per cent)
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
Chart 9: Real pay growth along the pay distribution
Sources: ONS ASHE and Bank of England calculations.
Notes: Data based on real weekly pay (using CPI as the deflator).
During the subsequent period of jobs recovery (2011-17), these distributional patterns went into reverse.
Lower deciles of the wage distribution experienced real pay rises. For the bottom three deciles, these
averaged around 1 ¼% per year. They came, at least in part, courtesy of the National Minimum and,
subsequently, the National Living Wage. By contrast, real pay among the upper pay deciles continued to fall
over this period. Wage inequalities began to shrink.
Until, that is, 2018 when real pay of the bottom two wage deciles fell by a further 2%, partly reflecting the
effects of sterling’s depreciation after the referendum which pushed up consumer prices faster than wages.
The upper wage deciles appear largely to have been insulated from these pressures, with real pay rising by
around 1% among the top two wage deciles.
Comparing the past 10 years as a whole, real pay today is lower at every point along the wage distribution
and, with the exception of the bottom decile, by broadly similar amounts of around 5% (Chart 10). These
real pay falls have been felt by both men and women, by the young, middle-aged and young, and by the low,
medium and high-skilled. The lost decade of pay was an equal-opportunity disaster.
Alongside this period of stasis in real pay have been some important on-going shifts in the nature of work.
There has been strong growth in Alternative Working Arrangements (AWA), such as self-employment,
zero-hours contracts and agency work (Chart 11).4
 The number of self-employed, zero-hour contract and
agency workers in the workplace has increased by 1.1 million, 0.7 million and 0.6 million, respectively, over

4
 For example, Taylor (2017).
-8
-6
-4
-2
0
2
4
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95
Pre-crisis (97-08) Crisis (08-11) Post-crisis (11-17) 2018
Average annual real wage growth (per cent)
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
the past decade. There are now close to 1 million agency and zero-hours contract staff and 5 million
self-employed.
Chart 10: Real pay along the pay distribution
Sources: ONS and Bank of England calculations.
Notes: Nominal pay data for 2008 translated into 2018 prices using CPI.
Chart 11: “Alternative work” arrangements
Sources: ONS and Bank of England calculations.
Notes: Agency workers calculated on a quarterly basis using LFS data on those reporting to work for, or receive payment from, an
employment agency in their main or second job. Data on zero hour contracts is published annually up to 2012, and then bi-annually
from 2013. Zero hour contract and agency data are not seasonally adjusted. Some of the increase in the agency work and zero hours
lines may reflect increased awareness and reporting.
These structural shifts in the workplace are often called the “gig economy”. Their common denominator is
greater working time flexibility in the jobs contract. For some, this increased flexibility has been welcome. In
0
200
400
600
800
1000
1200
10 20 30 40 50 60 70 80 90
2008 (2018 prices)
2018
Gross average weekly earnings (2018 prices)
0
1
2
3
4
5
2001 2003 2005 2007 2009 2011 2013 2015 2017 2019
Number of people on different employment contracts , millions
Self employed Agency workers In employment on a zero-hours contract
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
surveys, around 30% of workers says they have benefitted from this flexibility, particularly female workers
(re-)entering the workforce and older workers tapering into retirement.5
For others, though, these shifts in working arrangements have been a cause for concern. In surveys, around
half to two-thirds of workers say they would prefer either to be working more hours or to have greater
certainty around their hours.
6
 For them, AWA are not welcome flexibility but unwelcome insecurity. The
TUC estimate that as many as 1 in 9 workers may be in work which is insecure in income terms.7
This income insecurity has been a key and recurrent theme in my recent discussions with charities, faith and
community groups across the UK. For some, these income insecurities have been worsened by the
introduction of Universal Credit. This has added time lags in payments to claimants, caused rising concerns
about benefits sanctioning and has heightened uncertainties about eligibility for work criteria.
8
Recent research by the Resolution Foundation has found that pay volatility in fact affects a significant
number of UK workers, not just those on low pay or the gig economy.
9
 Over 90% of those who remained
with the same employer during 2016-17 experienced at least one month when their take-home pay varied by
more than 5%. This pay volatility is more prevalent for those on lower earnings and among the young.
Income volatility appears to have significant welfare costs for workers. A recent study found the average
worker would be willing to sacrifice a fifth of their wage to avoid a variable work schedule set by their
employer.10
 In practice, gig workers tend to be paid less not more. Pay of self-employed and zero-hours
contract workers lies significantly below employed and permanent contract counterparts (Chart 12). This is a
malign coincidence of lower and less certain income.
Chart 12: Pay for alternative work
Sources: ONS and Bank of England calculations. With thanks to Steve Machin and Rui Costa at the CEP for kindly providing the data
for these charts.

5
 Datta, Giupponi and Machin (2018).
6
 For example, Datta, Giupponi and Machin (2018) and https://www.tuc.org.uk/news/two-thirds-zero-hours-workers-want-jobsguaranteed-hours-tuc-polling-reveals
7
 https://www.tuc.org.uk/news/1-9-workers-are-insecure-jobs-says-tuc
8
 https://www.bankofengland.co.uk/events/2018/november/townhall-blog-scotland-november-2018
9
 Tomlinson (2018).
10
 Mas and Pallais (2017).
60
70
80
90
100
110
120
2007-2008 2009-2010 2011-2012 2013-2014 2015-2016 2017-2018
Employees
Self-employed without workers
Self-employed with workers
Median real weekly income
(2018 prices), 2007/08 = 100
0
2
4
6
8
10
12
14
2002 2004 2006 2008 2010 2012 2014 2016 2018
Zero Hour Contract Workers
All Workers
Median real
hourly wages
(2018 prices)
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
These changes in the nature of work are likely to have contributed to the recent weakness of pay. The
creation of larger numbers of, on average, lower paid jobs will have had a direct, dampening effect on pay.
And income insecurity and the lack of unionisation among gig workers may also have lowered their pay
bargaining power.
Summing up, then, a jobs boom has reduced the scourge of job insecurity that troubled workers in the past.
But in its place has emerged a different scourge – static or lower pay and rising income insecurity. Job
insecurity at the end of the 20th century has given way to income insecurity at the start of the 21st. This
insecurity is an important part of the explanation for recent patterns in jobs and pay.
The Jobs Ladder
The framework most often used by macro-economists to explain pay and jobs is the Phillips curve – the
negative relationship between pay growth and the stock of unemployment.11
 Despite reports of its demise,
there is still reasonably clear empirical evidence of a Phillips curve in labour market data, once allowance is
made for a possible downwards shift in the NAIRU and/or slope of the Phillips curve.12
An alternative, complementary, lens on the labour market comes from looking at flows of workers between
jobs, rather than stocks of unemployed. There is a large literature studying the process by which people
search for work and are matched into jobs.13
 There is empirical evidence that job-to-job flows – the velocity
of circulation in the labour market – can play a significant role in explaining wage growth.14
When studying labour market flows, a “jobs ladder” pattern of behaviour has often been found. In a growing
economy, workers move between jobs by climbing a ladder of opportunities based on job characteristics
such as relative rates of pay and firm productivity. The tighter the jobs market, the harder it is for companies
to fill vacancies, the greater their incentive to offer workers higher-paid positions and the stronger the
incentives among workers to move into these positions.
15,16

More productive firms are better able to offer higher rates of pay. This means more productive firms are also
the ones most likely to expand, as they bid-away workers with higher pay offers. Less productive firms, by
contrast, are unable to pay-up and are more likely lose workers and shrink. As workers climb the pay and
productivity ladder, we would thus expect to see simultaneously higher levels of job moves, and higher rates
of pay and productivity growth.
17

11
 Phillips (1953).
12
 For example, Broadbent (2017).
13
 For example, Pissarides (2000).
14
 Melosi and Faccini (2019).
15
 Moscarini and Postel-Vinay (2016) and Moscarini and Postel-Vinay (2018).
16
 Abel, Tenreyro and Thwaites (2018) discuss the role of monopsony power in the labour market.
17
 Moscarini and Postel-Vinay (2016) and Moscarini and Postel-Vinay (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
The reverse is true during a slowdown. Then, rates of hiring fall and rates of job transition slow. Workers
are either forced off the jobs ladder entirely (“unemployment”) or, in coordination with employers, the labour
market outcome results in them moving to a lower rung of the pay and productivity ladder
(“under-employment”). Lower-productivity firms expand while higher-productivity firms shrink. The result is
slower rates of job move and lower rates of pay and productivity growth.
One of the attractions of this approach is that it explains the velocity of circulation in the labour market, pay
and productivity growth as a single, simultaneous process as workers traverse the jobs ladder. We do not
need to rely on unexplained shocks to productivity to explain pay. Rather, it is behaviour by companies and
workers which generates simultaneous movements in pay and productivity.
So how useful is this framework when characterising recent behaviour in the UK labour market? At a high
level, this story fits the UK facts. Chart 13 plots job-to-job flows in the UK over recent decades. It displays a
pro-cyclical pattern, with job moves picking up during a recovery and falling during a recession. From their
low point in 2010, job-to-job flows in the UK have picked up as the labour market has tightened.
Chart 13: Job-to-job flows
Sources: ONS ASHE and Bank of England calculations.
Hiring and quit rates among companies are also pro-cyclical, as we would expect. More than that, the
degree of pro-cyclicality in hiring is strongest, and in firing is weakest, among the medium and large firms.
To the extent size is a proxy for productivity, this is also as we would expect.
18
 During expansions, more
productive firms hire more, and lose fewer, of their workers than less productive firms (Charts 14a and 14b).
Other patterns in the data less obviously fit the facts, however. Despite jobs flows picking up, pay growth
has until reasonably recently remained slow and low. Meanwhile, productivity growth has remained

18
 For example, Bernard et al (2014) and Wales et al (2018).
0%
2%
4%
6%
8%
10%
12%
14%
16%
18%
1976 1979 1982 1985 1988 1991 1994 1997 2000 2003 2006 2009 2012 2015 2018
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
stubbornly subdued despite a seven-year jobs recovery. So has the jobs ladder broken? Have its rungs
been removed? To understand these developments, it is useful to break the decade down into phases.
Charts 14a and 14b: Hire and quit rates by firm size
Hire rate Quit rate
Sources: ONS LFS and Bank of England calculations.
Notes: Charts show four-quarter moving average of hire and quit rates.
(a) Descending the Jobs Ladder
The global financial crisis generated a sharp fall in output. Given that fall, the subsequent rise in
unemployment was more modest than expected. Between 2008 Q1 and 2009 Q2, UK output fell by around
6 ¼%, while unemployment rose by just over 2 ½ percentage points. By comparison, in the early 1990s
recession output fell by around 2% peak-to-trough, while the unemployment rate increased by over
2.5 percentage points.
Chart 15: Forecasts for unemployment
Sources: ONS, Bank of England and Bank of England calculations.
0.0%
0.1%
0.2%
0.3%
0.4%
0.5%
0.6%
0.7%
0.8%
0.9%
1.0%
0%
1%
2%
3%
4%
5%
6%
7%
Small firms (< 50 workers) - LHS
Medium firms (50-499 workers) - RHS
Large firms (500+ workers) - RHS
0.0%
0.5%
1.0%
1.5%
2.0%
2.5% Small firms (< 50 workers)
Medium firms (50-499 workers)
Large firms (500+ workers)
0
2
4
6
8
10
12
2005 2007 2009 2011 2013 2015 2017 2019
February 2009 February 2010 Actual
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
Put differently, in the immediate aftermath of the global financial crisis, unemployment rose by less than was
expected by most mainstream forecasters, including the Bank. Chart 15 shows Bank forecasts for
unemployment in early-2009 and early-2010. Having expected unemployment to peak at over 10.5%, the
eventual peak of 8.5% was reached in late-2011.
More recently, the “Okun’s Law” relationship appears to have steepened as weak productivity growth since
the crisis has seen unemployment fall sharply since 2012 despite more modest GDP growth (Chart 16).
Chart 16: Okun’s Law
Sources: ONS and Bank of England calculations.
Notes: Chart shows calendar year GDP growth plotted against annual percentage point change in calendar year average
unemployment rate.
A different way still of illustrating, and explaining, these developments comes from looking at job moves. As
the crisis struck, job-to-job movements fell. Instead of moving between jobs, workers instead transitioned
into unemployment. But these transition rates into unemployment rose by less than in previous recessions.
Workers and their employers bargained to remain on the jobs ladder rather than jumping-off.
Remaining on the ladder did, however, come at a cost in terms of pay. For workers staying in the same job
this meant accepting pay freezes, with the fraction of workers accepting these rising sharply during the crisis
(Chart 17a). For workers moving, this meant moving down the jobs ladder by accepting lower-paying jobs at
lower-productivity firms: the fraction of switchers taking pay cuts spiked during the crisis (Chart 17b).
Median wage growth for switchers fell from over 9% in 2007 to 4 ½% by 2010 (Chart 18).
-3
-2
-1
0
1
2
3
4
-6 -4 -2 0 2 4 6 8
1972-2009 2010-2018
Annual change in
unemployment rate
(percentage points)
Calendar year GDP growth (per cent)
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
At the time of the global financial crisis, then, workers and their employers were faced with a stark outcome
of job insecurity (jumping off the ladder) or income insecurity (stepping down the ladder). Unlike in previous
recessions, most chose the latter path. The result was lower-than-expected unemployment, but weaker pay
and productivity growth.
Charts 17a and 17b: Proportion of workers facing nominal pay freezes or cuts
Job stayers Job switchers
Sources: ONS ASHE and Bank of England calculations.
Notes: Chart shows percentage of workers either in current job or moving job who faced nominal pay cuts or freezes in a given year.
0
5
10
15
20
25
30
35
40
1990 1994 1998 2002 2006 2010 2014 2018
Cut Freeze
Per cent
0
5
10
15
20
25
30
35
40
1990 1994 1998 2002 2006 2010 2014 2018
Cut Freeze
Per cent
Chart 18: Wage growth for job movers
Sources: ONS ASHE and Bank of England calculations.
Notes: Median hourly pay.
0
2
4
6
8
10
12
1998 2001 2004 2007 2010 2013 2016
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
(b) Climbing the Jobs Ladder
As the economy recovered after the crisis, so did aggregate jobs growth. In response, workers began to
climb the jobs ladder, with transition rates between jobs rising. We would expect this to have then boosted
aggregate pay and productivity, as workers rotated into higher-paying, more productive firms. In practice,
this process appears to have been sluggish for pay and to have stalled for productivity. Why?
There appear to have been two effects at play in slowing the rate of climb up the jobs ladder. First, although
the rate of jobs transition picked up, it did so only slowly, especially given the degree of underlying strength
in the jobs market. Despite picking up, the velocity of circulation in the labour market has remained relatively
subdued in the face of the biggest jobs bonanza since the Second World War.
Job-to-job transition rates have only just reached their pre-crisis levels after seven years (Chart 13). And
that still leaves them around 25-30% below their peaks in the 1970s and 1980s. This relatively subdued
pattern is even more striking if we compare transition rates with the degree of tightness in the labour market,
defined by the V/U ratio (Chart 19). This suggests job-to-job transitions have not really picked-up at all over
recent years, when measured relative to the strength of the jobs market.
Chart 19: Ratio of job-to-job transition rate to V/U
Sources: ONS, Bank of England Millennium of Data and Bank of England calculations.
What explains these subdued rates of job transition? A strong candidate is that 21st century scourge, income
insecurity. The crisis scarred risk appetite. Risk-aversion discourages workers from moving to reach higher
rungs up the jobs ladder.
19
 It also discourages companies from offering workers those higher-rung positions.
Risk scarring slows the velocity of circulation in the jobs market and hence pay and productivity growth.

19
 One rationale for risk-aversion in moving to higher paid jobs at a different firm could be so-called “last-in-first-out” whereby new
workers are mostly likely to lose their jobs in the event of redundancies.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
1990 1993 1996 1999 2002 2005 2008 2011 2014 2017
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
This risk-aversion has not been confined to those switching jobs; it has also affected those staying put. A
reluctance to move reduces these workers’ bargaining power, weighing on their pay. Put differently, if
companies are less fearful of workers being poached, for a given degree of the strength in the jobs market,
they will feel less need to pay-up to retain those workers.
There is evidence of this behaviour in the differential pay growth of stickers and switchers recently
(Chart 20). Switchers have consistently experienced higher rates of pay growth than stickers, as we would
expect. But it is striking how this differential has widened as the labour market has tightened over recent
years, with movers securing pay increases of 7-9%, while stickers’ pay growth has until recently flat-lined at
2-2½%.
Chart 20: Switchers’ and stayers’ pay growth
Sources: ONS and Bank of England calculations.
(c) Rungs on the Jobs Ladder
The second dynamic at work is that even those climbing the ladder have tended to do so at a less rapid pace
than in the past. Or, put differently, the rungs in the jobs ladder have been closer together. This, too, may
have reflected workers’ unwillingness to move occupation, industry or geography at a time of increased
income insecurity.
The Resolution Foundation has recently looked at rates of movement between UK regions.
20
 They find that
rates of geographic mobility – people taking a new job in a different region – has been lower since the crisis
than in the preceding period. Their analysis finds that this reduction in mobility is driven by the young and, in
particular, young graduates. Graduates under the age of 35 were close to 6 times more likely to move region
and employer than non-graduates in the 1990s but are now just 3 times more likely. And mobility has
decreased for both groups.

20
 Clarke (2017).
0
2
4
6
8
10
12
1998 2000 2002 2004 2006 2008 2010 2012 2014 2016 2018
Stayers
Switchers
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
Charts 21 and 22 look at rates of worker transition between different industries and occupations. These
moves are grouped into “rungs” on the ladder, where the rungs for the 9-rung occupation ladder are ordered
by the standard ONS classification, which itself reflects differences in skills used within each occupational
group. For the 21-rung industry ladder, the rungs are ordered by the mean hourly gross pay of workers
within each industry, across the whole sample period. The blue zone defines zero rung movements. The red
zone defines large upward rung movements and the grey zone large downwards movements.
Chart 21: Industry job ladder
Sources: ONS and Bank of England calculations.
Notes: Chart shows percentage of jobs moves based on extent of upward or downward movement along the industry job ladder.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1990 1995 2000 2005 2010 2015
 11+
 8-10
 4-7
 1-3
0
 -1 to -3
 -4 to -7
 -8 to -10
 -11+
All speeches are available online at www.bankofengland.co.uk/speeches
20
20
Chart 22: Occupation job ladder
Sources: ONS and Bank of England calculations.
Notes: Chart shows percentage of jobs moves based on extent of upward or downward movement along the occupational job ladder.
During the crisis, we would expect a larger number of moves down the industry and occupational ladder.
There is some evidence of that, with the lower tail (grey zone) fattening and the upper tail (red zone) thinning
a little. Thereafter, as the jobs market recovered, we would expect a fattening of the upper tail (red zone)
and a thinning of the lower (grey and blue) zones. There is no real evidence of that. The zone of small rung
movements has remained fairly static.
We get a similar story if we look at the company destination for jobs moves, ranked by productivity. We
would expect to see workers move down the productivity ladder during downturns and to rescale that ladder
during upturns. The first part of that story is borne out in the data. Rates of productivity gain from a job
switch fell from around 8% pre-crisis to around 3% afterwards. More surprising is that there has been no
recovery in these rates of productivity gain during the subsequent jobs recovery (Chart 23). The rungs in the
jobs ladder have had a smaller productivity spacing than in the past.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1990 1995 2000 2005 2010 2015
8
7
6
5
4
3
2
1
0
-1
-2
-3
-4
-5
-6
-7
-8
All speeches are available online at www.bankofengland.co.uk/speeches
21
21
Chart 23: Firm productivity difference when moving jobs
Sources: ONS and Bank of England calculations.
Notes: Chart shows percentage difference in productivity between new firm and old firm with the median workers moves between firms.
Summing up, then, a strong jobs recovery has not resulted in workers vigorously re-climbing the jobs ladder.
Both their willingness to climb, and the pace of this climb, has been subdued relative to the strength and
tightness of the jobs market. Post-crisis risk aversion, and changes in the nature of work, may have
increased insecurities and dimmed workers’ appetite for a vertiginous career climb – or indeed any climb.
The result has been subdued rates of pay and, in particular, productivity growth.
Just recently, some of these patterns in pay, if not productivity, have begun to reverse themselves. Nominal
pay growth has been going slowly through the gears over the past few years, from being in the “low 2s” three
years ago, to the “high 2s” 18 months ago to having a “3” in front of it now. Private regular pay growth is
currently running at 3.7% year on year.
The reasons for this rise can also be found, to some extent, in job dynamics. There is only modest evidence
that those moving position are climbing the ladder any faster. But it is notable that, as the labour market has
tightened and skill shortages have emerged, we have seen pay growth among stickers beginning to rise after
a long period of tranquillity. In other words, employers’ increased appetite to retain staff, and employees’
increased willingness to consider moving, appears now to be affecting pay.
This might also help explain why, although pay has picked up (as stickers are rewarded not to move), there
is little sign so far of productivity following suit (as job moves towards higher-productivity firms have not
picked-up). With productivity subdued alongside higher pay, growth in annual unit wage costs is now
running at elevated levels of over 2 ½% year on year.21
 That is in excess of levels consistent with meeting
the Bank’s inflation target and begs a question about the stance of monetary policy, to which I now turn.

21
 Bank of England estimate using monthly AWE, employment and monthly GVA.
0%
2%
4%
6%
8%
10%
12%
14%
16%
2004 2006 2008 2010 2012 2014 2016 2018
Firm productivity change
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
22
22
A Polarised Economy
At present, we are being fed a rich daily diet of news about polarisation in our politics and societies, globally
and nationally. Perhaps it should come as no surprise, then, that there are also signs of polarity when it
comes to the prospective fortunes of our economies, globally and nationally. Certainly, downside risks to the
global and UK economy have increased through this year.
At the global level, a key current source of risk is trade. For a 60-year period after the Second World War,
barriers to trade were significantly and systematically dismantled, courtesy of multilateral trade liberalisation
initiatives. Almost 300 global trade agreements were reached between 1950 and 2018 (Chart 24). As a
result, global tariffs have fallen by two-thirds since the early 1990s (Chart 25).22
The dismantling of trade barriers ushered in a halcyon period of rising trade peace and prosperity. Global
goods trade volumes grew, on average, by 10% per year between 1950 and 2008. Global supply chains
lengthened and deepened.
23
 A post-War consensus forged global cooperation in trade, finance and security,
embedded and embodied in the Bretton Woods institutions which this year celebrate their 75th birthday.
Chart 24: Cumulative WTO-notified regional trade agreements in force
Source: WTO Regional Trade Agreement Database.

22
 Some of the observed decline in tariffs is also attributable to the General System of Preferences (GSP) and other unilateral
preferential treatment schemes.
23
 Baldwin (2016).
0
50
100
150
200
250
300
350
1948 1958 1968 1978 1988 1998 2008 2018
All speeches are available online at www.bankofengland.co.uk/speeches
23
23
Chart 25: Global tariffs
Source: UNCTAD TRAINS from World Bank WITS.
Notes: Weighted global mean of all applied tariffs across all products.
Recently, that tide of cooperation, including on trade, has shown signs of turning – what Mark Carney
recently called a “sea change”.
24
 After a prolonged period of peace, trade wars are now being waged: tariffs
have been introduced on over $200 billion of bilateral goods trade between the US and China; further
US-China tariffs are threatened; and there are ongoing trade tensions between the US and Mexico and the
US and Europe. These developments are reverberating around global financial markets and among
companies.
Among global investors, trade wars have risen to the top of their worry lists. Negative trade news has
weighed on global equity and corporate bond prices and government bond yields, as investors have sought
safety. Chart 26 shows the responsiveness of a selection of global asset prices to (positive and negative)
trade news. Asset prices have been particularly responsive to trade news during 2019.

24
 Carney (2019).
0
1
2
3
4
5
6
7
8
9
1988 1992 1996 2000 2004 2008 2012 2016
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
24
24
Chart 26: Asset price responsiveness to trade news
Sources: Bloomberg LP and Bank of England calculations.
Trade tensions are also taking their toll on companies’ sentiment and activity. Global goods trade has
slowed (Chart 27). But the weakening has been even more striking in business surveys of sentiment,
manufacturing and export orders. This weakening has been broadly-based across the world’s major trading
economies, with simultaneous falls in sentiment across the United States, Asia, Europe and China.
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
US Dollar ERI CNY
2018 - Positive news Per cent
2019 - Positive news
2018 - Negative news
2019 - Negative news
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
S&P 500 Shanghai
Composite
VIX
Per cent,
pp for VIX
2018 - Positive news
2019 - Positive news
2018 - Negative news
2019 - Negative news
-8
-6
-4
-2
0
2
4
6
8
10-year US Treasury yields High-Yield US corporate credit
spreads
bps
2018 - Positive news
2019 - Positive news
2018 - Negative news
2019 - Negative news
All speeches are available online at www.bankofengland.co.uk/speeches
25
25
Chart 27: Global activity indicators
Sources: IHS Markit, CPB and Bank of England calculations.
It is notable that movements in global asset prices and companies’ expectations appear to have been large
relative to the modest announced tariff increases. Based on the higher trade tariffs announced so far, and
using standard trade models, we would expect a negative impact on global growth of a few tens of basis
points.25
 The adjustments in asset prices and business survey responses have been multiples of that.
One way of beginning to make sense of this puzzle comes from casting our minds back a decade. Back
then, we also saw outsized responses in global asset and output markets following a modestly-sized shock –
a collapse in the US sub-prime market which at the time accounted for less than 1% of global wealth. This
market’s collapse prompted the sharpest fall in global output and asset prices since the 1930s.
We now have a much better sense of why: closely inter-connected global markets, once disrupted by deep
uncertainty, can cause contagious collapses in confidence, output and asset prices as participants take
precautionary action through fight or flight.
26
 This is the economics of infectious diseases. The stronger the
connections, and the greater the uncertainty about them, the larger the potential scale of the epidemic.
A version of this contagion problem may be operating now. The network is the world trading system and
global supply chains rather than global interbank and security markets. And the uncertainty surrounds import
tariffs rather than sub-prime defaults. But the same precautionary responses, whether fight or flight, appears
to be in play, causing out-sized responses in asset prices and activity.

25
 Carney (2019).
26
 Haldane and May (2011), Allen and Gale (2000) and Watts (2002).
44
46
48
50
52
54
56
-5
-4
-3
-2
-1
0
1
2
3
4
5
2017 2018 2019
Index, 50 = no change Percentage change, 3 months on 3 months ago
CPB world IP
CPB world trade in goods
Global services PMI (lhs)
Global manufacturing new orders PMI (lhs)
Global manufacturing PMI (lhs)
All speeches are available online at www.bankofengland.co.uk/speeches
26
26
Chart 28 plots the global trade network on two dates – 1980 and 2018. The wiring of the global trading
system has become conspicuously more dense over that period, with the ratio of trade to GDP increasing by
50%. This increased connectivity in the world trading system has generated huge benefits. But that same
connectivity has also increased the system’s potential to transmit recent fear and loathing, as uncertainties
have clouded its future.
Chart 28: Global trade network in 1980 and 2018
1980 2018
Sources: IMF Direction of Trade Statistics and Bank of England calculations.
Notes: Line thickness is proportional to total goods trade between regions. Circle size is proportional to regions’ total goods trade with
the other regions shown in the chart. Data based on nominal US dollars.
These concerns about global trade, output and asset prices have had significant knock-on effects to global
financial conditions. Expectations of short-term interest rates in the major advanced economies have fallen
by around 60 basis points, and global bond yields by around 70 basis points, since the start of the year.
27

This leaves global financial conditions 0.9 standard deviations looser than at the end of last year (Chart 29).

27
 Global taken as simple average of UK, US and euro-area 3-year and 10-year forward rates, respectively.
All speeches are available online at www.bankofengland.co.uk/speeches
27
27
Chart 29: Global financial conditions index
Sources: Bloomberg Finance L.P., Eikon by Refinitiv and Bank calculations.
Notes: Financial conditions indices (FCIs) estimated for 43 economies using principal component analysis. The FCIs summarise
information from the following financial series: term spreads, interbank spreads, corporate spreads, sovereign spreads, long-term
interest rates, equity price returns, equity return volatility and relative financial market capitalisation. An increase in the index indicates a
tightening in conditions. Series shows the average of all country FCIs, weighted according to their shares in world GDP using the IMF’s
purchasing power parity (PPP) weights.
The UK economy has also been facing high and rising degrees of uncertainty about future trade, though the
source has been different – Brexit. According to our Decision Maker Panel, the number of companies
reporting Brexit as among their top three risks have risen from one-third in April 2018 to around 50% now.
Most recently, the perceived risk of a “no deal” Brexit has risen, with betting odds placing a roughly 30%
probability on that event, double the probability earlier this year.
With no precedent, it is very difficult to gauge the economic impact of a “no-deal” Brexit. One simple way of
doing so is simply to ask companies how they think it would affect them. This is something the Bank’s
regional Agents have been doing since late-2018. Chart 30 shows companies’ expected responses for
output, employment, investment and exports under “no deal/no transition” and “deal with transition” scenarios
using the combined results of four surveys from December 2018 to April 2019.
28
The expected responses to a “no deal” are negative, significantly so. Output is expected on average to fall
by 3 ½%, employment by 3% and investment by 1 ¼%. By contrast, expected responses to a deal scenario
are all positive. Looking at the different waves of the survey, the good news is the expected negative effects
of “no deal” were somewhat smaller in the later waves compared to the first in December 2018. That likely
reflects the significant increase in Brexit planning and preparation, by companies and the government, over
that period.

28
 Companies’ responses are translated to impacts using the assumptions described under Chart 30.
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
2011 2013 2015 2017 2019
Standard deviations from average
All speeches are available online at www.bankofengland.co.uk/speeches
28
28
Chart 30: Bank of England Agents’ Survey of impact of different Brexit scenarios
Sources: Bank of England and Bank calculations.
Notes: Chart shows aggregated results from surveys from December 2018 to April 2019.
Companies were asked ‘Relative to the last 12 months, what is the likely impact on the following for your business over the next year in
each scenario: a deal and transition period and no deal and no transition period?’ For each relevant business factor, respondents were
asked to choose between ‘Fall greater than 10%’; ‘-10 to -2%’; ‘Little change’; ‘+2 to +10%’ and ‘Rise greater than 10%’.
Responses weighted by employment. Half weight was given to the +/- 2-10% response and full weight was given to those that
responded 'Rise/fall greater than 10%'.
The approximate estimated change for output, employment, investment and exports are based on a simple calculation assuming a midpoint figure for each of the response categories. For “Fall greater than 10%” the mid-point was -15%; for “-10 to -2%” the mid-point was -
6%; for “Little change” the mid-point was 0%; for “2 to10%” the mid-point was +6%; for “Rise greater than 10%” the mid-point was
+15%.
The likely accuracy of these expected responses is extremely unclear. But “no deal” expectations are
already affecting financial markets. As the perceived probability of a “no deal” Brexit has risen, sterling has
weakened and UK short-term interest rates have shifted downward (Chart 31). Having expected rates to rise
modestly at the time of the May Inflation Report, financial markets now expect UK interest rates to fall over
the next year or so, before rising gradually.
-4
-3
-2
-1
0
1
2
3
Output Employment Investment in UK Exports
Deal and transition
No deal and no transition
Approximate estimated change (%)
All speeches are available online at www.bankofengland.co.uk/speeches
29
29
Chart 31: UK short rates
Sources: Bloomberg LP and Bank calculations.
Notes: May 2019 IR line refers to 15-day average curve used to condition the May Inflation Report projections.
This forward path of interest rates is, in practice, a rather peculiar one. It is likely to reflect a probabilityweighted mix of two quite different possible outcomes for the economy – a “deal” scenario in which the
economy grows and a “no deal” scenario in which some expect it could shrink. In that sense, the market
path of interest rates is not an accurate reflection of the most likely path of interest rates.
To illustrate, consider a simple thought-experiment. Assume, in the event of a “no-deal”, the economy
followed the course expected by companies in Chart 30. Assume also, in that event, that monetary policy
followed a simple Taylor Rule and that inflation was unaffected.
29
 These are clearly very stylised
assumptions. Nonetheless, on those assumptions, a Taylor Rule would suggest interest rates would need to
be cut sharply to their zero bound, to cushion the hit to growth.
If we do the same thought-experiment under the “deal” scenario, again using a simple Taylor Rule, we get
instead an upwards-sloped trajectory for expected interest rates, on average 25-50 basis points higher than
the slightly upward-sloped market path for interest rates prevailing in May. Clearly, these are two very
different interest rate trajectories, reflecting two quite different Brexit economic outcomes.
30


29
 In the event of a ‘deal’, upward pressure on demand would also likely be accompanied with a stronger exchange rate, so the ex-ante
impact on inflation is ambiguous.
30
 My MPC colleague, Jan Vlieghe, made a similar point in his recent speech (Vlieghe (2019)).
0
0.25
0.5
0.75
1
1.25
1.5
1.75
2
2014 2016 2018 2020 2022
Latest (cob 18 July) May 2019 IR
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
30
30
The market-implied path for interest rates is a probability-weighted mix of these two paths. With a 30%
probability of “no deal”, this expected path would have interest rates falling in the near term before rising
gently thereafter. Despite our scenarios being highly stylised and simplified, this expected path broadly
mirrors the current market yield curve.
In practice, as the MPC has emphasised, the actual path of interest rates in the event of either Brexit
outcome would never be automatic; it will depend on the response of demand, supply and the exchange
rate to these outcomes. This is impossible to predict in advance. If a “no deal” were to lead to a sharp fall in
sterling and a sharp rise in inflation expectations, it is not clear the MPC could cut interest rates, as the
market expects, if it was to meet its inflation mandate. The risks to rates in a “no deal” scenario are in that
sense two-sided, if not necessarily symmetric.
This bi-polarity in possible economic outcomes poses a dilemma for UK monetary policy. Monetary
policymakers are often cast as one-club golfers. In the current conjuncture, the problem is more that the
MPC does not know which of two quite different fairways it should be aiming at. Brexit uncertainty is not, by
itself, a reason for leaving interest rates on hold, as the two tightenings in monetary policy by the MPC since
the Brexit referendum demonstrate. Nonetheless, with the economic road ahead potentially forking, the case
for holding rates until the road becomes clearer is strong.
With Brexit uncertainty high, the global economy softening and other major central banks expected to loosen
monetary policy, it is not difficult to see why short-term interest rates have fallen and are now pricing a nearterm loosening of UK monetary policy. UK growth is expected to stall in the second quarter. Companies’
investment remains strikingly and significantly subdued. Business sentiment is weak. And UK growth for
the year as a whole, at less than 1.5%, is expected to lie below potential.
My personal view, though, is that I would be very cautious about considering a monetary policy loosening,
barring some sharp economic downturn.
First, despite some Brexit-related volatility quarter-to-quarter, underlying UK growth remains fairly steady, if
not spectacular, at a fraction below its cruising altitude. Consumer confidence and spending remain robust,
underpinned by a still-strong jobs market and rising real pay. The UK housing market may be bottoming-out.
When British consumers have more money in their pockets, it takes a lot to persuade them not to spend it.
Nothing so far has dissuaded them. And they are three-quarters of all spending in the economy.
Second, the starting position for both the economy and monetary policy need importantly to be borne in
mind. As best we can tell, there is little, if any, slack left in the UK economy. That is why pay is picking up at
pace. UK inflation, meanwhile, is already at target and the monetary stance remains accommodative, with
short-term real interest rates still negative, augmented by almost £½ trillion of asset purchases.
All speeches are available online at www.bankofengland.co.uk/speeches
31
31
Third, there is sometimes a tendency in the current environment to extrapolate from international experience,
or recent historical experience, to the current stance of UK monetary policy. That is dangerous when the
starting point for the UK economy, and the shocks to which it is potentially subject to, are very different to our
overseas counterparts or indeed the UK’s own recent past.
Since the start of the year, UK short-term interest rates have moved in lockstep with those in the major
economies, notably the US and the euro area. The correlation of daily changes between them has been
around 0.7, a little above its historical average. Common exposures to a slowing global trade cycle can
explain some, if not all, of this correlation. But what stands out, for me, are the differences as much as the
similarities between the UK, US and euro-area economies.
Chart 32 is a simple plot of unemployment against short-term real interest rates in the UK, US and euro area
over the past 20 years. You might expect a negative relationship between the two: the lower
unemployment, the smaller slack in the economy and the higher the level of real interest rates necessary to
prevent inflation rising. This is broadly what you see in the data.
Also shown in the chart are the current settings for the UK, US and euro area (the darker dots). The US has
low levels of unemployment and, courtesy of a progressive tightening of policy over recent years, a nowpositive – if still low – real interest rate. The euro area is in the opposite place, with a higher level of
unemployment and a still-negative short-term real rate.
Chart 32: Real interest rates and unemployment (1999 – latest)
Sources: Thomson Reuters Datastream, ONS and Bank of England calculations.
Notes: Real interest rates defined in simple terms as current short-term central bank policy rate minus headline CPI inflation rate.
0
2
4
6
8
10
12
14
-6 -4 -2 0 2 4 6 8
UK
US
Euro area
Unemployment rate (per cent)
Real interest rate (per cent)
US latest
Euro area
latest
UK latest
All speeches are available online at www.bankofengland.co.uk/speeches
32
32
The UK occupies an unusual, mid-Atlantic position. It has similar levels of real interest rates to those in the
euro area, at the same as having similar levels of unemployment to those in the US. Put differently, relative
to both the US and the euro area, and its historical past, the current UK monetary stance looks relatively
accommodative. For the same output gap and a higher rate of inflation, the UK’s monetary stance is
2 percentage points looser than in the US.
There may well be good grounds for these international differences – for example, differing levels of neutral
real rates and different natural unemployment rates. Nonetheless, the very different starting positions of the
economy and monetary policy should give us cause to pause before simply assuming central banks should
be moving synchronously, even when faced by a common global shock.
One reason why financial markets might be over-extrapolating international experience is because, at least
since the crisis, monetary policy internationally has borne most of the burden of supporting growth in
advanced economies. Over that period, central banks have often been “the only game in town”.31
I believe that game made economic sense. The crisis led to a large gap opening-up between aggregate
demand and supply in the economy. Pump-priming aggregate demand back towards aggregate supply,
through accommodative monetary policy, was the right policy response. And this monetary medicine, as
best we can tell, worked. In the UK and US, aggregate demand is now back in line with supply. And in the
euro area and Japan, the output gap has shrunk materially.
Today, however, the conjunctural starting position for most advanced economies is very different than a
decade ago. With the output gap closed in the UK, economic growth and pay rises in future can no longer
rely on monetary policy pump-priming demand. It will instead require a lasting expansion in the economy’s
supply potential, if capacity constraints are not to bite and inflationary pressures are not to snuff out pay
rises.
It is not just the economic starting point that differs, now versus a decade ago. So too do the potential
shocks. A decade ago, the world faced a sharp shock to aggregate demand courtesy of the global financial
crisis. This opened-up yawning output gaps and fed fears of the scourge of 1930s-style mass
unemployment returning. Monetary policy responded with a force never previously seen in history.
Today, the most likely shocks or downside risks to the economy are very different in their source and impact.
They arise from factors such as global trade wars and Brexit. These shocks are different in their duration –
more slow-burn than spontaneous conflagration. And they are radically different in their source, as they hit
the supply potential of the economy, as much as its demand side.

31
 El-Erian (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
33
33
Consider the effects of Brexit uncertainty on the UK economy. By far the largest effects have been on
companies’ investment and productivity. According to our Decision Maker Panel, investment among UK
companies is estimated to be between 6-14% lower, and firms’ Total Factor Productivity (TFP) around 2-5%
lower.32
 These are large negative shocks to the economy’s supply-side potential. They are hits that
monetary policy is not itself well-placed to mitigate.
There is a risk of people normalising the deviance in monetary policy since the crisis. Around a quarter of
adults in the UK working population have never experienced Bank Rate above 1% in their adult lives.
33
 In
these changed circumstances compared with a decade ago, it is important that monetary policy is not a
prisoner of its past, that the monetary cavalry are not called at the first whiff of grapeshot, that a dependency
culture around monetary policy is not allowed to develop.
A decade ago, central banks were the only game in town and monetary medicine was the right prescription.
A decade on, the game has changed and so too might the policy prescription needed in dealing with any
downturn. When the challenges, as well as the huge opportunities, lie on the supply-side of the economy,
the right medical prescription is fiscal and structural policies. Super-charging the supply-side of the economy
is what is now needed. That is every bit as true here in Scunthorpe as it is elsewhere across the UK.

I am delighted to be here to discuss the final report from the RSA Citizens’ Economic Council on “Building a
Public Culture of Economics”. This is the culmination of a two-year project, working with around 250 citizens
and a number of organisations from across the country, including the Bank of England. I would like to
congratulate Matthew Taylor, Chief Executive of the RSA, and Reema Patel, Kayshani Gibbon and
Tony Greenham from the RSA as leaders of the project, for their terrific work in pulling this together. It is an
impressive, progressive and ambitious agenda for the economy and for economic policymakers like me.
Of course, citizens do already have a crucial stake in the economy. As economists we often divide the world
into neat institutional packages – companies, governments, trade unions, charities. While useful
organisationally, these names are no more veils, part legal, part social. Behind the veil lies the ultimate
owner – citizens, whether as workers, customers, shareholders, stakeholders or voters. Institutions are
executive gatekeepers of these citizen democracies. Ultimately, it is citizens to whom policymakers,
economic and otherwise, are accountable. Those of us in the policy game need never to lose sight of that.
Despite those ownership rights, it is nonetheless the RSA’s contention that citizens are not at present
engaged in the economy, and in economic policy, as fully as would be desirable. They make the case for
moving to a deliberative democracy, one in which citizens have a greater say on matters economic and
financial. This is a big contention. Nonetheless it is one that I – as a citizen and as someone who works for
one of those institutional veils, the Bank of England – fully accept. More than that, the Bank is making
strides towards increasing citizen engagement in the economy and understanding of economic policy.
In what follows, let me first say a word about the aims and objectives of a programme of citizen engagement,
the like of which the RSA are proposing – the “ends”. I will then turn to the “means” – what practically could
be done, and indeed is being done, to improve citizen engagement with the economy and economic policy.
For reasons of familiarity, I will focus on Bank of England initiatives as one example of such public
engagement, although the RSA’s recommendations cover a range of other policymaking functions.
The Ends
So what could we hope to achieve by increasing citizens’ engagement with issues of economics and
finance? My short answer to that question is a great deal. Economics, and economic policy, faces a “twin
deficits” problem – a deficit of public understanding and a deficit of public trust.
1
 Twin deficits, be they fiscal
and external or trust and understanding, carry dangers for the economy and for economic policy. Closing
them would, I believe, deliver significant benefits to society, to individuals and to policymakers who sit twixt
the two. Here’s why.
Let’s take public understanding of the economy. As the public themselves would be the first to admit, this is
not high and not as high as we would wish. On economic and financial matters, there is often a yawning

1
 Haldane (2017a).
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
understanding deficit on the part of the general public. That is not because the public are yawning at the
prospect of engaging with these issues. To the contrary, surveys regularly show that a significant majority
recognise the importance of economic and financial issues to their everyday lives. Around 80-90% say they
would like economic and financial issues to be covered more comprehensively in the school curriculum.2
It would be wrong, however, to think that public engagement only benefits the public. Just as important are
the improvements to policymakers’ understanding of everyday economic issues. There is now a wide body
of evidence, drawn from a wide variety of settings, on the benefits of combining the insights from expert and
non-expert sources when making sense of a complex world, when forecasting an uncertain future.3

Harvesting insights from a non-expert audience appears, in these uncertain settings, to be especially
valuable by providing diverse perspectives and different insights.
If we turn from understanding to trust, the picture has many similarities. Trust lies at the heart of the social
contract between institutions and societies, between policymakers and citizens. But the very nature of trust
appears to have changed rather fundamentally over the recent past. Societies and individuals have
undergone what Rachel Botsman in her recent book calls a “trust shift”.4
 Where once trust was
institutionalised and anonymised, today it is socialised and personalised.
This shift in the gears of trust has had consequences. Public institutions of all stripes have been one of the
casualties, with measured trust having fallen steadily over the recent past.5
 Most central banks have not
escaped these shifting sands, in particular since the global financial crisis. For them and us, the age of
innocence, or at least the era of taking public trust as given, may be over.
That matters. Credibility and trust are the secret sauce of central banking. They are what enable them to
shape expectations in ways which stabilise the economy and financial system. An economy or financial
system without trust, history tells us, ceases to be a system. An economic or financial policymaker without
credibility, history tells us, ceases to be a functioning policymaker. Removal of the veil, breaches of the
social contract, leaves the economy and financial system vulnerable.
There is no single measure, nor quick fix, for closing these twin deficits with the general public. The supply
chain for secret sauce, of building and restoring trust and understanding, is long and arduous. Nonetheless,
it seems clear that increased engagement between policymakers and wider society is a necessary ingredient
for its manufacture. From other spheres, such as medicine, we know that trust and understanding are built
on everyday engagement. For the Bank of England, with the scars of the financial crisis still healing, the
case for continuing to build those foundations is strong.

2
 Haldane (2017a). The RSA call these benefits improved “agency” among citizens.
3
 Tetlock and Gardner (2016). The RSA call these benefits improved “efficacy” of policymaking.
4
 Botsman (2017).
5
 See, for example, the Edelman Trust Barometer. The RSA call these benefits improved “legitimacy” of policymaking.
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
The Means
If the case is strong, how best is this citizen engagement to be achieved? The RSA report provides a nice
framework for addressing this question. This is based on the “ladder of participation” model, devised
originally by Sherry Arnstein in the late 1960s.6
 The rungs of the engagement ladder are, in ascending
order: inform; consult; involve; collaborate; and empower. As we move up the ladder, we increase the
degree of citizen engagement in decisions that affect them. The RSA stop short of recommending the top
rung, but do propose various initiatives to increase citizen involvement and collaboration.
Armed with this framework, I have been thinking about how far the Bank of England has so far climbed the
citizen engagement ladder. Truth be told, I suspect for the first 300 years of its life, the Bank of England
scarcely had its foot on the first rung. It saw little need even to inform the wider public about the economy
and policy. Trust among the public was anonymised, institutionalised and given. When tempted to step up a
rung, the Bank quickly contracted an acute form of vertigo.
Over the past 25 years the Bank has taken significant steps up the engagement ladder, in part in response
to the public’s trust shift, in part reflecting the Bank’s wider policy responsibilities. These steps have
transformed how the Bank “informs” and “consults” the public on its views and policies.7
 One simple
diagnostic is the rise in the Bank’s publication count. Seventy years ago, the Bank published precisely one
speech a year. Today, rarely a day passes without multiple publications. Then, the Bank uttered publically
fewer than 5,000 words per year. Last year, it uttered in excess of 4 ½ million.
Over the past two decades, it would not be an exaggeration to say there has been a genuine revolution in
central bank communications. Perhaps even more importantly, there has been a revolution in central bank
attitudes towards public engagement. This revolution has, with luck, increased the public’s understanding of
the economy and economic policy. But there is no question in my mind that it has also enhanced very
significantly central banks’ own understanding of the economy. Having ascended some rungs of the
engagement ladder, our panorama on the economy has been much enhanced.
Looking back over the past five years, and looking forward to the next five, the Bank is taking further steps
towards securing a foothold on the next rungs of the engagement ladder. This will increase the extent to
which we “involve” and “collaborate” with a wider set of citizens, as well as “informing” and “consulting” them.
From a potentially long list, let me highlight four examples of how we are going about that.

6
 Arnstein (1969).
7
 Haldane (2017b). A number of excellent historical examples are given in Kynaston (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
(a) Layered Communications
There is a natural temptation among technocrats to communicate in a language that is only understood by
fellow technocrats. This tendency has deep sociological roots, most probably dating back to our
hunter-gatherer past. At that time, language was a means of bonding and binding people; it was social glue.
It still is. Language can also, however, be a barrier to understanding and, as in our hunter-gather past, a
source of tribal conflict. These linguistic barriers apply everywhere and to everyone. But they seem to be
especially troublesome among the economic and financial tribe, which stand out relative to others in their
insularity.8

Central bankers are not immune to these primordial tendencies. They, too, sometimes use the tribal
language of technocracy. And this too can sometimes serve as a barrier to understanding and a blockage to
trust. My “stylised fact” (itself a piece of technical jargon) is that around 95% of all the words central banks
utter are inaccessible to around 95% of the population. (Imagine speaking 4 ½ million words and more than
4 ¼ million of them falling on deaf ears?) If you conduct a semantic analysis of central bank text, you will
find that I have erred on the side of generosity with these fractions.9
 When someone says to me “spoken like
a true central-banker”, they come to bury Andy not praise him.
But things are changing. The Bank is making a big effort to simplify and demystify its words and to challenge
its ideas and thinking. The Bank’s staff blog, Bank Underground, is one example of such challenge. Last
year, it recorded its millionth hit. Recently, the Bank has begun issuing its publications in a “layered” format,
to cater for different audiences: a one-sentence, one-graphic version for the time-poor; a 60-page
chart-tastic technical text for the masochist. As best we can tell, this has opened up the Bank’s views to a
far-wider audience. The number of website hits has increased markedly since publication of our first layered
Inflation Report in November last year.
Recently, we have tried to pinpoint more precisely the benefits of layered communication in boosting the
public’s understanding and trust. This involved conducting a so-called “randomised control trial” on two
groups – a group of experts (students studying economics at university) and a group of non-experts.10
 Both
groups were randomly split with some given layered, and others un-layered, Bank of England content from
the Inflation Report. This enabled a direct assessment of the impact of layering on understanding and trust.
It would have been a tad disappointing if layering had had no impact. And thank heavens for small mercies
that, for both the expert and non-expert groups, layering boosted reported trust and understanding scores.
What was more interesting was by how much layering seemed to help. In our sample, the boost to
understanding of the economy and monetary policy that came from layering the Inflation Report was larger

8
 Haldane and Turrell (2017).
9
 Haldane (2017b).
10
 Haldane and McMahon (forthcoming).
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
than the boost to understanding that came from having studied for an advanced degree in economics.
That’s a depressing statistic for those of us who have studied economics, but great news for everyone else.
(b) Educational Programme
A second strand of the Bank’s engagement strategy is education. You could see this initiative as helping to
meet citizens’ self-expressed desire for firmer educational foundations on all matters economic and financial.
To help dig the footings of that foundation, the Bank will be visiting around 200 schools this year to give talks
and lectures using its newly-enlisted ambassador army, some with the help of our partner, the charity
Speakers for Schools. The Bank also now runs two schools competitions, the most recent announced just
last week in partnership with the Financial Times.
11

The most significant of the Bank’s educational initiatives will take place in the classroom itself. We have
developed, in conjunction with We Are Futures, classroom materials for use in the Personal, Social, Health
and Economics (PSHE) component of the curriculum. We have involved and consulted teachers in
designing and refining these materials. They will be rolled-out in schools from 27 April. The Bank would like
to see PSHE and, within that, economics become a compulsory part of the curriculum, echoing the views of
parents and pupils. I have recently written to the Department of Education to say that.
12
(c) Regional Tours
A third strand is regional engagement. The Bank has had, for the past 90 years, a network of regional
Agencies dotted across the UK.13
 They are the Bank’s eyes and ears, playing a key role in collecting
intelligence on the economy and financial system, largely from companies. That intelligence feeds directly
into the Bank’s policy deliberations. Last year alone, the Bank’s Agents met with over 8,000 companies.
Recently, we extended the Agents’ reach to include charities, community groups, trades unions and wider
civil society. On the engagement ladder, these are examples of the Bank “involving” and “collaborating” with
citizens, on a practical day-to-day basis across all parts of the UK.
Last year, I extended my own regional programme, with a separate set of visits designed to engage with, and
listen to, some of the quieter voices in society, voices otherwise drowned-out in the media cacophony – the
voice of charities, community and voluntary groups, local councils, trade unions. And I have sought out
those voices in parts of the country that may not otherwise always get a hearing – the under-performing parts
of thriving cities as well as communities, towns and cities which are struggling. This regional focus on
engaging with the less well-performing parts of the UK is in line with another of the RSA’s recommendations.

11
 Details available at https://www.bankofengland.co.uk/news/2018/february/boe-and-ft-school-blog-competition
12
 See https://www.bankofengland.co.uk/news/2018/february/department-of-education-consultation-response?sf84498143=1
13 England, Hebden, Henderson and Pattie (2015).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
These visits use approaches to collecting intelligence which are, for me, new – listening methods more
familiar from anthropology and sociology than economics and statistics. They involve open, unframed
discussions of the issues that matter most to people in their everyday lives, their lived experiences in their
local communities, with me as a fly (not bluebottle) on the wall. Some of my findings – in the form of words,
photos and cartoons – are available at https://www.bankofengland.co.uk/outreach#.
I find that cartoons can be particularly effective, indeed much better than words, in capturing the sentiment
and narrative in the room. For example, consider this one on what the local community saw as their key
challenges and frustrations, taken from my visit to South Wales.
Or this one, on the role of charities in the community from my visit to Leicester.
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
Or this one, on housing and other issues facing young people from my recent visit to Blackpool.
These pictorial narratives provide a simple visual summary which, unlike the written word, can be digested
by anyone in less than a minute. They are also a very useful cross-check on my own understanding and a
check on my own cognitive biases. Often, I find that the message my non-economist scribe, David, takes
from these meetings is different than my own.14
 In these situations, I take David’s reading of the room as
probably the truer reflection of what citizens have really said.
I call these visits my Townhall Tours, though this is a rubbish description as I have yet to visit a town hall. I
am told that anthropologists would call what I am doing “deep hanging out”, after anthropologist
Clifford Geertz.
15
 Deep hanging out sounds more appealing than visiting town halls. Nonetheless, I am not
entirely convinced the Bank of England press release which announces “This is Andy deep hanging out in
Stoke” will scan. So I am open to crowd-sourced alternative suggestions.
Let me give one or two concrete examples of how this regional engagement has helped my own
understanding of the economy, its hidden corners, its quiet voices.
When UK GDP returned to its pre-crisis peak in 2014 economists told the world the economy had
“recovered”.16
 Around 30 seconds into a talk about the “recovery” with charities in Nottingham in 2016, I was
put straight. The language of recovery resonated with no-one in the room. This experience was repeated on
TV, some months later during the referendum debate, when a questioner in Newcastle put straight one of the
participants on tonight’s panel with the memorable line “That’s your bloody GDP, not ours”.

14
 http://www.visualscribing.com/
15
 Geertz (1998), with thanks to Grace Blakeley at the Institute for Public Policy Research.
16
 The date that real GDP returned to its pre-crisis peak has changed between different vintages of ONS data and was originally
thought to have occurred in 2014. In the latest vintage of ONS data, it was in 2013.
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
When I returned to my desk from Nottingham and plotted GDP regionally as well as nationally, something
surprising emerged from the numbers. In fact, GDP had passed its pre-crisis peak in only two regions of the
UK – London and the South-East. No other region in the UK had in fact experienced a “recovery”, whether
Nottinghamshire or anywhere else. My “recovery” narrative, while true at the national level, had rightly fallen
on sceptical ears at a regional level. Although the Bank’s policies only operate at a national level, this is
important context for how we and others communicate about the economy.
A second example came when I visited South Wales towards the end of last year. At the time, inflation was
picking up as a result of the fall in the pound after the referendum, taking it above its 2% target. But this was
a modest inflation overshoot by historical standards, with commensurately modest costs. Or so I thought.
But when people in South Wales described for me its everyday consequences, the costs seemed anything
but modest. Their personal inflation rates were running at closer to 10%. For some, that made for a stark
choice between “heating or eating”. This made very concrete for me the benefits of the inflation target and
monetary policy in supporting people’s everyday lives.
A third example would be the narratives – that word again – I have knitted together from my various regional
visits on the structural impediments to growth in communities, towns and cities across the UK. My deep
hanging out has, to date, unearthed, six such impediments – the “big six”. They are schooling, housing and
shopping, finance, jobs and transport. These are not problems that central banks’ tools can help solve. But
any structural plan for the economy worth its salt needs I think to have a song to sing about each of the big
six, not just in London and Leeds, Cardiff and Edinburgh, Belfast and Birmingham, but as importantly in
Sunderland and Stoke, Blackpool and Bradford, Boston and Orkney.
(d) Citizens Panels
Last but not least, there is a recommendation in today’s RSA report which is aimed explicitly at the Bank of
England. Building on the RSA’s own project, it is that the Bank set up its own set of regional citizen councils,
centred around our Agency network. The aim is to establish, in a structured, systematic and comprehensive
way, a two-way dialogue and collaboration between the Bank and a panel of citizen representatives on the
economy, financial system and policy, as a means of enhancing the understanding of both parties.
This is a bold recommendation. That language would usually preface an official like me lavishing praising on
this recommendation, before gently burying it. But not today. I am delighted to announce that the Bank
intends implementing in full the RSA’s recommendation. During the course of this year, we will begin the
process of setting-up regional citizen councils using our Agency network. This is a natural next step, and will
become a central plank, of the Bank’s engagement strategy in the years ahead.
This is, for the Bank, the next rung in our ascent of the engagement ladder. I am confident our view of the
economy, and our setting of economic policy, will be greatly enhanced by this wider panorama. It will give
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
the Bank a new lens on the economy’s hidden corners, a new set of often-quieter voices to listen to and
learn from. It is an example of what the Bank, and I hope others, will be doing much more of over the period
ahead: listening to a much wider audience, digesting their narratives, and then using this information to
support the economy and the citizen stakeholders that make it up.
All speeches are available online at www.bankofengland.co.uk/speeches
11
11

Terrible title, right? Not the “everyday” bit – that’s fine. It is the word “economics”. For most people, this is
an instant turn-off. “Economics” just sounds too conceptual, too academic, too impersonal, too dismal.
Stephen Hawking was once told that every equation he included in a book would halve its readership.
1
 The
word “economics” probably has a similar effect.
If I’d taken media studies rather than economics, I’d have gone about things differently. I’d have drawn you
in with a “hook”, such as people or children or dogs. Had my title been “Economics for the People”, I’d have
doubled my readership. Had it been “Economics for the Kids”, I’d have quadrupled it. And if I’d opted for “A
Fluffy Labrador Puppy Explains the Economy”, the Bank’s website would have crashed.2

It is too late now. And having decimated my readership in the first line I am now going to argue that having
done so was a terrible mistake. If economics or economic policy is elitist and inaccessible to most people, it
is not doing its job. That is because the economy and economic policy affects most people’s lives, every day
of their lives. More than that, an improved understanding of the economy and economic policy would
probably help many people when making everyday decisions, big and small.
You do not have to take my word for it. As it turns out, most people agree. Although many find economics
difficult to fathom, they fully accept its importance to them personally and societally. Indeed, most people
say they would like to know more about the economy to help them when making everyday decisions about
jobs and money and voting. There is a demand for better public understanding of the economy from the
public themselves.
What about the supply of this public understanding? The Bank of England is owned by, and run for the
benefit of, the public. It is a public institution supplying public goods – goods such as stable money and safe
banks. Indeed, this has been the Bank’s role for the whole of its almost 325-year life. Another of those
public goods is public understanding of the economy, the financial system and economic and financial policy.
In what follows, I wish to say a little about why an improved understanding of the economy and economic
policy matters, for the general public when making everyday decisions and for the Bank of England when
making policy decisions. I want to say a word, too, about what the Bank is doing to support public
engagement, public understanding and thus public policy, with a focus on engaging children in schools.
The Twin Deficits
Economists often talk, with some trepidation, about economies which face a “Twin Deficit” problem. For
example, the UK currently faces both fiscal and current account deficits running to several percentage points

1 Hawking (1988).
2 Kumar et al (2015) show that Google searches in the US for the word ‘puppies’ far exceed searches for macroeconomic variables,
such as GDP, inflation and unemployment.
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
of GDP. In economist circles, such Twin Deficits are usually the prompt for a pursing of lips, a sucking of
teeth and a sharp intake of breathe.
Yet economics may be facing its own “Twin Deficit” problem. These twin deficits are not unique to, but are
probably more acute in, economics. With the public at large, there is an understanding deficit and a trust
deficit in economics. Because a lack of trust inhibits understanding, and because a lack of understanding
contaminates trust, these Twin Deficits are inextricably entwined.
Starting with the understanding deficit, the most compelling evidence comes from the public themselves. A
survey in 2011 asked about the meaning of the words “inflation” and “GDP”, the two central concepts in
macroeconomics. Only 16% of the public could clearly define inflation, while only 10% could define GDP. 3

Other surveys have reached similar conclusions, sometimes with slightly higher fractions.4
This unfamiliarity with economic concepts extends to a lack of understanding of these concepts in practice.
For example, the Bank of England regularly surveys the general public to gauge their views on inflation.
When given a small number of options, less than a quarter of the public typically identify the correct range
within which the current inflation rate lies. More than 40% simply say that they do not know.5
If we turn from economics to finance, the picture is no better. In a recent survey, the UK’s Financial Conduct
Authority (FCA) found that almost half the public believed they had “low knowledge” of financial matters.6

Some surveys put the fraction of the public who are financially capable at only around 60%.7
 Fully two-thirds
of the public say they are confused when making decisions about money.8
 And the UK consistently performs
poorly in international league tables of financial literacy.9
This understanding deficit appears to extend to economic and financial policy. For example, fewer than one
in ten people know that the interest rate in the UK is set by the Bank’s Monetary Policy Committee (MPC).
10

Only around a quarter understand “quantitative easing” and fewer still “forward guidance”.11,12
 My own
experience is that the Bank is believed by many to be either an arm of government or a high-street bank.
If we turn from understanding to trust, much the same picture emerges. A survey of trust in the professions
earlier this year ranked economists second bottom, with only politicians lower.13
 Doctors and nurses ranked

3
 Quoted in Thompson (2016).
4 YouGov / Post Crash Economics Society Survey (2015).
5
 Bank of England / TNS Inflation Attitudes Survey (2017).
6
 Financial Conduct Authority (2017).
7
 Money Advice Service (2015).
8
FLY Project (2015).
9
 For example, see OECD/INFE International Survey of Adult Financial Literacy Competencies.
10 Bank of England / TNS Inflation Attitudes Survey (2017). It is worth noting that 29% cite the Bank of England in their response to this
question, so appear aware of the institution responsible for setting monetary policy.
11
 YouGov / Post Crash Economics Society Survey (2015).
12 Bank of England (2014).
13 YouGov (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
top. In surveys of the public, slightly more people tend to mistrust than trust the opinions of economists,
even on economic matters.14
 In other words, there is a trust deficit to match the understanding deficit.
That trust deficit also affects those setting economic policy, including central banks. Measured trust or
satisfaction in the European Central Bank, the Federal Reserve and the Bank of England fell after the global
financial crisis. It has yet to recover. This mirrors the fall in trust in institutions and experts generally.
Tellingly, the UK and US have suffered a greater erosion of institutional trust than most other countries.15

If the Twin Deficits in economics are wide, and possibly widening, what have been the root causes? They
seem to be two-sided. In part, the deficits reflect the how well (or, rather, badly) economic material is
conveyed to the general public. And in part they reflect how well (badly) the public are able to digest this
material. The public’s own views make clear the two-sided nature of the Twin Deficits problem.
A survey last year found that a significant majority of the public think that economics is inaccessible to them,
even when it is intermediated by the media or politicians.16
 Only 12% believed the opposite. Inaccessibility
problems are particularly acute when it comes to financial products. There are over 12 million people in the
UK who feel they do not have the information they need to choose wisely between insurance products.17
This inaccessibility problem extends to public bodies charged with safeguarding the economy and financial
system, like central banks. One way to gauge this is by studying the way these public bodies communicate
in public. By measuring the complexity of their public communications, we can infer how easy it is for the
public to make sense of what these public bodies are saying to them.
Central banks are one such body. The minutes and Inflation Report published regularly by the Bank’s
Monetary Policy Committee (MPC) are pitched at a linguistic level which means they are readily accessible
to only around 5% and 12%, respectively, of the UK population.18
 Minutes of the meetings of the United
States FOMC are pitched at a level which makes them accessible to only around 2-3% of the US population.
To some extent, this inaccessibility problem applies to all technical disciplines. Economics appears,
however, to have a particularly acute case. On many measures, economics is the most tribal of the social
sciences, judged by cross-disciplinary collaboration and citation. There are signs recently of improvement,
but from a low base.19
 Economics ranks high on the tribalism scale even by comparison with most natural
sciences.20
 Only theoretical physicists (with the probable exception of Stephen Hawking) come close.

14
 ING-Economics Network Survey of the Public’s Understanding of Economics (2017).
15
 Edelman (2017).
16
 YouGov / Rethinking Economics Survey (2016).
17
 Financial Conduct Authority (2017).
18
 Haldane (2017). See also Jost (2017) for an analysis of the Bank of England’s Inflation Attitudes Survey.
19
 Angrist, Azoulay, Ellison, Hill and Lu (2017).
20
 As measured, for example, by the degree of cross-citations (Haldane and Turrell (2017)).
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
The other side of this coin is the public’s level of understanding of economic and financial issues. This, by
the public’s own admission, is not as they would wish. Around two-thirds of the public say they have never
had any formal education and training in economics.21
 In practice, that fraction may be higher still. And
almost half of all people never discuss economic and financial matters, or do so no more than once a
month.22
Why This Matters
If economists are a remote and mistrusted tribe, and if the public feel their understanding of this tribe is
inadequate, does this matter? Yes. The reason we know this is because the general public say so.
In a recent UK survey, over 80% of people said that economics was relevant, or very relevant, to their
everyday lives.23
 Respondents expressed a clear desire to improve their understanding of economics, to
help them when managing money, voting or understanding the economy. Like economists, the general
public also think that the Twin Deficits are reason to purse lips, suck teeth and take sharp intakes of breath.
It is not difficult to see why. Imagine that, instead of economists plumbing the depths of the trust league
table, it was high-flying doctors. A loss of public trust and understanding in our health services would impose
significant costs on society. Fewer sick people would seek expert medical advice. Non-expert, alternative
medicine would rise in popularity. And there would be an increased risk of medical scare stories taking root.
There is plenty of evidence to suggest these costs can be considerable. Fifty years ago, there were almost
half a million suspected cases annually of measles in England and Wales. In the following year, a
vaccination for Measles, Mumps and Rubella (MMR) was introduced in the UK. Cases of measles fell
dramatically, by the mid-1990s reaching fewer than 10,000 per year.
In 1998, a widely-publicised article appeared in The Lancet linking the MMR vaccine to autism and bowel
disease.24
 News of these findings spread as contagiously as measles itself. In response, rates of MMR
vaccination, which had peaked at over 90%, fell to 80% by 2004.25
 The scare had socially, as well as
clinically, contagious consequences for public health, causing lower rates of vaccination against un-related
diseases. Only recently have rates of MMR vaccination returned to their late-1990s highs.
The 1998 MMR study has since been comprehensively discredited and the lead author struck off as a doctor.
Yet the costs of this episode are still being felt. Rates of measles infection began rising in the UK from the

21 ING-Economics Network Survey of the Public’s Understanding of Economics (2017).
22 YouGov / Post Crash Economics Society Survey (2015).
23
 ING-Economics Network Survey of the Public’s Understanding of Economics (2017).
24
 Wakefield et al (1998). The article has since been retracted.
25
 Public Health England, The National Archives.
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
early 2000s and only recently have cases peaked. In 2016, there were over 500 confirmed cases of
measles, five times higher than at the turn of the century.
If we turn from public health to economic and financial health, the costs of a lack of public trust and
understanding are in many respects quite similar: expert advice and services may not be taken-up, worse
alternatives may be sought out and contagious scare stories may be allowed to proliferate. Each of these
has societal costs which may be significant.
Financial exclusion (people excluded from access to core financial services and advice) remains pervasive in
the UK. There are almost 2 million people in the UK without a bank account.26
 In the most deprived parts
(including parts of London, Manchester, Middlesbrough, Liverpool, Bradford, Birmingham and Glasgow),
over a third of people do not have access to a bank account.27
 While not all of these problems can be
blamed on a misunderstanding and mistrust of finance, these are still depressing statistics.
Many more people suffer the consequences, not of financial exclusion, but financial illiteracy. Studies have
identified a range of ways in which this can adversely affect people’s financial health. Financial illiteracy is a
key contributor to excessive indebtedness.28
 It is associated with an increased incidence of default, including
in the run-up to the sub-prime crisis.29
 And illiteracy causes people to seek out high-cost, non-standard
sources of credit, the financial equivalent of alternative medicine.30
Debt, default and dependency on high-cost credit are also a recipe for another d – depression. Debt and
financial illiteracy affect physical and mental health.31
 Adults in debt are three times more likely than those
without to have a common mental health disorder.32
 Financial stress is associated with high levels of
personal anxiety, depression and alcohol-dependency - problems that tend to felt more acutely by people
with low levels of financial literacy.33
The link between debt, financial illiteracy and mental health is clearly two-way. In a survey by mental health
charity MIND, over 90% said that financial problems had worsened existing mental health problems, while
around two-thirds said mental health problems had contributed to their financial difficulties.34
 There is a
debt-cum-depression spiral. These statistics are, in every sense of the word, depressing.
A lack of trust or understanding of finance and the economy can have wider societal costs. Economic
mistrust and misunderstanding can spread contagiously across societies, much as happened after the MMR

26 House of Lords Select Committee on Financial Exclusion (2017).
27
 Resolution Foundation (2014).
28
 Lusardi and Tufano (2009).
29
 Gerardi, Goette and Meier (2010).
30
 Disney and Gathergood (2013), for example, find that borrowers with poor financial literacy hold higher shares of high cost credit.
31
 Richardson et al (2017).
32
 Earwicker (2016).
33
 Earwicker (2016).
34
 As quoted in Earwicker (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
vaccine scare. As then, this causes people to seek safety in ways which in fact increase risk, for them
individually and for societies collectively. The flames of this epidemiological fire are increasingly being
fanned by social media.
The most visible manifestation of these failures occurs at times of financial crisis, such as during a bank run.
These are contagious losses of trust. While it may seem individually rational to run, bank runs can be
collectively calamitous for societies as a whole, as during the global financial crisis and MMR episodes. As
in those episodes, contagion is often spread by social as well as financial channels. “Popular narratives”
emerge in crisis that propagate panic, tipping economies into recession.35
Robert Shiller has recently shown how past recessions and depressions have often generated pessimistic
popular narratives about the economy or financial system.36
 Economic expectations, like measles, can
follow epidemiological dynamics as pessimism spreads contagiously. This tipping point in expectations
provides a plausible account of the cliff-edge fall in GDP at the time of the Great Recession, the Great
Depression and several episodes in between.
These epidemics in expectations can be potent even during tranquil times. Sociologists have shown that
individuals’ everyday decisions are often shaped by friends’ and associates’ views and behaviours,
irrespective of fundamental factors. It is not just extreme optimism and pessimism, exuberance and
depression, fear and hubris that are contagious. So too are dieting, exercise and gym membership.37

That appears to be true of financial market dynamics too. Recent research using Facebook data has shown
that the views of (economically and geographically-distant) friends can materially change peoples’ views of
the housing market, independently of local factors and conditions.38
 This is a concrete example of social
media fanning economic and financial flames, even when the fire itself is small.
There are other, distinct, costs that arise from a loss of trust in the economy, financial system and the
institutions charged with safeguarding them. All businesses and policy institutions exist courtesy of a social
contract with society. That contract is maintained not by law but by trust. A loss of trust in institutions is
important because it is a breach of that social contract and signals an erosion of social capital.
That puts the loss of trust in institutions over recent years into sobering context. The associated breach of
social contract explains why the legitimacy of businesses and public institutions has been called into

35
 Akerlof and Shiller (2009).
36
 Shiller (2017).
37
 For example, Aral and Nicolaides (2017) find evidence that exercise is socially contagious. Christakis and Fowler (2007) find that
social networks can allow obesity to spread.
38
 Bailey et al (forthcoming).
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
question.39
 The accompanying loss of social capital could carry significant societal costs. The absence of
institutions endowed with such social capital has, historically at least, been the reason Why Nations Fail.
40
Recently, the Bank hosted a conference to mark 20 years of operational independence in monetary policy.41

Central bank independence is also part of a social contract with the public. The conference made clear that
there is nothing inviolable about that contract. Like any social contract, central bank independence needs to
be adapted and improved in response to shifting social norms and economic circumstance.
Correcting the Deficits
Engaging wider society is part of the process through which central banks can renew and strengthen that
social contract, in the face of new social norms and changed economic circumstance. How might that best
be achieved? Let me discuss three areas where the Bank of England has made progress recently and is
seeking to make further progress in the period ahead: communication; conversation; and education.
(a) Communication
When I first arrived at the Bank of England, I was told I would need to learn a new language. It was not one I
had come across previously. There was no book from which to learn, nor any recorded tapes. It seemed to
involve quite a bit of Latin, which put those of us from a state school at an instant disadvantage. You didn’t
have to wear a top hat and a tailcoat when writing it, but it helped. The language was called “Bankese”.
Despite its obscurity, the semantic principles of Bankese were easy to detect. The language was ruthlessly
consistent in tone and content, for fear of giving mixed messages. It was free from personal judgement, for
fear of calling into question the Bank’s judgement. It was free from emotion, for fear of questioning the
Bank’s objectivity. And it was free from split infinitives, for fear of the wrath of the Bank’s wordsworths.
This reign of semantic terror is, fortunately, now long gone. In its communications, the Bank no longer backs
a single horse, notable for its consistency, lack of emotion and technical prowess. Instead it has invested in
a diverse stable of styles and publications. The messages they deliver are now deliberately mixed, in tone
and content, to meet the mixed needs of a mixed audience. Let me give three recent examples.
The first is Bank Underground, the Bank’s staff blog. Bank Underground has levels of linguistic complexity
up to 4 years lower than the Inflation Report, making it accessible to up to 11 million more people, or around
a fifth of the adult population. Perhaps reflecting that, it recently exceeded 1 million hits. That is around 6 or
7 times the number of readers of the Inflation Report over the same period.

39
 Haldane (2015).
40
 Acemoglu and Robinson (2012).
41
 See http://www.bankofengland.co.uk/research/Pages/conferences/2017/28290917.aspx
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
A second is KnowledgeBank. This is an attempt to explain the economy and central bank policies using a
combination of simple prose, cartoons, videos, games and competitions. It draws on different messaging
conveyed through different media. Its level of linguistic complexity is 5 years lower than the Inflation Report,
making it accessible to around 14 million more people, or around a quarter of the adult population.
A third example is the Bank’s flagship publication, the Inflation Report. In November, the Bank for the first
time published the Report in three layers: Layer 3 was the usual “50 page, 50 chart” version; Layer 2 was
the “1-2 page, 1-2 chart” version; and Layer 1 was the “1 line, 1 chart” version. With a level of complexity
more than 6 years below Layer 3, Layers 1 and 2 in principle could reach an extra 21 million people.
Early evidence suggests this layering has helped in reaching a broader audience. Website hits on the
November Inflation Report were double the average from earlier quarters. These extra hits were entirely
accounted for by Layers 1 and 2, while Layer 3 hits were essentially unchanged. This suggests that layering
has enabled the MPC to reach a new, wider audience without cannibalising its existing audience.42

Producing simplified material, like layering, is neither a short cut nor an easy option. Nor is it simply an act of
communication. Message simplification is an act of policy. As such, simplified messages need to be agreed
and signed-off by policy committees – as were the Layer 1 and 2 messages in the Bank’s November Inflation
Report. They are another example of the Marshall McLuhan maxim of the medium becoming the message.43
If one feature of more effective communication with the general public is simplification, a second is
story-telling. For most people, stories stick in a way facts and figures simply cannot. Recent experimental
evidence has explored whether media narratives are able to stick in a way that can reshape public discourse,
public opinion and public behaviour.44
 It finds compelling evidence that stories do just that.
Some stories are, of course, more adhesive than others. I recently attended a conference on the merits of
“Odyssean” and “Delphic” forward guidance. The use of Greek mythology is an example of story-telling. But
are these stories sticky with the public? For most people, fictional goings-on in Ancient Greece several
millennia ago are not especially sticky. Even modern-day citizens of Delphi might find them a stretch.
The trick is in creating not just narratives but “popular narratives”, to use the language of George Akerlof and
Robert Shiller.45
 One effective way of doing so is draw on stories that link to people’s personal, local or
recent experience. This helps people to visualise the story which, in turn, helps make it stick, in much the
same way visualising a list of random items helps them stick in your memory.

42
 The MPC’s decision to raise Bank Rate in November may also of course have had a role to play in generating extra hits.
43
 MuLuhan (1964).
44
 King, Schneer and White (2017).
45
 See Shiller (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
The MPC’s policy actions in November were described as “taking its foot off the accelerator” to hold the car
within its “speed limit”. This was intended to convey the sense of monetary policy slowing the economy
slightly, towards its lower potential growth rate, while still propelling it forward overall. It was a visual
narrative. Because most people (from Derry to Doncaster, Dunfermline to Dunvant, Delphi to Delhi) drive
cars, it was a local and personal narrative too. The car metaphor was used extensively by UK media.
There is a balance to be struck here, of course, a point beyond which simplification and story-telling morphs
into dumbing-down and trivialisation. We need to recognise those limits. Equally, most of the policymaking
world is at present some distance from these limits. In the transition, I’d be comfortable erring on the side of
over-simplifying, rather than over-complicating, if policymakers and the public are to begin to forge a link.
Let me give a concrete example: Quantitative Easing or QE. When QE was first introduced in 2009, the
Bank made strenuous efforts to explain this new policy to the wider world. It published speeches and
articles. It even did a Townhall tour of the UK to explain its actions. The conversations with the public about
QE typically took the following form:
Q: “Is QE money-printing? And, if so, can I have some please?”
Bank: “No. And no.”
Q: “Why not?”
Bank: “QE is not printing money. It is the Bank of England, through its operations in financial markets,
purchasing government securities of a given duration, typically from non-bank financial institutions such as
life insurance and pension funds, whose accounts are credited with the proceeds such that the asset
purchases are financed through the creation of central bank reserves held on the accounts of commercial
banks at the Bank of England”.
And that explanation, it was hoped, would provide the general public with the confidence needed to
encourage them to spend. You can be the jury in deciding how well this might have been achieved.
There was a different explanation, and conversation, with the public that could have been had. It might have
gone something like this:
Q: “Andy, is QE money-printing?”
Andy: “Yes. And when you get some, make sure you spend it.”
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
I fully accept that this would be the wrong side of the over-simplifying line for some people. But if doing so
delivers the desired policy outcome – building public confidence to support spending – that may be the better
side of the line on which to err. Just saying.
(b) Conversation
At the Bank’s first Open Forum in 2015, Gillian Guy, Chief Executive of Citizens Advice, said something that
has stuck with me. In a panel discussion on the role of finance in society, all panellists agreed on the need
for improved public understanding of finance. Gillian added a crucial qualifier: improved understanding of
the public by those in finance was every bit as important as improved public understanding of finance.
I think this is bang on and not just in the field of finance. Listening is every bit as important as talking when it
comes to public policy. The most important reason for this is the most obvious – as policymakers, we stand
to learn something. Indeed, because the voices of non-experts are rarely heard and are likely to convey
different information than that available from experts, we potentially stand to learn a lot.
This is not just conjecture. Statistical studies, drawn from a great many different environments and involving
very different decisions, show it to be the case. Expert opinion is an essential starting point for good
decision-making. But only a starting point. The judgements of experts can be improved materially by
drawing on the views of non-experts. 46
 Doing so is the key to what Philip Tetlock calls “super-forecasting.”
More fundamentally, non-experts are us. By numbers, if not salaries, they represent almost all of society.
Non-experts do most of the spending and saving in the economy, shaping its growth. It is their wage and
inflation expectations that determine whether central banks meet their inflation targets. And it is their trust
that needs to be nurtured, and recaptured, if policy institutions are to maintain their legitimacy.
Yet extracting views on the economy from non-experts is not easy. It takes extra effort to engage with those
with whom you may be unfamiliar. Often they speak a different language and have different starting points.
Sometimes, for understandable reasons, economic matters are not especially interesting for them. The thing
that makes these views worth listening to is the very thing that makes them sometimes difficult to hear.
One way of listening to the public is through structured surveys of households. Central banks, including the
Bank of England, make extensive and increasing use of such surveys – for example, when gauging
households’ indebtedness, spending and confidence. They play an important diagnostic role when setting
policy. Surveys are not, however, without problems.

46
 Tetlock and Gardner (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
One of the most important is that they use pre-determined sets of questions which may be “framed” by
experts’ views. If so, survey responses are likely to offer a restricted, often distorted, window on the
non-expert world. Framed questions make for answers which conceal more than they reveal. One of the
reasons experts have been surprised by how non-experts have behaved recently is because they have been
asking closed questions or, worst of all, dismissing answers that do not fit their own view.
Avoiding this risk requires you to ask questions which are as open and unpolluted as possible by experts’
priors and prejudices. This comes from listening, rather than just hearing. It comes from engaging in
two-way conversation, from genuine dialogue not monologue. It comes from drawing on social skills every
bit as much as cognitive skills.
Other professions offer an object lesson here. Why are doctors top of the trust league table? Studies show
that what matters for doctors’ patient satisfaction scores is not just their clinical competence. Indeed, their
knowledge of the latest Lancet article is irrelevant. What matters every bit as much, or more, to patients’
trust in their doctor is their social skills, their listening skills, their personal touch, their bedside manner.47
Few people would want an economist at their bedside. An economist with strong social skills is one who
stares at your shoes, rather than their own, when engaged in polite conversation. And conversation does
not appear to come easily to the economics profession. A recent study of Twitter behaviour found that
economists were the least likely discipline to engage in general conversation, on topics such as Fluffy
Labradors.
48
 Their conversations are technical in nature and narrow in reach.
The Bank has recently re-oriented fundamentally its outreach programme to improve its engagement with
non-expert audiences, with a less technical focus and a longer reach. The Bank’s Agents around the UK
have for many years collected intelligence on the economy and financial system, largely from companies.
Recently, the Bank’s Agents have placed greater emphasis on meeting charities, community and faith
groups and trade unions, widening its non-expert contacts and expanding its intelligence pool.
Over a number of years now, I have been reorienting my own regional visits towards these groups. Earlier
this year, I augmented these regional visits with a “Townhall Tour”, partnering with charities, citizen and faith
groups to engage with new sets of people across the whole of the UK. These are, by design, listening
events. The main messages are captured in my blog and animations and photos from each of the events.49

The Townhalls have made clear to me the benefits of asking open questions, uncontaminated by priors and
prejudices. They have taught me a lot about the economic and financial issues that really matter to people,
only some of which are captured by economists’ theories and surveys. They have underlined for me the

47
 Whittaker et al (2017), for example, found that patients were willing to wait longer for an appointment with a doctor with good listening
skills.
48 Holmberg and Thelwall (2014).
49
 Available from the Bank’s website at http://www.bankofengland.co.uk/Pages/outreachevents.aspx
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
benefits of harnessing non-expert opinion when it comes to understanding the economy. And the Townhall
visits, while not making me a super-forecaster, have thereby hopefully made me a slightly less bad one.
These Townhall visits are also an eye-opener for anyone in any doubt about the importance of the economy
and economic policy to the everyday lives of most people. When someone tells you that a higher cost of
living is, for them, the choice between heating and eating, it makes everyday economics real. It makes the
MPC’s concerns about rising inflation concrete. It is the reason I went into public policy.
The Bank is not the only body seeking to engage the wider public when discussing economic issues. The
Royal Society for the Arts (RSA) has recently proposed setting up Citizen Economics Councils and
Reference Panels designed to engage the public in economic policy debates.50
 I strongly support initiatives
of this type. The Bank’s own recent initiatives are seeking to achieve many of the same objectives.
(c) Education
When it comes to improving education on economics and finance, the public’s views could scarcely be
clearer. In surveys, around 80% of UK citizens say they think economics should form a part of the
educational curriculum.51
 In the US, it is a remarkable 97%.52
 My own work in schools, and recent research
by the Bank, has confirmed encouragingly high levels of interest and enthusiasm among children and young
adults in better understanding the economy.
This is a wonderful endowment. If you look more closely at the statistics, it is easy to see why both adults
and children feel so strongly. It is also easy to see why we might, unless we are careful, squander that
endowment of public enthusiasm about understanding the economy. The risk lies, to some significant
degree, in the current structure of the educational system.
In many subjects that have an important bearing on everyday decision-making, the educational system
ensures broad capture during the early years. The aim is to give almost everyone the everyday essentials of
the subject. Literacy and numeracy are prime examples here, where there is universal coverage during the
early school years. This makes perfect sense in building the educational basics.
Take mathematics. This is compulsory for every student in the UK (and in many other countries), at least up
to age 16. The bottom of the educational pyramid is, in effect, population-wide. After that point, the
educational pyramid narrows quite sharply (Figure 1). Only 12% of the population studies maths between
the ages of 16-18. And less than 1% study it beyond age 18, at college or university. This degree of
specialisation also makes sense. Not everyone wants, or needs, to be Stephen Hawking.

50
 Patel and Gibbon (2017).
51 ING-Economics Network Survey of the Public’s Understanding of Economics (2017).
52
 Report prepared for the National Council on Economic Education by Harris Interactive (2005).
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
How well the UK educational systems provide students with even the essentials of numeracy is another
matter.53
 The statistics here tell an ugly story, with 17 million adults in the UK having numeracy skills no
better than those expected of a primary school child.54
 And it is a short hop from innumeracy to financial
illiteracy, where the figures are equally ugly.55
 Nonetheless, in coverage at least, maths education has a
broad base, as befits a subject needed by almost everyone almost every day of their lives.
Turning to economics, the educational pyramid could not be more different (Figure 2). It is more diamond
than pyramid. Economics is not a compulsory part of the school curriculum at any stage. Up to age 16,
economics forms only a very small part of PSHE (Personal, Social and Health Education), a voluntary part of
the curriculum. Surveys suggest that PSHE provision is patchy and has decreased significantly in recent
years.
56 Among early-years children, there is little foundation for learning everyday economics.57

Beyond 14, students at some schools can opt to study economics. In practice, only around a third of schools
offer this option. As a result, in 2016/17 only just over 9,000 students across the UK studied GCSE
economics, typically between the ages of 14 and 16. That is a tiny 1.4% of the student population in that age
range. These numbers have been flat over the past decade. More than a decade into their educational
lives, then, the vast majority of students have had virtually no exposure to economics and business.
Beyond age 16 the number of schools offering, and students studying, economics picks up - the pyramid
begins to widen. Around 33,000 students completed A-Level economics in the UK in 2016/17, typically
studying between the ages 16 to 18. Encouragingly, these numbers have swelled, by around 70%, over the
past ten years. That remains only 4.3% of the total number of young people in this age range.
Beyond 18, at university and college level, student numbers in economics have also been rising rapidly, with
the number graduating reaching 9,000 in 2016/17, a rise of over a third in the past six years ago. This has
increased the fraction of university-level students taking economics to over 2%, similar to subjects such as
politics and accountancy. It is still of course only a tiny 1% of the total population in this age range.
If we look in greater detail at just who is taking economics, the selectivity bias is more acute still. At ages 16
to 18, around 22% of economics students are drawn from independent schools, more than double their
population averages. At university, around a fifth of economics students are drawn from independent
schools. So not only are economics student numbers pretty small, they have a heavy socio-economic slant.
58
 The same may well be true of other subjects.

53
 National Numeracy (2017)
54
 See https://www.nationalnumeracy.org.uk/research-skills-life-survey-2011
55
 Money Advice Service (2017).
56 PSHE Association (2017), Practitioner Survey
57 Perhaps reflecting that, there is strong support among parents for PSHE having statutory status (PHSE Association, Parent Survey).
58
 There are also strong gender and ethnicity biases in the study of economics. There are discussed in, for example, Crawford, Davies
and Smith (forthcoming), Tenreyro (2017) and Blackaby and Frank (2000).
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
Economics also appears to have a heavy regional slant, probably for related reasons. The fraction of
students from London studying economics between ages 14 and 18 is roughly twice the national average. A
student from the North-East of England, where I was born, is around five times less likely to study economics
than one born in London. The fraction of the population of the North-East who is likely never to have had
any education in economics could be as high as 98%, similar to levels in Wales and Northern Ireland.
This is a “good news/bad news” story. There are encouraging signs that economics is gaining in popularity
in the upper tiers of the educational pyramid. But this pyramid is widest towards the top and narrow at the
bottom, with a pronounced (societal and regional) slant. Such a structure is unlikely to be a strong and
stable educational foundation for most people. Little wonder economics is perceived as remote and elitist.
Perhaps a reasonable question to ask – and even if it is unreasonable I am going to ask it anyway – is why
the essentials of the economy and finance are seen as so much less essential than literacy and numeracy.
Is financial literacy so different than general literacy, in its impact on people’s lives and everyday
decision-making, that it justifies such a different educational pyramid?
Gandhi is often claimed to have said: “Be the change you want to see in the world”. I love that quote.59

Through its public understanding role, there is change the Bank can effect in the area of economics
education. Recently, it has begun doing so. The Bank has been working with the organisation ‘We Are
Futures’ to develop material on the economy to be taught in schools to children aged 11-16. The aim is to
help create that missing foundation stone in the economics educational pyramid.60

The Bank’s programme is called EconoME. As its name suggests, EconoMe aims to link the economy and
financial system to people’s everyday lives; it is everyday economics. It seeks to give economics the same
characteristics as sticky stories – local, personal, topical. Trial materials are available for teachers now. The
programme will be fully launched in Spring 2018, with a state schools focus. Over time, we will seek to
broaden the schools coverage and develop materials for 16-18 year olds.
To help with this roll-out, the Bank is increasing significantly the numbers of school visits it undertakes, with a
target of 200 in 2018. We are working with the charities Speakers for Schools and Inspiring the Future to
achieve that objective, again with a focus on state schools.
61
 Importantly, these visits will not just include the
Bank’s top brass, but staff at every level. From personal experience, I know just how important and uplifting
(at least for the speaker) these visits can be.62 They are also the best media-training I have ever
encountered, with performance feedback that is instant and brutal.

59
 As discussed by Morton (2011), this quote is widely attributed to Gandhi but is likely a case of paraphrasing his actual words.
60
 Specifically, the PSHE component of the curriculum.
61
 https://www.speakers4schools.org
62
 These and other initiatives were discussed at the Future Forum hosted by the Bank in Liverpool a couple of weeks ago.
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
The Bank has for a long time run a schools competition, Target 2.0. This had many virtues, but diversity was
not one of them. In the 16 years the competition ran, independent schools accounted for 43% of the total
participants and 56% of the national champions – large multiples of their population share. The Bank’s new
film competition, “Bank, Camera, Action” hopes to level that playing field. In its first year, this attracted 106
entries, with 80% of participating schools from the state sector.
Conclusion
A few months ago, I was giving a talk at a school in one of the poorest towns in the UK – indeed, one of the
poorest in the EU. The school had a “Needs Improvement” rating from Ofsted. It had had 50% - that’s five,
zero – teacher turnover the previous year. The children attending were from poor backgrounds and had very
low academic and career aspirations. I knew this was going to be a tough gig.
I ran through my material – why the economy mattered to them, what the Bank did for a living, what I did for
a living etc. It was going about as well as I could have expected. There were plenty of attentive eyes and
alert expressions. No-one walked out. “This is working”, I told myself. 59 minutes into the one hour talk, I
took a final question from a girl on the far-right in the front row. It is seared on my soul.
“Two questions”, she said. “Who are you? And why are you here?” At that point, those questions seemed
all too appropriate. Why exactly was I there? Several hours of introspection (and therapy) later, I now have
an answer. The key comes in how you keep score. If in a classroom of 50 kids you reach only 1, what is
your score? Have you lost 49-1?
No. You have won 1-0. This is very important when it comes to judging the success of outreach initiatives,
including the Bank’s. Not engaging everyone is no reason not to try to engage someone. One is a good
start. It increases the chances of two tomorrow and three the day after. The power of social networks,
popular narratives and epidemics in expectations means you may be only one click from critical mass.
The Bank will not reach everyone, every day of their lives. Nor does it need to, given what we do for a living.
People have more important things to do in their lives than think about interest rates and bank regulation,
about price and financial stability. Or at least I hope they have. Indeed, central banks would be failing in
their job if people were spending too much of their lives thinking or worrying about these issues.
Nonetheless, the actions of central banks are important for most people at key decision points in their lives.
At those points, a trusting and understanding relationship between the central bank and the public matters.
It matters for people when making good decisions. It matters for the economy when following a good course.
It matters for policymakers when setting good policy. That is why everyday economics, terrible title and all,
matters to the Bank of England. 

I am delighted to be here to celebrate the 100th anniversary of the founding of the Bank of Estonia. It is a
particular privilege to be giving this lecture in the Bank’s “Independence Hall” – the very spot where, on
24 February 1918, Estonia’s Provisional Government was formed. The founding of the Bank of Estonia
followed on the Republic’s first birthday in 1919.
Reaching your first century is a true milestone for any person or institution. In the UK, when you reach your
100th birthday you receive a signed card of congratulations from the Queen. I am afraid I have no royal
birthday card for you today. But I have the next best thing – another speech from another central banker.
Times are tough in central banking. Central banks have borne much of the burden of supporting the global
economy as it has recovered from the global financial crisis. In some advanced economies, such as the UK
and US where the output gap has largely been closed, that extraordinary monetary support is now being
gradually withdrawn. In others, such as the euro area, a gradual withdrawal has been clearly signposted.
The gradual withdrawal of exceptional monetary stimulus is unlikely by itself, however, to remove entirely the
weight from central banks’ shoulders. Many have acquired new powers and new responsibilities since the
crisis, particularly in the regulatory sphere. At the same time, the crisis has dented trust in the financial
sector, including in central banks. In combination that has raised expectations, but also some doubts, about
the role of central banks in serving society.1
Central banks have, over the course of the recent past, undergone a revolution in their degree of
transparency and accountability. This has delivered huge gains. By improving public understanding, it has
helped stabilise the economy and the financial system. And by improving public accountability, it has allayed
concerns about central banks being over-powerful and under-accountable to the societies they serve.
But time has moved thing on and, as in the past, central banks will need to move with the times. In response
to greater levels of responsibility, but more fragile trust, I will argue a second revolution in central bank
practices may be needed. This would need to be every bit as radical as the first, but would require central
banks to engage with, and draw on, the general public – their “folk wisdom” – as never before.
Central banks were put on earth to serve the public. The second revolution is about putting the public more
firmly into public institutions. At a time of raised expectations and lowered trust, a reinforcement of central
banks’ social contract with society could make them even more effective in the period ahead, whether you
are the Bank of England (into its fourth century) or the Bank of Estonia (commencing its second).
Let me start by saying a word about the central bank revolution of the recent past, before turning to the
present and the future and the challenges and changes that may lie ahead in engaging the general public.
The Bank of England has already begun taking important steps towards increasing its engagement. Some

1
 For example, Tucker (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
of the steps taken by central banks may have relevance to other public institutions, many of which face
similar challenges of raised expectations and diminished trust among the general public.
A Central Bank Revolution
You do not typically associate central banks with revolution, unless the “r” is silent. Yet I do not think it is an
exaggeration to say that, over the course of recent decades, a revolution has swept through pretty much
every central bank on the planet. That revolution has come in the area of external communications,
openness and accountability.
It came neither quickly nor easily. It was only a generation ago that, when asked by Mervyn King for a
one-word piece of advice, Paul Volcker replied “mystique”. To that point, opacity rather than transparency
coursed through central bankers’ veins. That bloodline ran from Montagu Norman a century ago (“never
explain, never apologise”) through to Alan Greenspan a generation ago (“I’ve learned to mumble with great
incoherence”).
Historically, the case for central bank secrecy was deeply rooted. It rested in part on custom and practice, in
part on theology. Central banks historically were rarely subject to external scrutiny, at least by the general
public. And central bankers were generally perceived as the high priests of finance. It was said that central
bankers, like faith, were better understood by their deeds than by their words.2

That was just as well because central bankers rarely uttered any words at all. A century ago, public
utterances by the Governor of the Bank of England amounted to one after-dinner speech each year to a
well-watered set of City of London bankers. Heroically, attempts were made by some academics to
rationalise this monetary mystique as a necessary means of central banks exerting influence over the
economy.
3

Times have changed. The central bank secrecy doctrine has been over-turned, in part for conceptual
reasons, in part for practical ones. Within academia, economic theorists began constructing models where
policy credibility was absolutely central if people’s expectations were to remain anchored and the economy
kept stable.4
 Establishing credibility called for clear policy targets and policy actions.
Secrecy about policy actions, whatever its potential shorter-term benefits, came at a potentially significant
longer-term cost. Monetary policy “surprises” might provide a short-term jolt to the economy. These
surprises were especially likely when policy was in the hands of politicians, perhaps seeking re-election.
But these actions nurtured longer-term suspicions that policymakers might neglect inflation, causing inflation
expectations to rise and policy credibility to fall.

2
 Kynaston (2017) discusses this era of the Bank of England’s history.
3
 Brunner (1981).
4
 Kydland and Prescott (1977), Barro and Gordon (1983).
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
If so, monetary surprises could prove self-defeating even in stimulating the economy. They would instead
merely stoke expectations of higher inflation. In the academic jargon, an “inflation bias” would emerge.
5

This theoretical story found empirical support in the behaviour of a number of advanced economies during
the 1970s, who experienced a so-called “Great Inflation”.6
In part in response to the Great Inflation, a number of central banks began to be given new policy
responsibilities to reduce the potential for electorally-driven monetary surprises. Perhaps the most important
of those was operational independence for the setting of monetary policy. This was an institutional attempt
to avoid the temptation to stoke the economy in the short-term, and to anchor inflation expectations in the
longer-term, thereby building policy credibility.
With these new policy powers came, reasonably enough, new responsibilities for central banks. For the first
time in their histories, central banks needed to be far more open and transparent about their policy targets
and actions. Why? Because it was through greater transparency that policy suspicions among the public
could be allayed, thereby helping anchor inflation expectations in the economy.
As unelected technocrats acting on society’s behalf, operational independence also called for new means of
ensuring central banks were sufficiently accountable to society. Holding central banks’ feet to the flames
called for greater public transparency and greater public scrutiny than ever previously. Trust in central banks
could not be endowed by statute. It needed instead to be earned through experience and societal scrutiny.
These forces brought about a revolution in central bank practices regarding transparency and accountability.
Over the course of the past century, the number of Bank of England appearances before Parliament has
risen around 20-fold; the number of publications issued has risen around 600-fold; and the number of words
uttered has risen around 1,000-fold. You don’t need Tracy Chapman to tell you I’m talkin’ ‘bout a revolution.
Elsewhere around the world, the pattern has been much the same. We have seen a rising tide of speeches,
publications and parliamentary appearances by central banks. The Bank of Estonia has been at or close to
the frontier of this revolution. It was one of the first central banks to have a website, way back in 1995.7

Today it provides a rich daily diet of reports, forecasts, working papers, speeches and tweets.
The 20th century may yet be seen by historians as a Golden Age for central banks. Central bank numbers
rose from under 20 at the start of the century to over 175 by its close. Those with operational independence
for monetary policy rose from fewer than 40 in 1971 to over 150 today. And the numbers of central banks
with responsibilities for regulatory policy has risen sharply too, especially since the crisis.
8


5
 Barro and Gordon (1983).
6
 For example, Nelson (2005).
7
 Eesti Pank (1999).
8
 Haldane (2017a).
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
The rise in central bank numbers in part reflects the greater numbers of nation states in the 20th century. But
this, too, is revealing. Establishing a central bank has increasingly been seen as an essential building block
of nation state-building, a crucial element in the fabric of democratic statecraft. Indeed, that was precisely
the Estonian experience, as events in this very room a century ago testify.
The Age of Innocence
This Golden Age of central banking has persisted for a century. As best we can tell, it has served economies
and societies well. Prior to the crisis, the global economy entered a golden period of its own with steady
growth and stable inflation, known as the “Great Moderation”.9
 Mervyn King, Bank of England Governor at
the time, gave this period a different name – the NICE (Non-inflationary consistently-expansionary)
decade.
10
It is not difficult to see why. Growth in advanced economies averaged 2.5% and inflation 2.2%. The volatility
of inflation was around 80% lower, and the volatility of output 5% lower, than in the preceding two decades
(Chart 1). With the economy performing NICE-ly, public trust in central banks was running high. Approval
ratings for the Federal Reserve, European Central Bank and Bank of England were all at healthy levels.
The global financial crisis has ushered in a new era. Great Moderation gave way first to a Great Recession
and then a Grudging Recovery. Advanced economy growth over the past decade has averaged only 1.5%.
The volatility of inflation has risen by 40%, and the volatility of output growth by 60%, relative to the pre-crisis
decade (Chart 1). The NICE decade has given way to a VILE (Volatile-Inflation Less-Expansionary) one.
11

It should perhaps come as no surprise that one of the casualties of the crisis has been levels of public trust in
the financial system, banks and central banks. Measures of satisfaction or confidence in central banks fell in
the UK, US and euro area at the time of the global financial crisis.12
 They have yet fully to recover and may
reflect diminished trust. The situation in Japan is similar, except their crisis came sooner and the bust in trust
has lasted longer.
Explaining this increasingly fragile trust in central banks is, in part, a story of the crisis. Pre-crisis, central
banks were seen as stewards of the financial system. A failure of that system inevitably then caused some
questioning of central banks’ stewardship role. The financial crisis was the catalyst for some loss of faith in
the stewards in much the same way the expenses scandal was for British political stewards a few years ago.

9
 Stock and Watson (2003).
10
 King (2003).
11
 “VILE” was coined by my MPC colleague Michael Saunders (see, for example, Giles and Atkins (2008)).
12
 Haldane (2017a).
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
But there are also deeper-seated changes underway in the nature of the public’s trust, not specific to central
banks, what Rachel Botsman calls the “Trust Shift”.13
 Once upon a time, public trust was endowed on public
institutions rather than needing to be earned. Trust was anonymous and centralised. The public did not ask
difficult questions and public institutions, by and large, did not feel the need to provide answers. For public
institutions, this was the Age of Innocence.
As centralised and largely anonymous institutions, central banks surfed on this wave of blind trust during
their early years. During the Age of Innocence, there was no public expectation of openness or
accountability or need to build bridges with the public or parliaments. When quizzed for an explanation by a
Parliamentary Committee in 1930, Montagu Norman replied: “I don’t have reasons, I have instincts”.
But the public’s attitudes towards trust have altered and so too has the trust-building process. Faith in public
institutions is no longer blind. Trust is no longer endowed anonymously. Instead it is built through direct
connections, often personalised and localised. Trust is no longer centralised in institutions. Instead it is built
on a distributed basis, peer-by-peer, across the network. In short, the Age of Innocence is over.
As the public’s trust has shape-shifted, central banks and other public institutions have faced new challenges
to their trust. Indeed, this helps explain the first revolution in central bank transparency practices. But the
crisis has added cyclical push to this structural fall in trust. Central banks are no longer surfing a wave.
They face instead a retreating tide. For them, too, the Age of Innocence may be over.
While some loss of trust in central banks might have been expected, it is disappointing that the pre-crisis
revolution in transparency practices did not provide a greater degree of insulation against it. Why was
central banks’ stock of reputational capital, built steadily through various transparency initiatives, inadequate
to protect them from the crisis storm? I think there were two factors at play.
First, central banks have tools, such as interest rates and banking regulations, which support the whole of
the economy and financial system. They affect people’s lives greatly, supporting jobs and incomes, a stable
cost of living, an adequate supply of credit, confidence in money. But the impact of central banks’ tools is
often indirect and lagged; it is difficult for people to observe these benefits in their everyday lives.
When people get a job or a pay rise or a loan, they tend to attribute this to their own good luck or good
management. Rarely do they link it to actions taken by central banks, months or perhaps years earlier. The
benefits of central bank actions are, for most people, articles of faith rather than fact. These days, with trust
needing to be earned, this puts central banks at a natural disadvantage.
We see this pattern in other professions. In surveys, the general public are typically mistrustful of politicians
and bankers, who appear close to bottom of their trust league table. Yet their trust scores for local MPs and

13
 Botsman (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
local bank branch managers have remained high because of their closer connection with people’s lives.
Central banks, operating economy-wide, perhaps face some of the same challenges as bankers and
politicians.
The second reason trust proved fragile is because its foundation is public understanding. Once public trust
is lost, public understanding becomes crucial to restore it. The hope was that the revolution in central bank
transparency initiatives had provided that improved public understanding of the economy and central banks.
In practice, the evidence suggests progress towards improving the public’s understanding of the economy,
finance and central banks has been slow and patchy, despite these initiatives.
To start with the positives, there is concrete evidence of the improved central bank communications having
improved the functioning of financial markets, the forecasting performance of economists and even the
reporting of the media on the economy.
14
 The transparency revolution improved understanding among those
whose day job it was to understand the economy and the role of central banks within it – the experts.
The revolution appears to have had far less impact on understanding among the public – the non-experts. In
1999, the Bank of England began surveying the public to gauge their understanding of the economy and the
Bank.
15
 Back then, only around 7% identified the Bank’s Monetary Policy Committee (MPC) as responsible
for setting interest rates. 19 years and one transparency revolution later, the same survey earlier this year
suggested around 6% of the general public had heard of the MPC.16

The failure to boost understanding of the economy and central banks among the general public helps explain
why trust in central banks proved more fragile than expected after the crisis. It is for these reasons I have
spoken about central banks facing a “twin deficits” problem – a deficit of public understanding and a deficit of
public trust.
17
 These twin deficits are clearly complementary and mutually-reinforcing.
These twin deficits have not, so far at least, had especially serious consequences for central banks. There
has been no de-anchoring of inflation expectations. There has been no rolling-back of central bank
responsibilities nor systematic retreat from central bank operational independence on monetary and
regulatory matters. If anything, central banks’ sets of responsibilities have recently been added to.
At the same time, more questions are probably being asked about the role and powers of central bank than
at any time in a generation. That is in part a reflection of the increased responsibilities central banks have
been handed; in part a reflection of the higher expectations being placed on central banks’ actions; and in
part a reflection of the dent in trust in institutions the crisis has delivered. This is a triple challenge.

14
 For example, Gürkaynak, Sack and Swanson (2005), Gürkaynak, Levin and Swanson (2010), Berger, Ehrmann, and Fratzscher
(2011).
15
 Bank of England Inflation Attitudes Survey, currently run jointly with TNS.
16
 The same survey also gives the “Bank of England” as option to the question of which group sets interest rates. More respondents
identify the Bank of England as being responsible for setting interest rates (around 30%) than the Monetary Policy Committee, but this
too has been broadly flat over time.
17
 Haldane (2017b).
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
A year ago, the Bank of England hosted a conference to mark 20 years since it was granted operational
independence for monetary policy. At that event, I was struck by the number of speakers calling time on the
Golden Age of central banks, speculating on whether “peak central bank” had been reached. Some asked
openly whether central bank independence was appropriate, at least on financial stability matters.
I am not as pessimistic or as fatalistic as that. But I do think some of those concerns about understanding,
trust and legitimacy are real. In response to them, new efforts may be needed to close the understanding
and trust deficits and bridge the transparency and accountability gaps. Doing so would, I believe, leave
central banks even better-informed and better-placed to deliver their core mandates in future. Here is why.
Correcting the Deficits
My answer comes in two, inter-related, parts. Let me take these in turn.
First, building understanding among the general public in the economy, financial system and central banks.
Most people would think an improved understanding of the economy among the general public is desirable.
It makes for improved decision-making by people on important everyday decisions on spending, saving,
borrowing and working. The better those individual decisions (the micro), the more stable the economy and
financial system are likely to be (the macro). Improved public understanding of the economy is in that sense
win-win, benefitting policymakers and the public alike.
But it is possible the benefits of improved public understanding in stabilising the economy and financial
system have been underestimated. “Popular narratives” among the public might be more important in driving
macro-economic behaviour than in the past. And these narratives may be more prone to destabilisation than
in the past. If so, too little may have been invested historically, by both the public and policymakers, in
informing these narratives and hence stabilising the economy.
Public understanding of the economy and finance starts from a low base. You do not have to take my word
for it. That is the strongly-held view of the public themselves. In surveys, almost half of the public say they
believe they lack an adequate understanding of economic and financial issues when making crucial
everyday decisions.18
 This is a weak endowment.
We now know quite a bit about the costs to individuals of this weak endowment, which appear to be large
and long-lasting. For example, poor levels of financial education have been shown to have large and lasting

18
 FCA (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
effects on individuals’ health – financial, physical and mental.19
 Financial illiteracy is a recipe for debt, default
and depression, whose effects appear to feedback on each another in a vicious spiral.
These individual costs are amplified when they are aggregated up to the macro level. How people’s
expectations evolve – their degree of optimism or pessimism, exuberance or depression – is crucial for
determining their individual decisions. It has long been recognised that these expectations can be shaped
importantly by others’ expectations. For example, “popular narratives” can emerge which shape collective
expectations among the public – optimism or pessimism, exuberance or depression – and which can then
drive aggregate economic fluctuations.
20
Recent research has demonstrated just how powerful those emergent popular narratives can be,
economically and financially. As one example, the work of Nick Bloom and co-authors has constructed
various measures of businesses’ degree of uncertainty around key issues, based on the words used in
media reporting.
21
 These narrative-based measures of uncertainty have been found to be important for
explaining the investment behaviour of companies over time.
David Tuckett and co-authors have constructed measures of the popular narratives used by participants in
financial markets.
22
 They find that these narratives can play an important role in explaining asset price
dynamics, over and above the impact of macro-economic fundamentals. In a similar spirit, the work of
Michael Bailey and co-authors has found, using Facebook data, an important role for shared social
narratives in explaining behaviour in the housing market.
23

At a macroeconomic level, the work of George Akerlof and Robert Shiller has looked at the popular
narratives which emerge during periods of boom and bust.24
 Using words extracted from newspapers, they
find the prevailing popular narratives about the economy have played a significant role in accounting for the
heights of the peaks and depths of the troughs during macro-economic booms and busts. Public
expectations, embedded in the stories they tell, are a key macro-economic driver.
The common theme in all of this research is the importance of the public’s “narratives” in shaping their
behaviour. That is true individually but especially collectively – hence “popular narratives” – with shifts in
collective expectations helping account for the width and depth of macro-economic fluctuations. As well as
contributing to this research, the Bank has begun using these “narrative” approaches itself to help
understand behaviour. Let me give you a topical example.

19
 Richardson et al (2017), Earwicker (2016).
20
 Shiller (2017).
21
 Baker, Bloom and Davis (2016), Bloom (2009) and Bloom, Bond and Van Reenen (2007).
22
 For example, Tuckett (2011) and Tuckett and Nikolic (2017).
23
 Bailey et al (2016).
24
 Akerlof and Shiller (2009) and Shiller (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
The Bank’s Agents around the UK have a wide network of company contacts who they visit regularly to
discuss the economy. These visits are then written up. These write-ups enable a semantic search to be
carried out to identify the key themes or narratives emerging in companies’ conversations. You will be
unsurprised to hear that a key theme among companies over recent quarters has been uncertainty
surrounding Brexit.
Figure 1 plots a word cloud extracted from those company write-ups. The size of each word connotes the
frequency of its use in relation to mentions of “uncertainty”. For a word that did not exist as recently as five
years ago, it is striking how Brexit is now dominating companies’ conversations. Currently, it crops up 10
times more often than the word “customer” and almost 7 times more often than the word “staff” in proximity to
uncertainty.
25
Chart 2 plots a time-series of the incidence of the words “uncertainty” and “Brexit/referendum” in Agents’
company write-ups. The two, unsurprisingly, are positively correlated after the referendum. Indeed,
mentions of Brexit have outnumbered mentions of uncertainty by a factor of around 3 since the referendum.
While mentions of Brexit have fallen from their referendum peak, it is notable that they have picked up
sharply over the past few months, with measures of uncertainty following suit.
This new research strengthens the case for improving public understanding of the economy. An improved
understanding could result in better-informed popular narratives, and more stable expectations, among the
general. And that, ultimately, would make for a more stable economy and financial system.
In practice, there may be structural factors which could be acting in the opposite direction. The great trust
shift may mean people are less willing than in the past to trust expert opinion (such as through mainstream
media) and more willing to trust non-expert opinion (such as some parts of social media). Around half of
those in the EU use social media as a news source and, among those aged 18-24 in the UK, around a
quarter use it as their main news source.26
Yet we know that information from social media sources is typically filtered and personalised.
27
 That fits
people’s (personalised, localised) preferences. But it also results in news narratives which are more likely to
be self-reinforcing and self-referential than in the past. The echoes in this chamber are louder, reach further,
last longer. They are also less likely to be balanced and objective. More powerful, but less balanced,
popular narratives are a potentially destabilising influence on expectations and the economy.
This research points in one direction. The case for improved public understanding of the economy and
financial system has always been strong. But in a world where popular narratives are even more important

25
 This analysis uses company visit write-ups produced by the Bank’s Agents themselves, rather than contacts so is of course only a
proxy for the latter’s views.
26 Newman et al (2016).
27
 For example, Papacharissi (2002).
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
in driving economic behaviour, and where these narratives may be even more susceptible to destabilisation,
the case for improving public understanding is significantly reinforced. This is one blade of the scissors of
public engagement.
The second blade is building central banks’ understanding of the economy and financial system through the
general public.
This is newer, but I believe fertile, ground for central banks. Central banks pride themselves, rightly, on their
technocratic skills and economic expertise. There are limits to that knowledge, as there are to knowledge in
every domain and discipline. Those limits and uncertainties need importantly to be recognised when
understanding the economy and when setting policy.
Nonetheless, it would be a false step to dispense with that expertise. No one seriously questions the need
for heart surgeons or car mechanics, the need for specialist knowledge and experienced decision-making
when repairing hearts or car parts. Human bodies and car engines are complex environments. But so too is
the economic and financial system. It too needs specialist knowledge and experienced decision-makers.
It does not follow from this, however, that expertise cannot be augmented or improved on by drawing on as
wide a set of opinions as possible, expert and non-expert. Non-experts can provide a different lens on
problems and solutions. This diversity of thought has been found to be especially useful in complex decision
environments. Colloquially, that is why we call it “folk wisdom”. Back in 2004, James Surowiecki gave it
another name – the “wisdom of crowds”.28
 There are now many examples of collective wisdom at work.
The most celebrated is 19th century English statistician Francis Galton’s amazement at the accuracy of the
average guesses about the weight of an ox among visitors to a county fair. Less celebrated (though, as it
turns out, more accurate) was English magician Derren Brown’s attempt to predict the UK National Lottery
results in 2009 using the wisdom of crowds. The wisdom of crowds is not magic. Subsequent research has
found that it relies instead on two factors: diversity and deliberation.
To take these in turn, the value of diversity in complex decision-making is fairly well-understood. Diversity of
thought simultaneously increases the chances of creative solutions being found, while reducing the chances
of terrible solutions being adopted. It fattens the upper tail, and thins the lower tail, of the insight distribution.
Diversity delivers higher return for lower risk.29
These benefits arise, in part, from having a wider pool of ideas in which to fish and a deeper pool in which to
dilute risks. But they also arise from the collective benefits of having these new ideas and risks challenged
and filtered by a group. The benefits of diversity arise as much from interactions within the group, as from

28
 Surowiecki (2004).
29
 Haldane (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
the new ideas each person brings. But both are important for good ideas to emerge and for bad ideas to
submerge. And it is this which fattens returns and slims risks, whatever the decision.
These benefits have been demonstrated in a number of real-world settings. In his book The Difference,
Scott Page goes a step further with a “Diversity Trumps Ability” theorem.30 This states that the
highest-performing team is very unlikely to arise from bringing together the individually-best experts.
Instead it comes from combining insights from diverse sources, expert and non-expert. It comes from a
cocktail of experience, some specialist, some generalist.
The returns to diversity are not limitless. When choosing the optimal asset portfolio, most of the benefits of
diversification come from the first few assets. The same is true when the assets are people. The diversity
benefits in moving from 2 to 4 people are large compared with the diversity benefits of going from 102 to 104
people. There are probably diminishing returns to diversity.
The second factor important for the wisdom of crowds is deliberation. Complex decisions require an
investment of time, a period of deliberation. This is needed to understand a complex problem, consider it
from different angles, to test it with different solutions. Solving complex problems does not in general call for
instinctive, “fast thinking” of the type described by Daniel Kahneman in his famous book, Thinking Fast and
Slow.
31
 It instead calls for “slow thinking”, for System 2 rather than System 1 thinking.
Time is also important when harvesting the benefits of interactions within a diverse group. It takes time for
groups to explore and test their ideas, winnowing out the winners and filtering out the losers. It takes time for
people to change their minds. As with diversity, there are decreasing returns to deliberation time. Research
suggests that spending as little as 1 hour on a complex problem can improve performance by 10%.32
 These
benefits are likely to be smaller when moving from 100 to 101 hours spent on the same problem.
While diversity and deliberation are the cornerstones of the wisdom of crowds, it is clear that crowds are not
always and everywhere wise. I am rarely much the wiser about the parentage of a football referee for having
listened to the collective chants of the crowd. This observation is not a new one; it has a long historical
pedigree. In 1841, Charles Mackay published a now-famous book whose title stands in stark contrast to
Surowiecki’s – Popular Delusions and the Madness of Crowds.
33

Many subsequent authors have provided examples of collective irrationality in crowd decision-making. Its
causes have also been extensively studied by psychologists, sociologists and economists.34
 This originates
in the contagious spread of opinion through networks. The larger and more connected the network, and the
less well-informed its participants, the greater the chances of irrational epidemics spreading.

30
 Page (2008) and Hong and Page (2004).
31
 Kahneman (2012).
32
 Tetlock and Gardner (2015).
33
 Mackay (1841).
34
 For example, Janis (1972), Turner and Pratkanis (1998)
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
The behaviour underpinning this group psychology is as old as humankind. The desire to conform to group
type has its roots in our hunter-gatherer past. It has been found, time and again, in studies of human
behaviour from the South Sea Bubble to the dotcom bubble, from the Bay of Pigs to the space shuttle
disasters.
35
 This collective irrationality sometimes goes by the name “groupthink”.
More recently, these psychological roots may have been fed and watered by technological advances, such
as social media. This has increased our digital connectivity. For reasons set out earlier, it has probably also
increased the chances of self-reinforcing and self-referential waves of collective irrationality taking hold.
36
 If
so, these waves of collective irrationality could be becoming larger than ever. While a vestige of our
hunter-gatherer past, the madness of crowds can these days spread to a global village.
That leaves us with a conundrum. If decision-makers in complex environments were to draw on a wider set
of non-expert opinion, what are they likely to encounter? Is it a case of harnessing the wisdom of crowds or
avoiding their madness? Will greater public engagement help inform and improve expert decision-making by
widening the pool of lived experience? Or will it hinder that decision-making by polluting the information pool
with irrational or irrelevant opinions?
Perhaps the most comprehensive study of the value of expert and non-expert opinion, as it applies to
complex decision-making in the social sciences, has been provided by political scientist Philip Tetlock.
Over a more than 20-year period, he has researched forecast accuracy and, most recently, run The Good
Judgement Project. This explored the performance of a set of experts and non-experts in forecasting the
outcome of a particularly complex set of decision problems.
So what does Tetlock’s evidence suggest? That the usefulness of non-expert opinion in complex decision
settings depends on how exactly this information is drawn in and drawn on. From Tetlock’s book with
Dan Gardner in 2015, Super-forecasting, I would highlight two key findings. These chime with the evidence
outlined earlier on the costs and benefits of expert and non-expert opinion in collective decision-making.
First, there is real value in non-expert opinion when it comes to complex forecasting tasks. In a famous
example, Tetlock convened a group of 1,000 amateur forecasters and entered them in a prestigious
forecasting tournament run by the US intelligence agencies (IARPA).37
 This team beat the control group by
60% in year 1 and 78% in year 2. It not only beat expert academics from MIT to Colombia, but professionals
from the US intelligence services with access to classified information.
This is what Tetlock calls Super-forecasting. The incremental value of non-expert judgements has been
shown in a wide array of settings and for a wide range of complex problems. What about judgments on the

35
 For example, Roubini and Mihm (2011) and Janis (1972).
36
 Shafik (2017) discusses the impact of the rise of social media.
37
 Tetlock and Gardner (2015).
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
complex dynamics of the economic and financial system? While we do not have a structured experiment on
which to draw, we do have illustrative evidence on the relative forecasting performance of different groups.
Chart 3 considers the forecast errors of five groups – three “expert” (MPC, a panel of professional
forecasters and financial markets) and two “non-expert” (companies and households). For each group we
consider their forecasts for UK inflation 1, 2 and 3 years ahead. The sample period is relatively short so our
conclusions need to be cautious. And forecast accuracy is not the same as policy acumen. Nonetheless,
Chart 3 suggests that, in a complex forecasting task, there is little to choose between experts and
non-experts.
38
Tetlock’s second key point, which qualifies his first, is that not everyone can be a super-forecaster. A
randomly-chosen panel of non-experts cannot by itself be expected to outperform the experts. Chart 3
demonstrates that. Super-forecasting is not simply a law of large numbers. Tetlock shows that non-expert
super-forecasters have particular characteristics, including inquisitiveness and independence of mind. Only
those with these characteristics consistently outperform the experts.
We do not have a clean test of this hypothesis for our inflation forecasts. But just imagine the Bank had
been able to identify in advance one “super-forecaster” among the pool of outside forecasters. How much
would that have improved its forecasts over the sample period? The gap in two year-ahead forecast errors
between the MPC and the best external forecaster in the panel of outside forecasters is around ¼
percentage point. 20/20 hindsight is a wonderful thing, but this gives some measure of the scope for
forecast improvement.39
If we take Tetlock’s points together, they suggest that there is real value in seeking the views of non-experts
to help inform difficult decisions. But there are limits to these benefits. It is unrealistic to expect everyone to
become a super-forecaster. For central banks the message is that, by widening their information net, they
could stand to benefit from the wisdom of crowds. But there are limits to how and to how far you would wish
to widen the net if the crowd is to add signal rather than noise.
To illustrate this point, consider the following thought-experiment. Imagine you have an endowment of
time – 10,000 hours – to make as informed a decision as possible on a complex question. I have not chosen
10,000 hours randomly. In his book Outliers, Malcolm Gladwell argues, based on a detailed set of case
studies, that it takes around 10,000 hours for one person to command true expertise in a subject area.
Now let’s choose a complex, and topical, question: will Artificial Intelligence (AI) cause mass
unemployment? It is well-known that the views of experts (and non-experts) differ widely on this complex

38
 The conditioning assumptions (such as the assumed path of interest rates) and purpose of forecasts will vary between central banks
and other forecasters.
39
 Mankodi and Pike (2018) discuss whether central bankers can become superforecasters.
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
question and that the outcome is very uncertain. The problem we need to solve is not this but a simpler one:
how many people should we choose to help inform our view on the impact of AI on unemployment?
One possible answer to this question is one person. With Gladwell’s 10,000 hours under their belt, this
person would be as well-informed on the impact of AI on unemployment as anyone on the planet – a true
expert, perhaps even a phenomenon. This is the single person or, if you like, Oracle (in the Ancient Greek
sense of the word) solution to this decision problem.
But is it the optimal one? There are reasons for doubt. Evidence clearly suggests that, in a complex
decision environment, drawing on non-expert opinion can provide added insight. It offers diversity of
perspective, whose benefits are amplified if those diverse perspectives are discussed and debated to arrive
at collective conclusions. This is Surowiecki’s “wisdom of crowds” at work.
So let’s go to the other end of the spectrum and draw instead on the insights of a big crowd – say, 1 million
people, roughly the adult population of Estonia. This would then, in effect, be an Estonian referendum on
AI and unemployment. Clearly, we would now be drawing on a vast array of expert and non-expert opinion.
But that would come at the expense of less time for deliberation. Given our time quota, each person would
now have only around 30 seconds to give an answer.
Facing a tight time constraint and a very complex question, most people would struggle. I certainly would.
The best a non-expert could probably do is to provide an instinctive, “fast thinking” response. Or, perhaps
more likely still, they might jump aboard a collective, but potentially poorly-informed, bandwagon by copying
what others say. This would increase greatly the chances of Mackay’s “madness of crowds”.
Finally, let’s try an interior solution – say 1,000 people each spending 10 hours. This is large enough a
number to gather most of the benefits of diversity, but small enough to harvest the benefits of deliberation.
It would blend expert and non-expert opinion in a way which would allow collective deliberation. This is the
“wisdom of crowds” solution.
Figure 2 plots the relationship between insight and the numbers of people providing information in a group, in
stylised terms. You might call it the “Insight Curve”. The curve (shown in dotted lines) results from
combining the effects of a downward-sloping “deliberation” curve and an upward-sloping “diversity” curve.
Consistent with the evidence, both exhibit diminishing returns. The inverted U-shaped “Insight Curve”
identifies the optimal number of people to draw on to inform a complex decision.
The two corner solutions – the “Oracle” (single person) and the “Madness of Crowds” (population) – involve
very different trade-offs between diversity and deliberation. A single source of information gives you plenty
of time for deliberation – a hermit-like 10,000 hours. But it leaves you short of diversity of thought. A
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
population-wide pool of information gives us as much diversity as you could ever wish. But it leaves you long
fast-thinkers, short slow-thinkers and vulnerable to irrationality.
Both are inferior to an intermediate solution – the “Wisdom of Crowds” solution. This involves combining the
insights from both experts and non-experts, to reap the benefits of diversity of thought and action. But this
larger pool is not so large as to dilute the time available for slow-thinking and deliberation. It is a cocktail of
expert and non-expert opinion, comprising diversity and deliberation in equal measure, that cause good
ideas to flourish and bad ones to perish.
A relevant, practical question is where the optimal point along the insight curve lies. What is the optimal
number of people from whom to seek views if we are to maximise the benefits of folk wisdom? My stylised
example cannot provide an answer to that question. But some studies have looked at this question using
experimental methods and, reach interesting conclusions.
Christian Wagner and Tom Vinaimont ask two questions.40
 First, how large does a group of non-experts
need to be to exhibit expert-like performance? Second, how large does this group need to be to outperform
consistently a group of experts? On the assumption experts’ views are ten times more precise than a
non-expert, their answer to the first question is around 30 and the second around 1,000. This bookends the
range of non-expert views we might need in seeking the “wisdom of crowds”.

Representative Decision-Making
If we take this analysis at face value, the question is how practically central banks can engage a wider,
non-expert, audience to help inform their views on the economy and financial system? What new
infrastructure or architecture is needed for collecting and assimilating these views? And how do we ensure
they balance diversity and deliberation so as to be representative of the general public’s views?
These are deep questions, but they are not new ones. They have been debated by philosophers and
political scientists for thousands of years. Alternative models of representative decision-making have been
tried and tested throughout the course of history. This experience contains some useful lessons for central
banks, and other public institutions, seeking to strengthen the infrastructure linking them to wider society.
Let’s start at the very beginning. The birthplace of democracy is, for many, ancient Greece. Around 2,500
years ago, the city-state of Athens pioneered a particular form of representative decision-making when it
came to judgements on some of the most complex, societally-significant, issues of the day. The centrepiece
of this decision-making infrastructure was a public body – a Council of Five Hundred, or boule.

40
 Wagner and Vinaimont (2010).
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
The boule comprised individuals drawn from the general public by random selection (or sortition). They
served for a year. The purpose of the boule was to debate key issues affecting public life and devise policy
proposals. These were then taken to a public assembly for discussion, where a wider set of the general
public participated, often running into the thousands. For a small number of positions requiring specialist
skills, public officials and experts were elected or appointed.
The Athenian boule was not democratic embroidery, like a modern-day focus group. It had real power. It
decided which proposals were taken to the public assembly and which were lawful. Brett Hennig calls it the
“nerve centre of power” in ancient Athens.41
 The boule had wide public acceptance and attendance.
Participation in public life, through the boule or public assembly, was an accepted part of Athenians’ civic
responsibilities. Those not taking part were termed idiotes, from which the modern word idiot derives.
The infrastructure was far from perfect; the boule excluded women, slaves and non-citizens. But it was
notable in seeking to strike a balance between the two features subsequent research has shown to be
crucial for effective, representative decision-making in complex environments: diversity and deliberation.
Indeed, for that reason some have called the Athenian model of decision-making a deliberative democracy.
42
Diversity of decision-making was achieved by calling on a rotating, randomly-selected pool of non-expert
citizens, augmented by experts in a limited number of positions. Deliberation in decision-making was
achieved by restricting the number of representatives, having them convene and discuss regularly and agree
on a set of proposals. The Council of Five Hundred sits slap bang in the middle of the optimal range of
people suggested by Wagner and Vinaimont.
The Greek Empire of course did not last. Nor did its model of deliberative democracy for decision-making.
In the period since, two alternative models of representative decision-making have risen to prominence.
Interestingly, these seek to strike a rather different balance between diversity and deliberation, both relative
to the Athenian model and especially relative to each other.
One is executive democracy. Under this model, an elected or appointed set of representatives take
decisions on society’s behalf, often through a Parliament or Congress. This model underpins modern
democracies in a great many countries; it has grown in popularity for much of the past 200 years. Such has
been its success, many people these days would define democracy in terms of this model.
For most of history, this was not the case. Aristotle was not alone in seeing an executive democratic model
as an “elected oligarchy”.43
 At the time many executive democracies were being put in place, their Founding
Fathers (they were all still men) were often deeply suspicious of Athenian-style democracy. John Adams,

41
 Hennig (2017), Van Reybrouck (2016).
42
 Gruen (2018), Fiskhin et al (2008).
43
 Politics, Aristotle.
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
second President of the United States, captured it thus: “Remember democracy never lasts long. It soon
wastes, exhausts and murders itself”.44
 The Greek Empire lasted only 350 years before exhausting itself.
While Adams was wrong about the durability of executive democracy, suspicions about whether it properly
represents all slices of society have persisted. Indeed, these suspicions have risen recently. At the same
time as the steady ascent in the numbers of countries adopting the executive democratic model, there has
been a steady descent in public trust in the representatives of executive democratic institutions.
At the heart of those trust problems are concerns about whether decision-making is sufficiently reflective of
wider society. By and large, the public accept that the executive democratic model is a reasonable vehicle
for discussion and deliberation in decision-making. Indeed, the public can watch those deliberations on TV.
But there is a concern among the public about whether decision-making and decision-makers are sufficiently
diverse, whether they properly reflect and represent societies’ views and needs.
This is not an accusation that could be levelled at the alternative model of representative decision-making –
so-called direct democracy. This puts voice and vote firmly in the hands of public on decisions with an
important bearing on wider society. This model has been used to varying degrees across various countries
at various times. In some countries, public referenda are reserved for decisions whose societal impact is
especially large and long-lasting. I am sure we can all think of examples.
This model is recognised as having both pros and cons. It draws in a much wider range of expert and
non-expert opinion, so achieves a potentially greater degree of representativeness and diversity than
alternatives. This very breadth, however, makes it difficult to have a deliberative dialogue in which
participants have the time to consider carefully the facts, competing hypotheses and possible solutions. This
risks the collective view being less well-informed, and potentially more polarised, than would be ideal.

With degrees of discontent about both the executive and direct democratic models, there has recently been a
rekindling of interest in the Athenian, or deliberative democracy, model. This model, at least in principle,
offers greater diversity than the executive democratic model. It also offers greater scope for debate and
deliberation than the direct democratic model. It is not a third way, as it was in fact the first. But it is a
middle way. It is in some ways the institutional incarnation of the “Insight Curve”.
Interestingly, a number of countries have over recent years begun experimenting with deliberative
democratic processes to debate and resolve some of societies’ most complex problems. Citizens’ panels or
juries have been used to tackle such varied issues as voting reform (in Canada), gay rights (Ireland),
reconciliation between indigenous and non-indigenous communities (Australia), nuclear power (South
Korea), membership of the euro (Denmark) and environmental pollution (United States).45


44
 From John Adams to John Taylor, 17 December 1814.
45
 See https://cdd.stanford.edu/deliberative-polling-timeline/
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
Here in Estonia, you have experience of using people’s assemblies to debate key societal issues. In 2013, a
people’s assembly (Rahvakogu) was used to debate Estonian electoral laws. Last year, a people’s
assembly was used to deliberate on the future of ageing and pension reform. People’s juries have also been
used to debate regional issues, including water transportation in the Emajõgi River region of the country.
Estonia has been a relative hotbed of activity on deliberative decision-making in practice.
Each of these experiments used slightly differently approaches. This has provided useful case law on which
to decide what works and when. One interesting finding is that citizens’ panels can be effective not just in
getting people to engage on issues of national or regional importance, but in providing an effective forum for
people to change their minds, and converge in their opinions, on these issues. They can reduce
polarisation. These benefits lie at the heart of the deliberative democratic process.
A second lesson is that technology is a potential game-changer for deliberative decision-making. This relies
on a distributed, modular structure. Historically, structures such as the size of the boule placed physical
constraints on the scope for information-making and debate. Digital structures loosen those constraints,
allowing longer reach and greater connectivity at greater speed. Some countries have begun using digital
technologies to support their deliberative processes, including here in Estonia.

The benefits of doing so are partly practical, with greater numbers of people able to connect and
communicate than ever previously. The public assemblies of Ancient Greece brought together people in
their thousands. Today, technology could do so for millions of people. There are also, however, behavioural
benefits. The “trust shift” among the public means credibility and understanding is more often these days
built through distributed means and direct connections. A digital infrastructure can help those ends meet.
It could be argued that these country-level experiments are just that – experiments. The deliberative model
has not been tried and tested in anger for the full gamut of societal decisions. As put, that is true. Yet there
is at least one domain of public policy where this model has been tried and tested (both literally and
metaphorically) for a wide range of societal decisions over many centuries: the judicial system.

Many judicial systems draw on both expert (judges) and non-expert (juries) opinion, like the Athenians.
Non-experts are drawn randomly from the population, as in Athens. There is a clear process for deliberation
among participants, as in the boule. This system is the nerve-centre of decision-making on a defined set of
issues, as in Ancient Greece. The English judicial system has already lasted for almost 1,000 years, more
than twice as long as the Greek Empire.
All speeches are available online at www.bankofengland.co.uk/speeches
20
20
The Next Revolution
Against this backdrop, what concrete steps might central banks take to build an improved infrastructure for
engagement with the general public? Can it build on others’ experience in engaging wider society, in ways
which improve the public’s and policymakers’ understanding of the economy? Let me offer some
suggestions, which are motivated by recent initiatives at the Bank of England aimed at doing just that.
(a) Minding Your Language
Historically, the language of central banks has often been pitched at a level which has made it difficult for
most non-experts to understand. This has contributed to the public’s understanding and trust deficits.
Slowly but surely, that is beginning to change. Central bank’s external communications are being reoriented
towards a wider, non-specialist audience. Let me give a couple of examples.
The first is so-called “forward guidance” about monetary policy.
46
 How you judge its success hinges crucially
on what you consider to have been its objective and audience. If you are a financial market participant
whose job it is to price the future path of interest rates, the most useful type of policy guidance is precise and
time-specific. By contrast, if you are a company or household considering whether to spend, a general idea
of the direction and destination of interest rates is likely to be sufficient.
The critics of forward guidance have tended to be financial market participants for whom it lacks sufficient
precision and time-specificity. But forward guidance simply cannot be that precise; it depends on the path of
the economy. And pricing interest rates in financial markets was not the main purpose of forward guidance
in the first place. Its purpose was to support spending in the economy by households and companies. That
calls for shorter and simpler guidance, focussed on the broad direction and destination of interest rates.
The MPC first used the words “limited and gradual” in 2014 when describing the likely future course of
interest rates rises. It was short and simple – three little words. It was guidance aimed squarely at
households and companies, offering them an indication (but no promise) of the broad direction and
destination of interest rates. Surveys suggest this message was understood by a majority of companies and
some households.
47
 By allaying fears about too-rapid a rise in rates, it is likely to have encouraged spending
and supported the economy.
When the MPC did come to raise interest rates, in November 2017 and again in August 2018, it is interesting
to see how well these were understood by companies and households. Around three-quarters of
households, and around 90% of companies, had expected a rise in rates within the year ahead in surveys

46
 Bank of England (2013) provides a useful summary of concepts and issues relating to forward guidance; see Carney (2018) for a
more recent discussion.
47
 Carney (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
21
21
held immediately prior to the decisions. Simple, directional forward guidance on monetary policy appears,
for this non-expert audience who comprise most of the spending in the economy, to have been effective.
A second monetary policy example comes from the Bank’s quarterly Inflation Report. This was first
produced over 25 years ago, as part of the first wave of the transparency revolution. It is a long and
technical document, with language requiring at least 13-14 years of education to understand.48
 The
complexity of its language meant the Report was probably only accessible to around 10% of the population.
In November 2017, the Bank began publishing two simpler, “layered” versions of its Report. Layer 1 is the
single sentence/single graphic version, pitched at a level which is accessible to around two-thirds of the
population. Layer 2 is the single page/simple graphics version, accessible to over 40%. Measured in web
hits and downloads, these layered communications appear to have broadened notably the reach of the
MPC’s monetary policy messages, in particular among a non-expert audience.
A couple of recent research studies have used experimental methods to look more closely at the impact of
the Bank’s simpler communications on public understanding and trust.49
 For example, in a randomised
control trial of over 2,000 members of the general public, a recent study found that use of the visual
summaries increased the public’s understanding of the Inflation Report’s messages by around 25%. This is
a very substantial gain in public understanding for a fairly modest change in format.
This study went, however, one step further and asked whether further improvements in understanding were
possible. The format they considered personalised the language, making it more relatable to people’s
everyday lives – for example, by greater use of the first and second person (us/you), greater use of
commonly-used words (“rising prices” rather than “inflation”) and with more engaging, interactive graphics.
Doing so increased the gain in public understanding by 40%.

There are limits to what simplification, personalisation and visualisation can achieve. The same study
assessed how these changes in format affected the general public’s trust in the Bank of England’s policies.
These gains were modest. Perhaps that should come as no surprise. Understanding is a necessary but not
sufficient condition for trust-building. And trust tends to be earned slowly from repeated experience.
Nonetheless, these studies point to an important role for simpler, personalised communications in lowering
the twin deficits. There probably further for this agenda to run. For example, there is no reason in principle
why the same methods could not be applied to other central bank publications, covering the financial system
and banknotes. The Bank has already made some strides in this direction through its Financial Stability
Report and Knowledge Bank, aimed at improving public understanding of the economy and finance.

48
 Haldane (2017a).
49
 Haldane and McMahon (2018) and Bholat et al (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
22
22
Looking ahead, in my view greater personalisation of central bank communications holds promise. Central
banks often affect people’s lives in ways which are lagged and indirect and hence difficult for them to
observe. Earlier this year, I constructed “personalised” scorecards, which set out the impact of the Bank’s
monetary policy easing actions after the global financial crisis on the income, wealth and well-being of
citizens across the UK.
50
This recognised that the impact of monetary policy on people’s lives is often quite different depending on
their personal circumstances – old versus young, borrower versus saver, home-owner versus renter, North
versus South. Casting the impact of monetary policy in terms of averages or aggregates tends not to
resonate with people’s personal experience. Spelling out the benefits in personalised terms can potentially
increase understanding and awareness of central banks’ actions.
There is already a precedent for this in another arm of UK policymaking – tax policy. Taxpayers in the UK
receive a personal scorecard, telling them how their tax payments have funded spending on various public
goods at the macro-economic level. This personalisation has been found to have improved the public’s
understanding and acceptance of tax policies.51
(b) Public Education
A second way of improving public understanding is through improved public education. Many central banks
have made efforts in this direction over a number of years, including through school visits, schools
competitions and schools materials, including at the Bank of England and Bank of Estonia. That does not
yet appear to have satiated demand from pupils and their parents, almost 80% of who say they think
economics should form part of the curriculum.
52
In part in response, earlier this year the Bank of England launched a new education strategy. There are a
number of elements. One is a new set of schools competitions, aimed at reaching a broader range of
students. A second is a schools ambassador programme, with Bank officials giving 200 talks in schools
across the UK this year. We are on schedule to hit our target.
The most ambitious part of our education strategy has been to develop classroom materials on the economy
and financial system for use in schools as part of the curriculum for pupils aged 11-16. These go by the
name econoME. As the name suggests, and like our recent public communications efforts, these materials
describe the economy and finance in terms which are personal and relatable to young people’s lives. Why
does the economy and finance matter to me? And how do my decisions in turn affect the economy?

50 Haldane (2018a).
51
 Haldane (2018a) and Barnes et al (2017).
52
 ING-Economics Network Survey of the Public’s Understanding of Economics (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
23
23
We set ourselves a target of reaching 400 state schools with econoME during the course of this year. So far
since launch in April, over 1,000 schools have downloaded the materials, conceivably covering around
90,000 pupils.
53
 This demonstrates the potential pent-up demand for school materials on economic and
financial issues. The Bank’s aim is to reach close to 1,500 schools by mid-2020.
We are currently considering where next to take our education initiatives. A promising avenue would be a
younger age range, say 7-11 year olds. This will require different materials and possibly a different
approach. A more challenging market still would be adults, perhaps linked to existing initiatives to boost
economic and financial literacy in the public at large. The scale of the public understanding deficit means
central bank efforts can only be part of the solution but, as public institutions, an important part.
(c) A Distributed Architecture
Central banks historically have tended to be, well, centralised. But in a world where understanding of the
economy is often localised and trust is built on a distributed basis, a centralised infrastructure may be far
from ideal. What is needed instead is a distributed architecture for engagement. In response, central banks
have over time changed their structures and further change seems likely.
The Bank of England has had a network of Agents around the UK for around 90 years. Today, the Bank’s
twelve regional agencies cover all corners of the UK, with an extensive network that generates around 9,000
meetings with company contacts each year. This is a valuable source of intelligence on the economy and
financial system. Indeed, I would say it is increasingly valuable. It provides colour and context to
accompany our data and models. It offers folk wisdom, the wisdom of the company crowd.
But companies are only one crowd among many. Companies are an important lens on the economy, but far
from being the only one. Reflecting that, the Bank has recently sought stronger connections with some other
crowds, crowds with a different lens on the economy with which it has perhaps had less contact historically –
trades unions, charities, community organisations and the like.
For more than a year, I have augmented those Agents’ efforts with my own “Townhall” meetings across the
UK.54
 By design, these seek out the views of a wider range of citizens, in a wider range of locations on a
wider range of issues around the economy and financial system than has been the case in the past.
These events have established for me that there is wisdom in these crowds when gauging some of the key
issues shaping the economy and financial system. The lived experience of everyday people making
everyday decisions is what makes the economy tick (and sometimes tock). Tapping that lived experience

53
 Assuming 30 students per class and 3 classes per school.
54
 See https://www.bankofengland.co.uk/outreach
All speeches are available online at www.bankofengland.co.uk/speeches
24
24
can add to the Bank’s understanding of the economy and help it when setting policy to keep the economy
stable. There is wisdom in crowds of citizens, as there is in crowds of companies.55

The Bank is now taking a further step towards putting in place an infrastructure to formalise its engagement
with citizens. Earlier this year, in response to a recommendation from the RSA, we announced that the Bank
would be setting-up a set of Citizens’ Panels across the UK.56
 In the remainder of this year, we will be
running trials of these citizen panels to help us decide how best to organise them, before rolling them out
systematically, UK-wide, next year.
There are a number of key design questions that need to be answered. How do you ensure as wide, diverse
and representative a set of citizens as possible? How do you engage citizens who might not understand or
trust the Bank? How much time needs to be set aside to enable the deliberative process to be fruitful? And
how many people should you engage to get the balance right between diversity and deliberation? We are
working with a range of partners, including the RSA, to come up with answers.
The plan would be to set up one citizen panel for each of the Bank’s 12 regional agencies. With roughly 25
citizens per panel, this would give us a non-expert panel of around 300 people. This is edging up towards
the mid-point of the Wagner/Vinaimont range and just shy of the Athenian boule. By using digital
technology, we would aim to connect these citizen panels to allow national, as well as regional, deliberation.
This is deliberative democracy in practice. As with the judicial system, it will involve a blend of expert and
non-experts. As with juries and judges, there will be a structured process for deliberation. As in the legal
system, the role of the non-experts (citizen panels) is to help establish the facts and of the experts (MPC) to
set policy. What’s good enough for decisions on incarceration is good enough for decisions on inflation.
Conclusion
Central banks, like all public institutions, were created by the public to serve the public. Despite a
communications revolution, public understanding of central banks remains low and public trust in central
banks has fallen. Improving both calls for a second revolution, focussed on educating and learning from the
general public. The Age of Innocence may be over, but the Era of Engagement has only just begun.
This new era may mean 21st century central banks becoming more disbursed and personalised in their
communications, creating new infrastructure to interact with the public and extract their folk wisdom.
Thomas Jefferson said democracy is something you can only learn by doing. The same is true of central
banking. For the Bank of Estonia entering its second century, and for the Bank of England in its fourth, there
is much still to learn and even more to do. 

I am honoured to be here at the University of Melbourne to deliver this year’s Finch Lecture.
Colin David Finch built his brilliant academic and professional career on nurturing international co-operation
on economic and financial matters: in his studies here at the University of Melbourne and at the London
School of Economics; in his long and distinguished time at the International Monetary Fund; and, in later
life, at the Institute for International Economics in Washington DC.
Citizens of the UK owe Finch a particular debt of gratitude. He led the IMF rescue mission to the UK in 1976.
That, in many respects, marked the turning point in the UK’s economic fortunes. It heralded the start of a
long period of liberalisation and integration of markets in goods, services, people and money. The world
economy broadly mirrored those trends towards increased international liberalisation and integration.
The benefits the world economy has reaped from having pursued that path are now only too clear. Global
integration and co-operation has boosted dramatically flows of goods, services, people and monies. Each is
at levels never previously seen. This, in turn, has helped deliver higher living standards and lower levels of
poverty in pretty much every country on the planet.1
 Finch would have wholeheartedly approved.
Looking to the future, however, this wind is at risk of changing direction. The global financial crisis has left
lasting scars. That has created pressure to place speedbumps, or in some cases roadblocks, on flows of
goods, services, people and monies. One of the key policy challenges ahead will be to prevent the hard-won
gains from global integration and co-operation being lost. The stakes could scarcely be higher.
Tonight, I want to discuss a different policy challenge facing policymakers, specifically central banks. It too
has risen in prominence since the global financial crisis. It too has posed big questions about the existing
policy order. And it too needs, I believe, to be tackled transparently and comprehensively if stability and trust
in the international monetary system is to be preserved.
Just before the 2016 referendum on EU membership, Professor Anand Menon of King’s College London was
explaining to an audience in Newcastle that, in the view of most economists, leaving the EU would be bad for
their economic health. GDP was likely to fall. A woman rose from the audience and, with finger pointed,
uttered the memorable line: “That’s your bloody GDP, not ours!” There was no right of reply.
Issues of inequality have loomed large in many public debates, not just Brexit, over the past decade or so.
Recent books on the rising tide of inequality by Thomas Piketty, Tony Atkinson, Joe Stiglitz and Branko
Milanovic have, somewhat surprisingly, become best-sellers.2
 So too have books by Martin Ford and
Eric Brynjolfsson and Andrew McAfee discussing how the rise of the robots may further worsen these
inequality problems.3

1
 See, for example, Carney (2016) and World Bank Group (2016).
2
 Piketty (2014), Atkinson (2015), Stiglitz (2013), Milanovic (2016).
3
 Ford (2016), Brynjolfsson and McAfee (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
At around the same time, monetary policy in a number of countries became more expansionary than at any
time in recent history. Interest rates in a number of advanced economies fell to their lowest-ever levels
during the financial crisis. In some countries, additional stimulus was provided through asset purchases –
so-called Quantitative Easing or QE. Those asset purchases are currently running at around a cumulative
15% of annual global GDP.4

Given the increased interest in distributional issues, and the increasingly activist role of monetary policy, it is
perhaps no surprise that there has been increasing public interest in the fusion of the two – that is to say, the
distributional impact of monetary policy itself, which is what I’m going to talk about today. Chart 1 shows a
simple metric of that increased public interest since the financial crisis based on that most scientific of
metrics, the Google search.
Interest in these issues has sometimes spilt over into sharp criticism of central banks’ actions. Surveys of
the general public have suggested that a large proportion may believe lower interest rates have actually
made them worse off.
5
 Meanwhile, QE is held by some to have increased inequalities between rich and poor
and to have harmed pension funds and the companies sponsoring them.
6
Some have gone further, suggesting that QE may have caused central banks to cross the thin line between
monetary and fiscal policy, between economic policy and political economy.7
 Others still have suggested
that these unconventional monetary measures may have called into question central banks’ operational
independence from government in the setting of monetary policy.8

My aim in this lecture is not to resolve these questions, one way or the other. Rather, it is to provide a
framework for assessing the first claim – that recent monetary policy actions have had a significant
distributional impact. The answer to this question clearly has an important bearing on the broader normative
questions some have posed about the role of central banks.
I begin by explaining why I think distributional issues are relevant to central bankers and to policymakers
generally. This is not always agreed territory. I will then set out a framework for assessing the quantitative
impact of monetary policy on different cohorts of society. This is done in both standard units of
measurement (money amounts and percentages) and in non-standard units (welfare and well-being).
This framework is calibrated using as a case study the loosening of UK monetary policy after the global
financial crisis. The main results are easily summarised. There is nothing to suggest monetary policy has
had significant effects on either income or wealth inequality in the UK over recent years. Indeed, the

4
 QE calculated with reference to combined purchases by US, UK, euro-area and Japanese central banks between start-2007 and
end-2017.
5
 Bank of England and NMG survey (2017).
6
 For example, Lysenko et al (2016) and Altmann (2009).
7
 For example, Buiter (2014).
8
 See Bank of England ‘Independence 20 Years On’ conference: https://www.bankofengland.co.uk/events/2017/september/20-yearson
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
loosening of monetary policy after the crisis appears to have delivered significant financial and welfare
benefits to almost all cohorts of the UK economy, albeit often through different channels.
I conclude with thoughts on the implications of this analysis for policymakers. My view is that there is a
strong case for making, on a periodic basis, comprehensive and transparent assessments of the
distributional impact of monetary policy. This would help people understand the purpose and impact of
monetary policy, both on the economy in general and on them as individuals, on “their GDP”.
I give some illustrative examples of a “Monetary Policy Scorecard” summarising the impact of monetary
actions on particular cohorts. Greater transparency of this type would not, by itself, reduce any distributional
effects of policy. But it could help in explaining the impact of these actions, in a localised and personalised
way, as a means of improving understanding and trust in central banks. Both have been a casualty of the
crisis.
Why Distributions Matter
Let me start with a bald statement: all public policy is distributional, be it monetary, fiscal, structural or social.
The reason I know this is because redistribution is the way public policy works; it is what policy does for a
living. Some policies redistribute resources between agents at a point in time. Others redistribute resources
between agents over time. If policy is not working through one of these channels, it is not working.
If all policy is distributional, it does not take much of a leap of imagination to see that policymakers may wish
to understand and explain its distributional impact. In some policy settings, that already happens – in fiscal
policy, social policy, climate change policy, pension policy. Published assessments of policy impact can
improve public understanding of, and debate about, often difficult distributional choices.9
When it comes to monetary policy, the position until recently has been rather different. There has been no
particular clamour for published assessments of the distributional impact of monetary policy. Why? Because
monetary policy has benefitted from, not one, but two “Get Out of Jail” cards. Both have their origin in Milton
Friedman’s 1968 Presidential Address to the American Economic Association, half a century ago.
10
The first follows from the neutrality of monetary policy with respect to real variables over the longer run.11

Typically, this neutrality is associated with the total level of resources in the economy, such as employment
or output – the vertical long-run Phillips curve. But neutrality applies, with no less force, to the distribution of
resources in the economy, whether between sectors or regions. In theory, monetary policy ought also to be
neutral in its longer-run impact on economic inequality.

9
 For example, Ball et al (2013) and Büchs et al (2011).
10 Friedman (1968).
11
 Patinkin (1987) traces the history of thought relating to the neutrality of money.
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
The second Friedman point is that the monetary policy tools of the trade should be simple and singular – for
example, the money supply or short-term interest rate. These instruments are levied at an economy-wide
level. Central banks simply cannot set different interest rates for different sectors or individuals or regions.
Monetary policy tools are thus too blunt an instrument to offset distributional differences.
So whether viewed from an objectives or instruments perspective, the case for monetary policy needing to
take much account of its distributional impact is weaker than for the other arms of public policy, such as fiscal
and social policy. The latter have objectives which are often explicitly distributional and their instruments are
better-equipped to achieve such redistribution. This much is relatively well-accepted in academic circles.12
It does not follow from that, however, that distributional effects are irrelevant in the setting of monetary policy.
Both in theory and in practice, there are several reasons why distributional effects might still matter to
monetary policy and to monetary policymakers and hence why understanding and explaining these effects
might be important public policy-wise.13

First, even though it may be neutral over the longer-run, monetary policy can and does have potent effects
on the economy over the shorter-term, including potentially on the distribution of resources. This should not
be a bone of contention. We know this is likely to be the case because this is the very reason monetary
policy is non-neutral in the first place. Monetary policy, like all policy, relies on redistribution for its efficacy.
For example, changes in interest rates redistribute interest payments between savers and borrowers at any
point in time. They also affect, over time, the balance between saving and borrowing in the economy.
Central bank asset purchases potentially affect differently those with assets and those with debt, as well as
affecting the balance between asset and debt-holding over time. This is how monetary policy works.
When it comes to understanding how (indeed, whether) monetary policy is working, then, it is important to
understand and monitor these distributional moving parts. Distributional analysis provides a window on the
monetary policy engine at work. For example, when interest rates change it is important to gauge not only
how cash flow switches between savers and borrowers, but whether it is being saved or spent.
14

Second, explaining the distributional impact of monetary policy is potentially important for building and
maintaining understanding and trust among the general public in these policies. It can help preserve that
all-important social contract between policymakers and citizens.15
 Doing so effectively may call for
explaining the impact of policy on the general public, not just at an aggregate level but on a disaggregated
basis.

12
 For example, Lipton (2014) and Cœuré (2012).
13
 For example, Haldane (2016) and Carney (2016).
14
 Bunn et al (2015).
15
 Haldane (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
Third, the ultimate yardstick of policy success is its impact on people’s well-being. That is affected
importantly by how the effects of policy are distributed across society. A rise in aggregate GDP in an
economy need not necessarily mean higher welfare for all its citizens or regions, as the lady in the audience
made clear. The more uneven the distribution of winners and losers, and the greater the skew in gains, the
less likely it is aggregate social welfare will have risen.
When it comes to evidence on the distributional impact of monetary policy, there are wide gaps in
understanding and even wider gaps in perception. Even among policymakers and academics, work in this
area remains embryonic and the results are often ambiguous.
16
 If I were summarising this research
evidence, I would say it suggests monetary policy can and has affected inequality, but that these effects
have probably been modest quantitatively.
There are a number of reasons why existing research may not have been clear-cut in its conclusions.
Studies have used different measures of monetary impulse (interest rates versus QE) and different
methodologies (macro versus micro). Perhaps most importantly, these studies have tended to focus on
different monetary policy transmission channels. Doing so can give quite different perspectives on the
impact of monetary policy on overall inequality.
The general public probably suffers from a particularly acute version of this problem. When asked to assess
the impact of changes in monetary policy, they tend to focus on those channels which have an observable
and immediate impact on their finances. These are likely to include the cash flow effects of interest rate
changes on interest payments and receipts and the effects of asset prices on wealth portfolios.
For example, when asked about whether lower interest rates have benefitted them, around a third of the UK
public – and more than half of those aged over 50 – suggest not.17
 Within that group, a large majority –
around 80% – focus only on the negative effects of lower rates on their savings income. This is only one of
the channels through which monetary policy works, albeit the most immediate and observable.
It is far harder for the public to take account of the other channels, many of them neither immediate nor
observable, through which a relaxation of monetary policy might benefit them. This includes the effects of
looser policy in boosting wages and jobs. Not taking account of these channels can give a distorted lens on
the impact of policy on the economy at large, on inequality and on individuals’ personal finances.
To gauge fully the distributional impact of monetary policy, then, we need to capture as many as possible of
its transmission channels, direct and indirect, immediate and slower-moving. It is only by considering all of
these channels in combination that we can then properly evaluate the impact of monetary policy on the
income, wealth and welfare of people and the distribution of these effects. It is to those we now turn.

16
 Recent summaries of the literature are contained in Deutsche Bundesbank (2016) and Monnin (2017). The Bank of England working
paper on this topic (Bunn, Pugh and Yeates (2018) on which this speech draws) contains a good survey of the recent literature.
17
 April 2017 NMG Consulting survey of households, as discussed in Bunn et al (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
Assessing the Quantitative Impact of Monetary Policy
We develop a quantitative framework to measure the impact of monetary policy on the economy as a whole
and on different cohorts within it. We use the UK as a case study. The framework draws heavily on recent
research by Bank of England staff, Phil Bunn, Alice Pugh and Chris Yeates. Their working paper contains
full details of the exercise, which involves three broad steps.
18
 As with any quantitative analysis, there are of
course uncertainties and confidence intervals around the results. As such, the broad qualitative conclusions
are probably worth emphasising more than any precise figures.
Step One: Starting Distributions
The starting point is an initial set of distributions for the variables of interest, looking across a representative
sample of households. For this exercise, data are taken primarily from the Wealth and Asset Survey (WAS),
a biennial survey of UK households’ assets and debts.19
 Specifically, we focus on a fixed panel of 10,000
households across 4 waves of this survey between 2006-08 and 2012-14.
The survey design and sampling means these households are broadly representative of the UK population
as a whole.
20
 It is well-known, however, that household surveys tend to under-sample the tails of the income
and wealth distribution, in particular the upper tail.21
 In the analysis, we focus on the Gini coefficient and the
ratio of the 90th to the 10th percentile of the distribution (“90/10 ratio”) as measures of inequality.
Charts 2 and 3 plot the distribution of income and net wealth across UK households just prior to the crisis.
22

This is the starting date for the exercise, after which UK monetary policy was loosened materially. As is
well-known, these distributions are extremely uneven. For example, in 2007 the richest 10% of households
accounted for around a quarter of total income and over 40% of wealth.
At the other end of the distribution, the whole bottom half of households by income accounted for only
around a quarter of income in the economy and only around 10% of net wealth. Around 50% of households
earned less than £20,000 in income each year and around 40% had fewer than £90,000 in net wealth.
Age is an important factor shaping these distributions, as life-cycle theory would suggest. Chart 4 plots the
net wealth distribution by age. Those over 50 account for around 80% of aggregate net wealth and those
over 70 for around a quarter. By contrast, those under 30 account for less than 5% of wealth. In general,
the young are borrowers with a stock of debt, the old are savers with a pool of assets.

18
 Bunn et al (2018).
19
 We also use the Family Resources Survey (FRS) for household incomes.
20
 See
https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/debt/methodologies/wealthandassetssurveyqmi
21
 As discussed in Vermeulen (2016), for example. The ONS Wealth and Asset Survey is probably the UK household survey with the
best coverage of the tails of the distribution.
22
 The measure of wealth used is net total wealth, comprising financial, property, physical and pension assets, net of debt. There is a
question about how much households are aware of pension wealth, and therefore value it. The distributions are not especially sensitive
to whether pension wealth is included.
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
If we overlay the income and wealth distributions in 2007 and 2013 (the end-date for our exercise), these are
pretty-much identically-shaped (Charts 5 and 6). On that basis alone, there has not been any clear shift in
income or wealth inequality since the crisis. Standard summary measures of inequality, such as the Gini
coefficient, confirm that conclusion: for both income and wealth, the Gini is largely unchanged.
23
On the face of it, then, this does not strongly suggest that the relaxation of monetary policy since the crisis
has significantly worsened inequality. Unless, that is, other factors have more than counterbalanced the
effect of monetary policy. To assess that, we need to identify the distinct impact of monetary policy on the
economy, both in aggregate and looking across the household distribution.
Step Two: Calibrating the Impact of Monetary Policy
The policy episode we consider is the relaxation of UK monetary policy after the global financial crisis.
These actions by the Bank’s Monetary Policy Committee (MPC) comprised: first, the reduction of short-term
interest rates by 5 percentage points (from 5.5% to 0.5%) between February 2008 and March 2009; and
second, the purchase of £375 billion of government securities between March 2009 and mid-2012.
In combination, these measures represented a very significant relaxation of UK monetary policy, perhaps the
largest in the Bank of England’s history. We consider their combined effects, as interest rates and QE were
part of a single monetary policy strategy by the MPC. Nonetheless, given their potentially different
distributional consequences, we also consider the effects of QE and interest rates separately.
To trace out the impact of these monetary interventions on the economy and on households, we use a
macro-to-micro simulation approach. This involves two mappings. The first is from the monetary impulse to
a small set of macro-economic aggregates (interest rates, employment, wages, equity and house prices and
consumer prices). This is done using the Bank of England’s macro-economic model.
Charts 7-9 show the effects of the combined monetary stimulus on aggregate GDP, unemployment and CPI
inflation.
24
 Without the monetary stimulus, GDP in the UK would have been around 8% lower,
unemployment 4 percentage points higher and the level of consumer prices 20% lower. Although there are
significant uncertainties around these estimates, especially at crisis time, these effects are clearly large.
The second mapping is from these macro-economic aggregates to the balance sheets of households. This
mapping focusses on four main channels of transmission: (a) cash flow channel (the direct effects on
households’ interest payments and receipts); (b) labour income channel (the effect on household wages and

23
 The same is true of other measures of inequality, such as the 90/10 ratio. Sampling issues mean caution is needed when
interpreting in particular the wealth-based measures of inequality.
24
 See Carney (2016) and Haldane (2016) for more details. Lower interest rates and QE operate through slightly different transmission
channels.
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
employment); (c) wealth channel (the effect on households’ financial, housing and pension wealth); and (d)
inflation channel (the effect of prices on real household deposits and debt).25

The key point is that these multiple channels of monetary transmission are fully recognised and calibrated.
Some of these channels are direct and immediate, such as cash flow. Others are indirect and slower-acting,
such as labour income. By combining them, we can calibrate the general equilibrium consequences of the
UK’s monetary relaxation on the economy, in aggregate and across cohorts.
As we would expect, the potency of these channels varies across households depending on their
characteristics. For example, the strength of the cash flow effect depends on individuals’ stock of
interest-bearing assets and liabilities. The labour income channel depends on the skills and age of the
household. And the wealth channel depends on the size and composition of their wealth portfolio.
Step Three: Final Distributions
With these mappings, we can calculate the impact of the MPC’s monetary loosening on different household
cohorts. Charts 10 and 11 plot this effect on household income, looking across the income distribution. This
impact is shown on both a “money amount” and “percentage of income” basis. 26
 These effects are also split
between the “cash flow” and “labour income” channels.
The average household has gained in income terms by around £1,500 each year, or close to £9,000
cumulatively, from the MPC’s monetary loosening. Put differently, the average household would have been
around 5% worse off each year had monetary policy not been loosened in response to the financial crisis.
The lion’s share of this boost resulted from the positive impact of looser policy on jobs and wages. This is a
slower-moving, harder to observe, channel by which monetary policy benefits households, but clearly a
quantitatively important one.
Looking at the distribution of these income gains across deciles, these are reasonably evenly spread as a
percentage of income. The percentage gains are slightly lower among lower income households and are
slightly negative for the lowest income decile. Nonetheless, if we calculate a Gini coefficient or 90/10 ratio
based on these distributions, they are largely unaffected by the monetary policy loosening.
27
If we look at money amounts, rather than percentages, the balance of benefits is significantly more uneven.
Around half the total income gain accrues to the top two income deciles. But this reflects the highly uneven
distribution of income prior to the crisis, rather than telling us anything about the effects of monetary policy.
These monetary gains may, nonetheless, have had a bearing on public perceptions of monetary policy.

25
 Bunn et al (2018) provide details on these mappings and the assumptions they make in arriving at them.
26
 The ‘money amount’ numbers that follow in this section refer to real income, defined in 2013 prices.
27
 Bunn et al (2018) provide more details.
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
Turning to wealth, Charts 12 and 13 do the equivalent analysis looking along the wealth distribution. As with
income, the average UK household has benefitted significantly in net wealth terms from looser monetary
policy, by on average almost £90,000 or 20% of net wealth. All asset classes have benefitted – financial,
housing and pension. These gains arise from the boost to asset prices from looser monetary policy.
As with income, in percentage terms these gains are evenly spread across the distribution. Every wealth
decile has gained from looser policy. Also as with income, the net effect of monetary policy on measures of
wealth inequality is negligible. The unequal prior distribution means, however, that monetary gains are
heavily skewed, with the top two deciles accounting for 60% of total net wealth gains.
Charts 14-17 look at the effect of monetary policy on income and wealth looking across the age distribution.
They show some notable generational differences. In percentage terms, the income gains from looser
monetary policy have been largest among the young, largely due to improved employment prospects.28

Recessions typically generate sharper rises in unemployment among those who are less educated or less
skilled, so it is also worth noting that the loosening in monetary policy during the crisis played an important
role in limiting that effect. Older cohorts, by contrast, lost out in income terms due to lower interest receipts
on their savings.
By contrast, wealth gains have been evenly spread across the age distribution in percentage terms, with
every cohort gaining. In money terms, the largest beneficiaries have been older age groups, the value of
whose pensions and houses have been boosted by the policy-induced rise in asset prices. Looking at the
oldest age group, this boost to wealth has exceeded their income losses by a factor of around 10.
Looking at the effects of monetary policy by region, the benefits of monetary policy on incomes have been
evenly spread in percentage terms (Charts 18 and 19). Monetary policy has not caused any significant
widening in regional inequality. Given the unequal starting position, however, the distribution of monetary
gains for both income and net wealth is heavily skewed, with a quarter centred in London and the
South-East.
Charts 20 and 21 look separately at the effects of lower interest rates and QE, respectively, on the income
distribution. Although part of the same strategy, these two instruments clearly operate through different
channels and have different distributional effects. The effects of QE are predictably slanted towards financial
wealth effects, while the effects of Bank Rate operate more through net interest income and house price
channels.
This analysis suggests that, with few exceptions, the impact of looser monetary policy on different cohorts of
society has been positive and significant in income and wealth terms. Another way of making this point is to

28
 Though a notable feature of the UK’s recovery has been that, notwithstanding the positive effects of monetary policy, the real income
of younger people has fallen further and recovered more slowly than older age groups (Haldane (2016)).
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
ask what fraction of households are likely to have been made worse off from the loosening of UK monetary
policy. Taking income and wealth effects together, only 4% of households were made £500 or more worse
off.29
At the same time, this analysis has made clear that there are often large differences in the nature and scale
of impact across cohorts. Some households will have seen these benefits through higher employment,
others through lower debt payments, others still through higher wealth. Even if people observed and
understood all of these channels, their lived experience would have been quite different.
In practice, we know people’s understanding of these channels is partial and imperfect. Some channels are
hard to observe or take time to take effect, such as the quantitatively important effect of monetary policy in
boosting jobs and hence household labour income. It will have been difficult for households to tie the effects
of looser monetary policy to their improved job prospects, given the long lags in transmission.
People’s subjective sense of well-being might also be influenced by more than their percentage gains. How
people’s gains, in money terms, compare with others’ monetary gains might also matter. In experimental
and real-world settings, these relativities often matter to people’s sense of fairness and hence well-being.30
With that in mind, we now turn to a fully-fledged welfare analysis of the impact of monetary policy.
Assessing the Welfare Impact of Monetary Policy
Assessing the impact of monetary policy on the welfare of different cohorts is interesting for several reasons.
First, it is the sum of individuals’ subjective well-being – that is, social welfare – that ought ultimately to
matter for policymakers, even though policymakers’ legal mandates are typically cast in terms of a set of
more objective macro-economic measures, such as inflation and employment.
31

Second, translating estimates of financial gain into welfare terms can help when deciding how best to
aggregate across different channels to gauge their general equilibrium impact. For example, how do we
appropriately aggregate changes to income and wealth in their impact on well-being? And should we focus
on percentage or money changes when judging the impact on people’s welfare?

29
 Carney (2016) notes that just 2% of households have deposit holdings in excess of £5,000, few other financial assets, and do not
own a home. Therefore, “the vast majority of savers who might have lost some interest income from lower policy rates have stood to
gain from increases in asset prices, particularly the recovery in house prices.”
30
 As discussed in Ball and Chernova (2008) and Clark, Frijters and Shields (2007), for example.
31
 Taking the sum of individuals’ utility is consistent with a Benthamite utilitarian concept of social welfare, but other specifications are
possible. A Rawlsian specification, for example, would define social welfare as the wellbeing of the worst off member of society.
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
In principle, a welfare-based analysis allows us to answer those questions. Doing so is not, however, simple.
In algebraic terms, consider a utility function of the general form:
𝑈 = ∅(𝛼(𝑦), 𝛽(𝑤), 𝛾(𝑋))
Let’s call this Equation (1). 𝑈 is utility, 𝑦 is income and 𝑤 is wealth. To turn our quantitative estimates into
welfare-equivalent measures, three empirical uncertainties loom large. First, calibrating the weights on
current and permanent income (wealth), 𝛼 and 𝛽; second, determining which variables other than income
and wealth affect well-being, 𝑋, and with what weight, 𝛾; and, third, calibrating the curvature of the utility
function with respect to its arguments, ∅.
That is a lot of free parameters. One approach to pinning down these parameters would be to calibrate them
using past studies. This might be a decent approximation but is not ideal if past behaviour might be a poor
guide to behaviour in today’s very different environment. This factor is likely to be relevant at times of
financial crisis and exceptional movements in monetary policy, for which there is no precedent.
While no approach can fully get around these problems, we use an approach that places fewer arbitrary
restrictions on people’s preferences. Instead these preferences are revealed, courtesy of empirical
estimation, and then used to parameterise a social welfare function. This involves direct estimation of the
parameters in Equation (1) using our sample of households.32

Estimating (1) requires a measure of household well-being. The Wealth and Asset Survey began asking
households about their well-being or “happiness” from 2010-12. Specifically, households are asked to rate
their well-being on a scale of 1-10, based on four questions based around satisfaction, happiness and
anxiety. We use an average of some of these well-being scores when estimating (1).
33
We tested a variety of specifications of Equation (1) to see which fit best, using different set of factors and
different functional specifications for the utility function. Some of these specifications are shown in Table 1.
Our preferred specification is shown in Column 5. Its key features are:
 Both current income and wealth (permanent income) affect household well-being, as we would
expect in a standard utility function, in a statistically significant way.
 Our preferred specification includes a variety of separate measures of gross wealth and gross debt.
The different components of wealth have a different likelihood of being spent and hence have a
different impact on well-being. For example, the coefficient on deposit wealth is eight times larger

32
 See, for example, Layard, Nickell, Mayraz (2008).
33
 The results are not sensitive to using subsets of the answers.
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
than the coefficient on pension wealth, consistent with the former being more accessible than the
latter and, therefore, more likely to be spent. Debt is bad for household well-being, as others have
found.34

 The functional specification which best fits the happiness regression is a logarithmic one. This
implies that there is diminishing marginal utility of income, as we would expect and as other studies
have found. It also implies that changes in happiness that arise as a result of changes in income
and wealth are appropriately captured in percentage, rather than in money, terms.35
 Various previous studies have found that employment and financial security can have an important
effect on people’s happiness, over and above their impact on income and wealth.36
 For example, it
is well-established that having a job improves significantly people’s confidence and self-esteem, over
and above any effect on pay. Those conclusions are borne out here. We find there is a statistically
significant role for unemployment and arrears in explaining household well-being, over and above
the effects from income and wealth.
 The effects of unemployment and arrears on well-being are quantitatively large. To gauge that, we
can translate them into income-equivalent terms using our welfare regression. The effects of having
a job are, in well-being terms, equivalent to around 3 months’ extra income on average, across all
households.
37
 This is consistent with powerful effects of work and financial security on happiness.38
Using this framework, our earlier quantitative estimates of the impact of looser monetary policy can be
translated into welfare terms. Charts 22 and 23 show these welfare benefits, broken down into their
component parts (income, wealth, unemployment), looking across the age and income distribution. A few
key points stand out.
First, the effects of monetary policy in lowering the probability of unemployment have a very material impact
on household well-being. These effects often account for more than half of the total welfare gain for
households across all cohorts. This means that, if we looked only at the boost to income and wealth from
looser monetary policy, this would significantly understate the well-being benefits to households from the
MPC’s monetary actions, by a factor of at least two.

34
 For example, Jacoby (2002).
35
 Both are shown in Table 1. Log utility is the best fit for happiness across households, but it is also common to use this functional
form at the individual level. We also experimented with a variety of other specifications, with higher order terms to capture greater
curvature in the utility function, but these were generally not statistically significant. Note that our regressions are based on a pooled
cross section of households and so are not pure estimates of the impact on happiness of changes in income and wealth over time.
36
 For example, Winkelmann and Winkelmann (1998) and Di Tella et al (2003).
37
 There are various ways to calculate the income-equivalence of the lower probability of unemployment, and these numbers would vary
depending on the assumptions made. For simplicity, we exploit the results from our analysis, which show the effect on welfare from the
lower probability of unemployment on the average household to be around six times larger than the welfare boost from higher labour
income.
38
 For example, Clark and Oswald (1994).
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
Second, these gains from higher employment are felt particularly strongly by younger people who gain most
from having a job. In welfare terms, the biggest gainers from the MPC’s monetary loosening have been the
young, though every age cohort benefits overall. The same is true looking along the income distribution,
where every cohort gains in welfare terms, even though the same was not true of purely financial gains.
Policy Implications
Where does this leave us? Let me summarise the key empirical conclusions before turning to policy lessons.
I take the key empirical conclusions to be:
 The material loosening of UK monetary policy after 2007 has had a significantly positive effect on
employment, income and wealth, without which average living standards in the UK would be
materially lower.
 These gains have been shared right across the distribution of income and wealth, age and region,
though the precise scale and nature of these benefits does differ across cohorts. Very few
households across the UK are likely to have been net losers in financial terms from the MPC’s
monetary loosening, once all of its effects are take into account.
 This conclusion is reinforced if we consider the impact of monetary loosening on people’s well-being.
In addition to the financial gains from higher income and wealth, many people have benefitted
significantly from greater job security and reduced financial anxiety. In welfare terms, these benefits
may have been as large as the financial gains. The number of households who have lost out in
welfare terms from looser monetary policy is only 12%.
 These quantitative results differ sharply from public perceptions of the impact of monetary policy. To
illustrate, Charts 23 and 24 compare the welfare impact of lower interest rates among different age
cohorts with people’s self-reported survey impact. These differ not just in scale but sign. It suggests
the public perception gap around monetary policy is large. This is symptomatic of a deficit of public
understanding, and is likely to have contributed to a deficit of public trust, on the public’s part.
 These deficits may not be that surprising. It is very difficult for people to identify, much less quantify,
the channels through which monetary policy affects their lives, directly and indirectly. For most
people, this information is simply not to hand. If it were, in a simple and personal form, this might
contribute to closing the gaps in public trust and understanding about monetary policy.
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
That takes me naturally to practical ways of closing the twin deficits. Inequality issues are likely to continue
to loom large in public debate in the years ahead, not least due to the impact of technology on jobs and
skills.39

It seems perfectly reasonable that policymakers should periodically explain the impact of their actions on
both aggregate outcomes and its distribution. Indeed, doing so consistently and transparently could I think
deliver important benefits, helping close the public perception gap and those twin deficits of trust and
understanding about monetary policy. But how is that best done?
Other domains of public policy have developed mechanisms for improving transparency about the impact of
policy. These often involve “framing” the effects of policy in ways which increase the public’s interest,
understanding and hence trust in policy.
40
 This can be done on either an aggregate or disaggregated basis.
One approach, “social good framing”, emphasises the aggregate or societal benefits of policy. For example,
tax payments by an individual can be allocated to the uses of taxpayer funds to support public or social
goods, such as improved hospitals, schools and roads. In experiments, this approach has been shown to
improve both understanding of, and compliance with, tax policies.41
As an example of this approach, in 2014 the UK authorities began issuing taxpayers with annual tax
summaries, breaking down the tax paid into the uses to which it was put. This was prompted, in part, by the
large perception gap between the public’s view of how taxes were being spent and their true destination – a
perception gap similar, but smaller, than for monetary policy. There is evidence these tax summaries
improved the public’s understanding of government spending and use of taxpayer money.42
What would be the monetary policy equivalent? There are estimates to hand on the social or public good
benefits of monetary policy. For example, without the post-crisis loosening of monetary policy, UK GDP
would have been around 8% lower and unemployment 4 percentage points higher. Greater awareness of
these public goods might help, at least a little, in closing the public perception gap.
A more promising approach is “personalised framing”. This links a policy intervention to the specific
circumstances of an individual or set of individuals. For example, experimental trials suggest people are
more likely to pay taxes or donate to charity if they are told about the local or personal impact of doing so.43

Personalising a message in this way increases the changes of it being heard, understood and acted on.
What would be the monetary policy equivalent? Imagine a personal monetary policy scorecard, similar in
spirit to a personal tax scorecard, tailored to an individual’s characteristics – assets and liabilities, housing

39
 For example, Ford (2016).
40
 The Behavioural Insights Team are specialists in this area, see Halpern (2015) for a discussion of their methodologies.
41
 For example, Hallsworth et al (2014).
42
 Barnes et al (forthcoming) and Sheffield Political Economy Research Institute (2015).
43
 Agerström et al (2016), Service et al (2014).
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
tenure, age, employment status etc. This would identify the channels and the amounts by which a significant
change in monetary policy affected people’s finances (income, wealth) and wider well-being (employment
and financial security).

Now stop imagining. Figure 1 shows such a scorecard for the contribution of monetary policy since the crisis
for the average UK household. In the upper part, it quantifies the annual impact on the average household’s
finances. For example, the MPC’s monetary loosening added around £260 per year to the average
household’s net interest income, £1,200 to their labour income and £14,000 to their net wealth.
The lower part of Figure 1 looks at the effects of non-financial channels on well-being, namely the reduction
in the probability of unemployment, translated into income-equivalent units.44
 The increased probability of
being in a job adds in excess of £7,000 each year for the average household. All in, this gives a boost to
welfare for the average UK household of close to £23,000. This is large relative to annual household income
of £32,000.
As a memo item, and point of comparison, Figure 1 also shows an estimate of the annual cost of undertaking
monetary policy, using data from the Bank of England’s accounts.45
 On a broad estimate, this annual cost
amounts to around £2.80 per household per year. This “servicing charge” for monetary policy is clearly a
very small fraction of the annual benefit to households from looser monetary policy over the period.
The disaggregated analysis set out means that, in principle, a monetary policy scorecard like this could be
fitted to any individual’s circumstances. As an illustration, Figures 2-4 show scorecards for three different
hypothetical households: a renter under the age of 30; a 30-50 year old mortgagor; and a 50-plus
home-owning household.
As we might expect, there are large differences in both the channels and scale of impact across these three
cohorts, though in each case the net effect is strongly positive. As a point of comparison, Figure 5 gives a
scorecard for the relatively small set of households for whom the net effects have been negative – a retired
household renting a property whose wealth is held mainly in deposits.
In principle, this sort of personalised monetary policy scorecard could be drawn up at any level of
disaggregation and for any monetary intervention, past or present. Clearly, the results and methodology
underpinning any scorecard-type approach need to be treated with caution. For example, there are wide
confidence intervals around the estimates presented here.

44
 There are various ways to calculate the income-equivalence of the lower probability of unemployment, and these numbers would vary
depending on the assumptions made. For simplicity, we exploit the results from our analysis, which show the effect on welfare from the
lower probability of unemployment on the average household to be around six times larger than the welfare boost from higher labour
income. This ratio is then applied to the boost to labour income experienced by different cohorts in Figures 1-5.
45
 The cost of monetary policy, broadly defined, was £77mn in 2017, and there are around 27.2mn households in the UK. The cost of
monetary policy includes the Bank’s monetary analysis area and the relevant share of other business areas’ (research, statistics,
markets, banking, HR, technology etc.) expenses toward monetary policy. This should probably be considered an upper bound.
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
These sorts of evaluation also have greatest value when conducted at times of significant change in the
monetary policy stance or when this change is not expected. That is to say, they are probably best
undertaken periodically rather than regularly.
One important point is that the interpretation of these evaluations depends greatly on the reasons monetary
policy is changing in the first place. For example, the loosening of monetary policy in the case study was
positive for most households. But that does not imply a tightening of monetary policy would thus be negative
for household finances and welfare. For example, if this tightening came in response to a stronger economy,
and helped avoid inflation overshooting its target requiring higher-still interest rates, its effects are likely to be
positive too. This suggests these scorecard exercises are likely to be context-specific and hence are best
done periodically at times of significant change to the monetary stance.
Reporting its effects in a disaggregated way should not call into question the purpose of monetary policy
which, as set in statute, is to stabilise aggregate economic activity and prices. Nor does it imply monetary
tools should be used to meet distributional ends. Rather, its purpose would be to improve public
understanding of the effects of monetary policy. This would hopefully help close the public perception gap
about the effects of monetary policy and the trust deficit facing monetary policymakers.
46

Conclusion
A detailed, disaggregated analysis of household balance sheets suggests the material loosening in UK
monetary policy after the financial crisis did not have significant adverse distributional consequences. To the
contrary, a detailed household-level analysis suggests that the majority of cohorts across the economy were
significant gainers from this loosening, in financial and welfare terms.
The channels through which monetary policy has affected people’s lives are often neither easy to observe
nor well-understood, giving rise to a large perception gap on the part of the public about the true impact of
monetary policy on them. This is not a failing on the part of the public. Nor is it a failing in the effectiveness
of policy. Rather it reflects the difficulty of capturing the often subtle ways in which monetary policy affects
people, in a way that is clear and relevant to them.
While difficult, this is not an impossible objective, as other domains of public policy have demonstrated. A
disaggregated analysis can be used to help decompose the ways and means and amounts by which
different sets of households have been affected by monetary policy. Some illustrative and tentative
examples of these personal “monetary policy scorecards” have been shown.

46
 As discussed in Haldane (2017), the Bank already has various initiatives in train to support an increase in public understanding of the
economy, such as layered communication, outreach initiatives and educational materials for schools.
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
Producing something along these lines would be a new departure for macro-economic policy, if not for some
of the other arms of policy. Analyses of monetary policy have tended to emphasise the economy-wide
effects, measured in terms of macroeconomic outcomes like GDP, unemployment and inflation. This still
makes sense as a means of explaining the social good monetary policy is offering.
But messages often land most effectively when they are personalised. Monetary policy has an important
personalised impact on most people’s lives. Some scorecard-like device could, at times of significant
change in the monetary policy stance, help explain, in simple terms, the personal as well as societal benefits
monetary policy confers. It would seek better to explain how monetary policy is affecting your job, your cost
of living, your GDP.
This should help make monetary policy relevant to people’s everyday lives. At a time of diminished trust,
and some scepticism, it could potentially make a contribution towards improving public understanding of, and
trust in, monetary policy. It would mean the next time someone shouts from the audience “That’s your
bloody GDP”, we would have right of reply: “Oh no it’s not; it’s yours too.”

Economic growth has been among the greatest gifts given to us, as individuals and societies. It may no
longer be fashionable to say so. Measures of economic growth, like Gross Domestic Product (GDP), can be
highly imperfect metrics of how well individuals and societies are faring, their subjective sense of well-being.
1

And growth in income and output is not, of course, an end in itself.
Nonetheless, it is now pretty well-established that growth is a vital ingredient, indeed pre-requisite, for
meeting many of the broader societal objectives many would view as important to our longer-term health,
wealth and happiness.2 While not an end in itself, economic growth appears to be a vitally important means
of achieving those societal ends.
Economic growth is the main reason why global levels of poverty and rates of infant mortality have fallen
spectacularly over recent centuries. It is why longevity and educational standards have risen secularly over
the same period. And growth may even have contributed to the incidence of global conflicts and wars having
fallen to their lowest levels, perhaps in human history.3
That makes growth a gift, one necessary (if not necessarily sufficient) to deliver secular improvements in
living standards. And it is what makes a good understanding of the determinants and drivers of growth so
crucial for societal progress. By understanding the forces driving growth, past, present and future, we can
begin to devise and implement economic policies that support improvements in society.
I thought I understood the story of economic growth, its drivers and determinants. But recently I have
changed my mind. I have a new story of growth. I think this story carries important implications for
understanding the future challenges of technology and for devising the future policies and institutions
necessary to meet them. That might require, among other things, a repurposing of successful institutions
like this one, turning them from universities into multiversities.
The Story of Growth
The story of economic growth often begins with a chart like this one (Chart 1). This plots the path of global
economic growth, measured by Gross Domestic Product (GDP) per head, over the past 1000 years. And
what a story it tells. It is a tale of two halves – or, more accurately, a tale of three-quarters and a quarter.4

For the first three-quarters of the past 1000 years, the global economy stood still in growth terms. GDP
growth per head of population averaged less than 0.1% per year. Just try and imagine those rates of growth.
They meant it took more than a millennium for living standards to double.

1
 Stiglitz, Sen and Fitoussi (2009).
2
 For example, Coyle (2014).
3
 Two recent excellent books (Pinker (2018), Rosling (2018)) chart this and other dramatic improvements in societal well-being over
time.
4
 De Long (1998).
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
This growth experience is likely to have had an important psychological, as well as financial, impact. The
average person would not see any improvement in living standards over their lifetime. Their lived experience
was imperceptibly different than their parents, grandparents or indeed great, great grandparents. People’s
lives were lived on a travellator. Rising living standards was not a social norm. Secular stagnation was.
You could argue, with some justification, that measures of GDP at this time might have been missing a trick.
We did not have any formal, statistical means of measuring incomes and outputs in the economy until well
into the 20th century, with the development of the National Accounts. And GDP anyway underplays
important factors shaping people’s well-being, such as levels of nutrition, infant mortality and longevity.5
Yet, as best we can tell, alternative well-being metrics tell broadly the same story.6
 Rates of infant mortality
were unchanged over this period. Life expectancy flat-lined at 30 to 40 years. Levels of calorific intake
stood still. And the global population scarcely budged, held in check by unchanged food supplies, as
predicted by Thomas Malthus in the late 18th century.
7
 Not for nothing was this called the Malthusian era.
Another example of this secular stagnation, a favourite of mine, is provided by plotting heights from
excavated skeletons records. These are a diagnostic on levels of health and nutrition. Male skeleton
heights were essentially unchanged, at 160-170cm, for at least 2,000 years prior to 1750. Humans grew
neither physiologically nor financially. And neither, then, did the societies in which they lived.
That makes the contrast with developments in the last quarter of the millennium all the more striking. From
around 1750, GDP per head suddenly took off and has kept on rising ever since, growing by around 1.5%
per year. The magic of compound interest means these growth rates have transformed people’s living
standards. The time it takes for living standards to double has shrunk to less than 50 years.
The impact on people’s well-being, financial and psychological, has been transformational. It has meant
each generation has been around 50% better off than its predecessor. After 1750, generational progress
suddenly became visible, children to parents to grandparents. Growth in living standards became a social
norm, in a way never previously true in human history. Life’s travellator turned into an escalator.
And not just for GDP. Broader metrics of well-being tell a similar tale. Rates of infant mortality have fallen by
40 percentage points since the end of the 18th century.
8
 The estimated lifespan of a human has risen by a
factor of over two.
9
 And estimated male skeleton heights have risen by 5-10cm.10
 Humans have grown,
financially and physiologically, in lockstep. The world has moved from a Malthusian Era to a Golden one.

5
 Stiglitz, Sen and Fitoussi (2009).
6
 For example, Clark (2009).
7 Malthus (1798).
8
 Our World In Data: https://ourworldindata.org/child-mortality
9
 Our World In Data: https://ourworldindata.org/life-expectancy
10
 Haldane (2015a).
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
A Story with One “i”
What explains this extra-ordinary inflexion point in the history of economic growth and living standards? So
fundamental is this question to so much of social and economic history that, understandably, it has attracted
huge amounts of empirical, historical, social and theoretical research. Out of this cloud of evidence, a clear
sunbeam of understanding has emerged about the deep causes and catalysts of economic growth.
If you ask a schoolchild what happened in England around the middle of the 18th century, as I often do, many
will give you the right answer. It marked the dawn of the Industrial Revolution. But what exactly was this a
revolution in and what exactly caused it? In the standard account, the catalyst for this revolution in economic
growth came from a single source, from one “i” - ideas.11
In the second half of the 18th century in the UK, ideas began sprouting like morning mushrooms. These
ideas emerged seemingly spontaneously and roughly contemporaneously. They were also relatively closely
clustered geographically. They included James Hargreaves’ spinning jenny in 1764, Richard Arkwright’s
water frame in 1769 and James Watt’s steam engine in 1775.
In the fullness of time, these ideas began to revolutionise both industry and work. After a lengthy adoption
lag, they spread across sectors and across regions. They migrated from being mere ideas to becoming
“GPTs” or General Purpose Technologies.12
 In the process, they generated waves of investment in new
factories, machines, processes and infrastructures. There was, in the jargon, capital-deepening.
Neo-Classical theories of economic growth are very clear what would be expected to happen next.13
 When
an outward shift in the economy’s technological frontier is combined with higher levels of physical capital, the
fuse is lit on higher productivity among companies, higher wages among workers and, ultimately, higher
living standards among societies. So it was during the Golden Era.
Chart 2 plots productivity, wages and GDP per head in the UK since the Industrial Revolution. All three
plateaued prior to 1750. Since then, all three have moved up in lockstep. Ideas and innovation have borne
continuous fruit in higher productivity, pay and living standards. This fruit has been shared roughly equally
between owners of companies (profits) and workers in companies (wages). We know that because labour’s
share of the national income pie is similar today to 1750.
Of course, it is not ideas and innovation from the 18th century that has sustained growth over the subsequent
period. Although the innovation process is continuous, historians often point to three distinct waves – three
Industrial Revolutions. The second came in the latter part of the 19th century with the emergence of GPTs

11
 For example, Romer (1990) describes a theoretical model of growth based around innovation.
12
 Bresnahan and Trajtenberg (1995).
13
 For example, Mokyr (2011) discusses Britain during the Industrial Revolution.
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
such as electricity, the internal combustion engine and sanitation. The third came in the middle of the
20th century with new GPTs such as computing, digitisation and the internet.14

Yet in each case the playbook was familiar from the first Industrial Revolution. The emergence of a new
strain of GPTs resulted in an outward shift in the production possibility frontier and capital-deepening. With
higher levels of investment in physical and human capital, the productivity of companies, the wages of
workers and the living standards of societies continued their Northerly ascent. The Golden Era continued.
Some people are now discussing the next innovation wave, a so-called Fourth Industrial Revolution. The
GPTs driving this are likely to include Artificial Intelligence (AI), Big Data, automation, robotics, 3D-printing
and nano-technology.15
 I will return to the Fourth Industrial Revolution when discussing future growth.
A Different Growth Story
As appetising as this growth story sounds, a reading of the history books make it difficult to swallow whole. It
is certainly plausible to think that a divine coincidence of Hargreaves, Watts and Arkwrights, in roughly the
same place at roughly the same time, resulted in a white-hot crucible of creativity. Indeed, we might be
seeing that self-same crucible of creativity in Silicon Valley today.
But the Industrial Revolution was hardly the first firing of that crucible. We have, through history, seen many
episodes of furnace-hot crucibles of innovation, spanning most continents and most centuries.
16
 Successful
cities, regions, countries, continents and empires of the recent and distant past were often built and
sustained on ideas and innovation and accompanying investment in machines and people.
The Roman Empire is a case in point. Some of you will remember the sequence from Monty Python’s The
Life of Brian which begins with the rhetorical question: “What did the Romans ever do for us?” The sketch
concludes: “But apart from better sanitation and medicine and education and irrigation and public health and
roads and a freshwater system and baths and public order...what have the Romans done for us?”
You do not need to go that far back for examples of big ideas which had a transformative impact on industry
and society. Immediately prior to the Industrial Revolution and running chronologically, these included the
windmill in the 12th century, the mechanical clock in the 13th
, the cannon in the 14th, the printing press in the
15th, the postal service in the 16th and the telescope and microscope in the 17th
.
17
It will surprise no-one that waves of innovation, big and small, have been lapping the shores of society for the
entirety of human civilisation. In other words, while ideas and innovation may well be a necessary condition

14
 Van Reenen et al (2010), for example, document the economic impact of ICT.
15
 See Bughin et al (2017) and Haldane (2018), for example.
16
 Flohr (2016), for example, discusses innovation in Roman society.
17
 Clark (2009).
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
for economic growth, the historical record suggests they may not themselves have been sufficient. Other
forces appear to have been at play, translating these ideas into sustained growth in living standards.
What might those forces be? I want to provide two different, but complementary, lenses on the growth story.
The first focusses on a rather broader set of “capitals” – not just physical capital (plant and machines) but
human (skills and expertise), intellectual (ideas and technologies), infrastructural (transport and legal
systems), social (co-operation and trust) and institutional (national and civic, private and public) capital.
18

History suggests each of these capitals may have played an important supporting role in the story of growth.
Ideas alone, without the support of one or more of these broader capitals, have historically run aground. For
example, in the UK many of the foundations for growth after the Industrial Revolution were laid in the
centuries preceding it. It was on this platform of “capitals”, plural, that ideas and innovation then built.
Take human capital. The most dramatic improvements in educational standards in the UK occurred prior to
1750 (Chart 3). So too did the largest improvements in measures of social, infrastructural and institutional
capital.19
 Ideas needed these foundations to flourish. A steam engine is not much use without the skills to
build it, the tracks to run it on, the institutions to oversee it, the trust of the public to accept it. The causes of
the growth inflexion in 18th century England were as much sociological as technological.
Another lens through which to view this alternative growth story has recently been provided by economic
historians, Steve Broadberry and John Wallis.20
 Indeed it was probably this lens on the world, above all
others, that led me to change my own story about growth. Broadberry and Wallis do two things.
First, they construct some new estimates of growth going back to 1300. Rather than relying on period
averages, they use higher-frequency measures of growth. This changes the growth picture significantly.
Period averages conceal large, long-lasting swings in growth over time (Chart 4). Their revised measures
suggest growth did not flat-line prior to 1750, even though it averaged zero. Rather, growth oscillated wildly.
Second, given the scale and duration of these oscillations, they decompose their growth data into
“expanding” and “contracting” periods. Chart 5 plots these for the UK over the past 700 years. A broadly
similar pattern of expanding and contracting periods can be found in Spain, France, Italy, Portugal, and the
Netherlands over roughly the same period. This decomposition changes the growth story radically.
Even prior to the Industrial Revolution, economies experienced notable periods of strongly positive growth.
Between 1300 and 1700, GDP expanded slightly more than half the time. Over these expanding periods,
growth averaged 5.3% per year. The reason average growth was far-lower over this earlier period – indeed,

18
 Solow (1956), Mincer (1984), Romer (1990), Iyer, Kitson and Toh (2005), North (1990).
19
 Haldane (2015a).
20
 Broadberry and Wallis (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
little more than zero – was because expanding periods were almost exactly offset by contracting periods.
They accounted for slightly less than half the period, during which growth averaged minus 5.4% per year.
So what has changed in the period since the Industrial Revolution? Growth during expansion periods is
relatively little changed. Since 1750, it has averaged 3.2% per year. That is in fact a bit less than growth
during expansion periods prior to the Industrial Revolution. This strongly implies it is not the greater
incidence of ideas-fuelled booms after 1750 that accounts for the growth inflexion.
The explanation lies instead in the dramatic fall in both the probability and cost of GDP contractions.
Recessions have occurred only 30% of the time since 1700 and only 17% of the time since 1900. During
these periods, growth has averaged minus 2.2% per year since 1700 and minus 3.4% per year since 1900.
Since 1750, recessions have become far less frequent and less painful. It is the avoidance of deep
recessions that differentiates the Golden Era from its Malthusian predecessor.
Where does this leave our story of growth? Well, the story that better fits the facts appears to be one in
which the conveyor belt of ideas and innovation has been continuous over the centuries, causing lengthy if
lumpy ideas-fuelled expansions. But whereas prior to the Industrial Revolution this conveyor belt was
regularly halted by recessions, more recently these interruptions have been far fewer and less costly.
Put differently, the real revolution in living standards after 1750 came about not exclusively, or perhaps even
mainly, from the surge in ideas and technologies. Rather, it resulted from societies having found some
means of avoiding the subsequent recessionary bullets. Prior to the Industrial Revolution, these killed
expansions dead. After it, societies appear to have found some effective means of dodging them.
A Second “i” on Growth
To understand growth, then, we need to explain why economies become less recession-prone after 1750.
For me, the explanation lies in a second “i” – institutions. I will argue that it was the emergence of institutions
that explains the rise in the other capitals that were essential pre-conditions for growth (human, social,
infrastructural, intellectual etc.) It was the emergence of these same institutions which also cushioned the
damaging effects of recessions. As with sight, to gain a true perspective on growth you need two “i"s.
This is, in one sense, not a new story. A number of distinguished economists have, over the years, placed
institutions centre-stage in explaining secular trends in social and economic development. They include
classical economists such as Adam Smith and John Stuart Mill and, more recently, Nobel Laureates such as
Ronald Coase and Douglass North.21
 Broadberry and Wallis themselves conclude that the most likely cause
of fewer downside growth surprises since the Industrial Revolution has been the rise of institutions.

21
 Smith (1776), Mill (1848), Coase (1960), North (1991).
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
Empirical work supports the institutional hypothesis. Cross-country growth regressions suggest nations with
high-quality institutions out-grow those without.
22
 Detailed country case studies reach a similar conclusion.
For example, institutional quality has been used to explain the very different economic fortunes of North and
South Korea after separation. The absence of institutions is, for some, Why Nations Fail.
23

If institutions hold the key, what exactly do we mean by them and how exactly do they come about?
Douglass North’s definition is a good starting point: “humanly devised constraints that structure political,
economic and social interactions”.24
 So defined, institutions are social infrastructure. They include formal or
legal institutions, like Parliaments, judiciaries, central banks, social safety nets and schools. But they also
include less formal associations and groups, such as universities, trade unions, guilds and charities.
As for their genesis, history suggests institutions emerge for a variety of reasons. Sometimes they have
been a direct response to political upheaval. Some of the largest transformations in political institutions have
resulted from revolutions: in the UK, after the Glorious Revolution of the 17th century, in France after the
French Revolution of the 18th century, in the United States after the Civil War of the 19th century.
At other times, institutions have emerged in response to the pressing financial or social needs of citizens.
Often, those times of most pressing financial and social need have coincided with sharp changes in the
economic environment which have left large swathes of society worse-off. And often, technological
disruption and displacement have been the root cause of these sharp changes in the economic environment.
The three Industrial Revolutions provide a useful set of case studies. Each caused technological disruption
and significant job displacement. Each had, as a result, a wrenching and lasting impact on the job and
income prospects of large swathes of society. Each caused a significant and sustained period of hardship
for many.
25
 And each caused a stretching of the social fabric, often to close to breaking point.
It is now well-established that each industrial revolution caused a significant loss of livelihood for workers
whose set of tasks was most susceptible to automation. The upshot, often, has been a “hollowing out” of
jobs across the skill distribution.26
 Because reskilling and retraining takes time and effort, job displacement
worsens pay prospects for a great many, significantly and persistently.
27 And these losses were often made
worse because they were concentrated occupationally and geographically.
28
A second, accompanying adverse side-effect of technological disruption has often been rising levels of
income inequality. Historically, those skilling-up to keep one step ahead of the machine saw demand for

22
 Levine and Renelt (1992).
23 Acemoglu and Robinson (2012).
24
 North (1990).
25
 Haldane (2015b).
26
 Katz and Margo (2013), for example, show that skilled blue collar employment in US manufacturing declined while white collar and
unskilled manufacturing employment increased.
27
 Autor (2017) documents lower relative wages for those in US most exposed to international competition.
28
 Autor (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
their skills rise and, with it, their wages. By contrast, those at the other end of the skills distribution saw their
incomes fall, due to reduced demand and increased supply of their skills. The resulting rise in income
inequality has tended, historically, to heighten popular discontent and worsen social cohesion.
A third adverse side-effect was that workers did not always benefit, fully or immediately, from
technologically-induced gains in companies’ productivity and profitability. In the early stages of each
industrial revolution, wages tended not to rise in line with productivity, causing labour’s share of the national
income pie to fall.
29
 This, too, tended to add to popular discontent and damage social cohesion. Labour’s
income share has also fallen in a number of advanced economies over the past few decades.30
A final side-effect is that periods of technological transition were often lengthy as well as painful. In the first
Industrial Revolution, many displaced workers had still to find alternative work by the middle of the
19th century, with income inequality running high and labour’s share falling. This period is known by
historians as “Engel’s pause”.31
 Subsequent industrial revolutions have seen similar, if less long-lived,
pauses.
If economic hardship is widespread and social cohesion damaged, a societal response is needed. One such
response is the creation of new pieces of social infrastructure – in other words, institution building. Each
industrial revolution saw a surge in such institution-building, to cushion the adverse side-effects of
technological change on jobs and pay. In this way, the recessionary hit to workers and the economy could
be shortened or dampened. Broadly speaking, two such sets of institution emerged.
One set equipped workers with the new skills they needed to thrive in a new jobs environment. It was only
by acquiring these skills that workers could transition to where new jobs were being created. By keeping one
step ahead of the machine, technological unemployment could be avoided. A shorter, more seamless skills
transition reduced the hit to workers’ incomes and lowered the risk of skills atrophy, which might otherwise
cause lasting damage to employment. This social infrastructure might be called “enabling” institutions.
Another set of institutions provided workers with support to cushion the hit to their finances and well-being
during the painful and lengthy period of job transition. This might be financial support, in income or loans. It
might be housing or shelter. Or it might be social or emotional support. This social insurance ensured that
lives were underpinned, inequalities held in check and the social fabric held together. It reduced recession
risk, for individuals and for societies. These might be called “insuring” institutions.32

29
 Allen (2005) documents the stagnation of real wages in Britain from 1800 to 1840.
30 IMF (2017).
31
 Allen (2009) discusses Engels’ pause in the context of the British economy from 1760 to 1913.
32
 Of course, many other institutions played a role in the labour market beyond the “enabling” and “insuring” ones mentioned here. For
example, the ‘Poor Laws’ in the 1800s meant that the poor had to enter workhouses with terrible conditions to receive help in the form of
clothes and food.
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
During the first three Industrial Revolutions, the skills workers needed to keep one step ahead of the
machine were largely cognitive. Machines undertook largely manual (“doing”) tasks, which had previously
used labour-intensive technologies. Cognitive (“thinking”) tasks remained, by and large, the exclusive
domain of humans. So institutions emerged to nurture thinking skills, largely in children and young adults, to
increase the chances of successful transition to the cognitively-intensive future world of work.
In children, the most important of these institutional innovations was probably compulsory schooling. For
children between the ages of 5 and 10, this was formally introduced in the UK with the Elementary Education
Act of 1880. It was extended to age 11 in 1893 and age 12 in 1899. The Fisher Act of 1918 provided for
compulsory education to age 14 and part-time education to age 18. Compulsory full-time education was
extended to age 15 in 1947 and age 16 in 1972.
33

As these pieces of social infrastructure developed, so too did standards of educational attainment and
cognitive skills. The fraction of the population aged 15 and over that attended secondary school rose from
23% to 60% between 1950 and 2010.
34
 In the 1950s, less than 11% of the relevant age group passed five
or more GCSE O levels in England and Wales. By 2010, that had reached almost 80%.35
Beyond age 18, there was in parallel a rise in both numbers and attendance at universities. For over six
hundred years from 1209, England had only two universities – this one (Oxford) and the other one
(Cambridge). It was not until the formation of University College London in 1829 that their duopoly was
broken. And it was not until the 20th century that some of the other main cities – Manchester, Liverpool,
Leeds, Bristol, Birmingham and Sheffield – established their own universities.36
 Today, there are around 130
universities in the UK as a whole.
Participation rates in higher education have also undergone a secular rise and, latterly, a revolution. The
share of young people attending university has increased from just over 3% in 1950 to over 8% in 1970, over
19% in 1990, 33% in 2000 and 49% by 2016.
37
 Having been less than 5,000 in 1920, the number of first
degrees obtained at UK universities had increased to over 365,000 by 2016.
38
Through the rapid rise in school and university attendance, there was a revolution in cognitive skills in the
workforce. That provided the human capital necessary to turn new technologies and machines into higher
levels of productivity among companies, higher wages among workers and, ultimately, higher living
standards among societies. Put differently, higher cognitive skills reduced the odds of a lengthy
recessionary period of technological unemployment for many individuals and societies.

33
 Legislative dates for compulsory schooling vary from dates of enforcement.
34
 Barro-Lee Educational Attainment Dataset: http://www.barrolee.com/
35
 Bolton (2012).
36
 Willetts (2017).
37
 Bolton (2012) for data from 1950 to 2000. 2016 data from Department of Education.
38
 Bolton (2012) and Universities UK (2017). Refers to full-time first degrees only.
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
Raising levels of skills and training for workers is one way of limiting the costs of technological disruption.
Providing them with access to finance or housing is another. This support helped smooth the hit to incomes
and lifestyles brought about by job displacement. Starting in the late 18th and early 19th centuries, at the
dawn of the Industrial Revolution, financial institutions began to emerge to provide financial support through
bridging loans, often operating on collective or co-operative principles. In the UK, the Friendly Societies Act
of 1819 established today’s credit unions. The Regulation of Benefit Building Societies Act followed in 1836.
Other sets of institution emerged to support workers in wider ways. Trade unions grew in importance during
the 18th century, as workers’ jobs were hollowed-out and their share of the income pie was consumed.
Unions gained rights progressively during the 19th century, culminating in the foundation of the Trade Union
Congress (TUC) in 1868. The Trade Union Act of 1871 gave unions fully-fledged legal status.
Trade unions helped workers by campaigning to protect their rights, improve their conditions and boost their
bargaining power in the workplace. They also provided direct financial support to unemployed workers,
through an early form of unemployment insurance, to smooth the income consequences for workers of job
loss. The National Insurance Act (1911) and the Beveridge Report (1942) subsequently made this a
responsibility of the state and put it on a comprehensive footing.
This was part of a wider-ranging shift in the role played by the state in society from the 17th century onwards.
State spending as a proportion of national income rose from around 1% in the 16th century to around 12% in
the 18th
, 14% in the 19th and 33% in the 20th
.
39
 It financed social infrastructure of various kinds supporting
those facing greatest hardship. This ranged from social housing to healthcare to income support. Its effect
was to cap the downside, recessionary risk to individuals, economies and societies.40
Another form of institutional support came from the creation of state-owned and/or central banks. The Bank
of England was set up at the end of the 17th century to manage the government’s debt and safeguard the
nation’s currency and finances. Over the course of subsequent centuries, this role has expanded. Since at
least the middle of the 19th century, the Bank of England has become one of the key institutional safeguards
against recession risk and financial crisis.41
 Indeed, today, those roles are defined in statute.
Non-state institutions have also played an important role in protecting those facing the adverse side-effects
of economic change. This included many religious, philanthropic and charitable organisations and citizen
movements that emerged from the 18
th century onwards, often as a direct response to hardship brought
about by technological change. These institutions offered, and many continue to offer, a combination of
financial, housing, training and emotional support to those in need.

39
 Clark (2009). Figures refer to government command of output as a percentage of GNP in England.
40
 Other 19th century legal changes that supported individuals from adverse side-effects of economic change included recognition of
limited liability companies and constraints on the ability of courts to sentence debtors to prison.
41
 Anson et al (2017), for example, discuss the emergence of the Bank of England as lender of last resort.
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
To mention one, the Young Men’s Christian Association (YMCA) was founded in 1844, at around the time of
Engels’ pause. It was established explicitly to meet the housing and social needs of young men who had
moved from rural towns and villages to find work in factories in the cities. It soon grew into a regional, and in
time global, network offering financial, housing, training and social support, to both men and women,
cushioning the recessionary hit to societies. And that remains its role today.
The story, then, is of a social infrastructure emerging to support workers and societies buffeted by
technological change. These institutions aimed either to reduce the length or to limit the costs of adjustment
by displaced workers. By enhancing skills and mitigating hardships, the incidence and cost of recessionary
adjustment was reduced, for individuals and societies. And this, ultimately, was the difference between the
living standards escalator of the Golden Era and the travellator of its Malthusian predecessor.
Joseph Schumpeter spoke powerfully about the forces of “creative destruction”.
42
 The lesson of history
seems to be that we need both to “cultivate the creative” and to “disarm the destructive” if innovation is to
translate into rising levels of social, human and infrastructural capital and, then, higher living standards. It is
only by establishing strong institutional roots that technological fruit can subsequently be harvested.
Future Growth and Future Institutions
From the past and present to the future. What lies in store for future economic growth? It is difficult to know
with any confidence. But if history is any guide, the story of growth will hinge on the interplay between the
two “i”s – the disruptive forces of innovation on the one hand, the stabilising role of institutions on the other.
There is an active debate underway on the first of these – the likely future path of innovation. Some have
recently argued that the pace of innovation may be declining.
43
 The argument here is that many of the
low-hanging fruit from the ICT revolution have already been picked. Diminishing returns to R&D may have
set in. And other secular forces – including adverse demographics and rising inequality – may have acted as
additional headwinds to growth.44
 This view sometimes goes by the name secular stagnation.45
An alternative school of thought believes we may instead be on the cusp of a new great wave of innovation,
a Fourth Industrial Revolution.
46
 This revolution in innovation will, it is said, be jet-propelled by a set of
potentially new GPTs, among them AI, Big Data and robotics. For technology optimists, the story is one of
secular innovation, not stagnation. And on this view, the potential gains in productivity and growth could be
as large as any of the earlier industrial revolutions.47


42
 Schumpeter (1942).
43
 Gordon (2012), Fernald (2014).
44
 Summers (2013) and Rachel and Smith (2015).
45
 Secular stagnation theory was originally proposed by Hansen (1938) in the aftermath of the Great Depression
46
 Brynjolfsson and McAfee (2014) discuss ‘The Second Machine Age’.
47
 Brynjolfsson, Rock and Syverson (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
Typically, the two sides of this argument are taken to be in a secular struggle: the dark forces of secular
stagnation pitted against the dynamic forces of secular innovation. Yet in practice, both arguments have
merit. They are certainly not mutually incompatible. It is perfectly possible to imagine a world of rapid
innovation which nonetheless leaves large swathes of society in its slipstream. More than that, recent
research suggests this is a likely outcome of the Fourth Industrial Revolution.
There are good conceptual grounds for thinking the displacement effects of the Fourth Industrial Revolution
may be larger than ever-previously. Every industrial revolution has resulted in a hollowing-out, typically of
mid-skilled tasks. Historically, that has meant largely manual, labour-intensive tasks. Machine has replaced
human in activities that are routine and repetitive.
The future could well be very different. For example, the dawning of AI means that humans will no longer
have the cognitive playing field to themselves. Thinking or non-routine tasks may increasingly be taken up
by machines. They will be able to process more quickly, more cheaply and with fewer errors than their
human counterpart, at least in some activities. That could make the hollowing-out of human tasks, now
cognitive as well as manual, far greater than ever before.
48
A cottage industry has emerged over recent years manufacturing estimates of job loss that might result from
this rise of the robots. As an early contributor to this industry, I am at least partly to blame.
49
 The estimates
of gross job loss span a pretty wide range, but lie anywhere between 10% and 50% of the global workforce,
depending on whether it is jobs or tasks that are assumed to be displaced (Table 1 summarises).
Even at the lower end of this range, the societal impact would be significant. At the upper end, they would
be truly transformative. If the truth lies in between, this could still make the jobs loss from the Fourth
Industrial Revolution greater than its predecessors. And if so, then the potential societal costs – such as
rising wage inequality and threats to social cohesion – could also be as great as ever previously.
50

Some of the potentially powerful effects of automation on jobs and wages are already apparent. In the US,
each industrial robot per thousand workers has been found to reduce the employment rate by
0.2-0.3 percentage points and wages by 0.25-0.5%.51
 In Europe, results are more mixed.
52
 Research
suggests that technology may have been the largest single contributor to falling labour shares over the
recent past.
53
Whether these effects on jobs and pay will be temporary or permanent is far harder to judge. At this stage of
the technological cycle, the evidence is more likely to be picking up the shorter-term displacement effects of

48
 Haldane (2015b)
49
 Haldane (2015b).
50
 Schwab (2017).
51
 Acemoglu and Restrepo (2017).
52
 For example, Dauth, Findeisen, Südekum and Woßner (2017).
53
 IMF (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
technology than their positive longer-term effects on pay, productivity and the demand for new goods and
jobs. It is the balance of these two effects, each large in gross terms, which will ultimately determine where
unemployment will settle.54

No one could say with confidence today the long-run net effect will be. What can be said with confidence is
that the scale of gross displacement may be larger than ever-previously. This means that, if technological
unemployment is to be avoided, positive effects on productivity, goods and jobs will also have to be
commensurately larger. That has led some to argue that technological unemployment is more likely this time
around.55
Even if it is avoided, the societal costs of transition could well be larger, in terms of rising wage inequality, a
falling labour share and damage to social cohesion. And if that recessionary impact of technological change
on individuals and societies is sufficiently large, it could well call into question the merits of having pursued
the creative course in the first place. Or that, at least, is the lesson of history.
What experience since the Industrial Revolution has taught us is that this risk can be mitigated by an
appropriate institutional response. To be effective in curbing recession risk, that response should have as its
objectives, first, speeding-up the process of reskilling by workers (“enabling”) and, second, cushioning the
impact of new technologies on displaced companies and their workers (“insuring”).
In the face of mounting concern about the societal impact of the rise of the robots, quite a lot has been
written recently on the sorts of new insurance mechanism that might be required to smooth this transition.56

Here, I will focus on two areas where new “enabling institutions” could ease the transition for workers and
companies. Both draw on one of the greatest institutional inventions of the past millennium – universities.
In 1852, John (later Cardinal) Newman published The Idea of a University, a book that has since assumed
classic status as a statement of the principles by which universities should be run and organised. There are
many themes to that text. From these, I would highlight two which have particular relevance to the debate
about the future role of universities within our societies.
First, Newman emphasised universities’ role in providing a liberal education for students, by which he meant
access to a range of disciplines. Rigid specialism was to be avoided. The capacity to think was what
mattered. Second, the role of the university was to propagate ideas, rather than necessarily generate them.
The capacity to diffuse knowledge, rather than generate it, was what mattered. With a twist, both are
relevant to the Idea of a University in the 21st century. David Willetts, for example, emphasises the role of
universities with just as much vigour as Newman in his recent book, A University Education.

54 See, for example, Acemoglu and Restropo (2018) for an organising theoretical framework.
55 For example, Acemoglu and Restrepo (2017), Susskind and Susskind (2015).
56
 Martinelli (2017) and Standing (2017), for example, discuss universal basic income.
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
(a) Human Capital
Let me start with human capital, workers’ skills and experience. Two fundamental shifts in the jobs and skills
market are likely in the period ahead. The first is demographic. A young person born today can be expected
to live a 100-year life.57
 That being the case, it is likely they will have a career of 60, perhaps 70, years.
Given changes to the future world of work, multiple changes of career, not just job, are likely during a
lifetime. This has never before happened in human history.
The second secular shift is in the demand for skills. In the past, this skill-shift has been uni-directional.
There has been a secular rise in demand for cognitive skills and a corresponding decline in demand for
technical skills involving routine and repetitive tasks. That is why cognitive skills have attracted a rising wage
premium (Chart 6). Put differently, demand for skills of the “head” (cognitive) have dominated those of the
“hands” (technical) and, to lesser extent, those of the “heart” (social) over the past 300 years.
In the century ahead, those skill-shifts may be about to go into reverse. To see why, we need to ask
ourselves what sets of humans’ skills robots are likely to find it hardest to reproduce and replace in the
period ahead. My reading of the runes is that there are three areas where humans are likely to preserve
some comparative (if not always absolute) advantage over robots for the foreseeable future.
The first is cognitive tasks requiring creativity and intuition (“heads”). These might be tasks or problems
whose solutions require great logical leaps of imagination rather than step-by-step hill-climbing. Existing
machine learning algorithms allow huge numbers of solution permutations to be tried quickly and costlessly.
This allows these hill-climbing algorithms to scale local peaks in record time.
Whether they scale global peaks is altogether another matter. It is harder for machines to solve problems
where it is difficult to define the solution steps in a logical sequence in advance. Humans call solutions
arrived at in this way “intuition”. Despite rapid progress in deep learning techniques, it remains far-harder to
devise search algorithms which solve problems requiring a leap into the unknown.
58
Even in a world of super-intelligent machine learning, a demand is still likely to exist for people with the skills
to programme, test and oversee these machines. Some human oversight and judgemental overlay of these
automated processes is still likely to be needed. Machine design, improvement and oversight require, at
least for the moment, humans and machines to operate in partnership.
The second area of prospective demand for humans skills is bespoke design and manufacture (“hands”).
Routine technical tasks are relatively simple to automate and are already well on their way to disappearing.

57
 Gratton and Scott (2016).
58
 AlphaGoZero is one such attempt. It is an updated version of AlphaGo, a computer programme that plays the game ‘Go’, which itself
was the first software to beat a human professional player in full match.
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
But the same is not true of non-routine technical tasks – for example, the creation of goods and services that
are distinctive in their design, manufacture or delivery.
A rise in global wealth and income is likely to create an increasing demand for luxury goods and services of
this type, whose characteristics are unique and supply constrained. Indeed, this can be seen already in the
rising demand and price of rare art and artefacts and independently-produced foodstuffs and beverages. To
meet this demand, Mark Carney has spoken about the re-emergence of a new artisan class. If so, this would
mark something of a return to our pre-industrial revolution future.59
The third, and perhaps the biggest potential growth area of all, is social skills (“hearts”). That is, tasks
requiring emotional intelligence (such as sympathy and empathy, relationship-building and negotiation skills,
resilience and character) rather than cognitive intelligence alone. These are skills a robot is likely to find it
hard to replicate. And even if they could replicate them, humans might still prefer humans to carry them out.
The future could see a world of work in which EQ rivals IQ for skill supremacy. Professions involving high
degrees of personal and social interaction – such as health, caring, education and leisure – could see
demand rise. Indeed, it is possible the balance between cognitive and social skills might alter significantly
even among jobs which traditionally have been cognitively-intense.
Take medicine. The doctor of the future might be valued far less for their clinical competence in diagnosing
illness and prescribing solutions. In a world of individual medical records and data-hungry diagnostic
algorithms, much of the process of diagnosis and prescription might fall to machine rather than human.
Indeed, the UK Prime Minister has recently suggested a strategy to deliver just that.60
But that is unlikely by itself to eradicate the need for doctors. Patients are still likely to want to discuss their
diagnosis and prescription. And they will want this advice delivered personally and empathetically. In
surveys of patient satisfaction it is a doctor’s bedside manner, rather than clinical competence, that matters
most.61
 In future, that balance between social and clinical skills may shift further. And, most likely, those
social skills will be demanded from flesh-and-blood rather than robo-doctors.

If both the nature of a career and the skills it requires are changing, does this carry implications for
educational institutions? That seems likely. For decades, the primary focus of these institutions has been on
providing young people with cognitive skills. That model has worked well in meeting the needs of the
skill-shifts seen through each of the three industrial revolutions.

59
 Carney (2016).
60
 PM speech on science and modern Industrial Strategy, available here: https://www.gov.uk/government/speeches/pm-speech-onscience-and-modern-industrial-strategy-21-may-2018
61
 For example: https://vanguardcommunications.net/doctor-online-review-study/
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
Whether that model will meet the needs of the Fourth Industrial Revolution seems more questionable, for two
reasons. First, in future it seems very likely young and old alike will be in equal educational need – to train,
retrain and retrain again through their 60 or 70-year careers. Second, the skills these people require for the
future world of work will no longer be cognitive. Rather, they are likely to be more evenly balanced between
the cognitive, technical and social – head, hands and heart.
It has become rather trite to talk of the need for “life-long learning”. But never has the need for such learning
likely to have been greater given the longer span and greater volatility in future career paths. Making these
career transitions will itself call for a particular set of skills – personal resilience, problem-solving and
flexibility. As is now well-recognised, these character attributes are best instilled in early years.
If we now turn to our existing universities and colleges, at present these do not appear to be ideally sited to
meeting either of these secular shifts in educational and training need. By and large, they are not currently
configured as centres for life-long learning. And nor, in the main, are they institutions providing a balanced
educational offering of cognitive, technical and social skills.
To meet the needs of the future world of work, that might need to change. The importance of a broad-based,
Newman-style, education is likely to be greater than ever. This might criss-cross disciplinary boundaries, as
one way of increasing people’s ability to make giant logical leaps. Those skills will be social and technical
every bit as much as cognitive, with head, hands and hearts sharing equal billing.
Put this way, the future university may need to be a very different creature than in the past. It may need to
cater for multiple entry points along the age distribution, rather than focussing on the young. And it may
need to cater for multiple entry points along the skills spectrum, rather than focussing on the cognitive. It
would, in short, need to be plural rather than singular – a “multiversity”, rather than a university.
(b) Intellectual Capital
Multiversities are about spreading future skills across the workforce. Another educational challenge is how
to spread future technologies across companies. This is crucial if ideas and innovations are to become
GPTs, boosting productivity, pay and living standards. At present, evidence on the diffusion of new
technologies across firms paints a rather mixed picture.
To understand why, it is useful to recognise that two forces are at work when a new idea is diffusing through
the economy. First, there is adoption (or extensive margin) – the time it takes a new technology to first reach
a country or company. Second, there is penetration (or intensive margin) – the extent to which this
technology then reshapes processes and products within a company or country.
62


62
 Comin and Mestieri (2013a).
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
Take an example like computing technology or robotics. The adoption lag would measure the time it took for
a country or company to have their first computer or robot. The penetration rate would measure how many
computers or robots the country or company had and how intensively they were used. You would expect
both to be important to the process of technological diffusion and to boosting productivity. And, empirically,
both are typically found to be important to these processes.
63
So what does the evidence suggest about these twin diffusion processes? Looking at adoption lags of a
range of different technologies across countries, the time-series evidence suggests these have fallen over
time. Average adoption lags have shrunk from over a century in the 18th century, to perhaps 40-80 years in
the 19th century and 20-50 years in the 20th century.64
 It seems possible these lags may have fallen further
entering the 21st century, with cross-border flows of information and ideas having risen to new heights.
This is a good news story for productivity. Indeed, one of the key benefits of globalisation over the past
several decades is that the freer flow of goods and services, people and capital, ideas and information, has
served as a vehicle for the transmission and diffusion of new technologies across borders. Other things
equal, this should have accelerated catch-up between countries operating at and inside the technological
frontier.
65
Other things, however, have not been equal. Penetration rates of new technologies, once adopted, tell a
different tale. They suggest that, over time, there has been a divergence between frontier and non-frontier
countries. In other words, the intensity with which new technologies have been used has expanded more
rapidly in frontier (especially Western) economies than in non-frontier (non-Western) ones. This will have
retarded catch-up between the two blocs. It is a less encouraging story about the diffusion of innovation.
With the intensive and extensive margins of diffusion pushing in opposite directions, which has won out? It
seems to have been the forces of divergence: slower rates of penetration of new technologies have more
than counter-balanced faster rates of adoption across countries. This has shown up in levels of productivity
between frontier and non-frontier which, since around 1980, have if anything diverged.
Chart 7 illustrates that. It compares levels of productivity across 44 countries since 1950 relative to the
frontier country (the US). Convergence prior to 1980 has given way to divergence subsequently. That
divergence in levels of productivity has, in turn, translated into divergence in levels of GDP per head across
countries over the same period.
66
 The process of livings standards catch-up has stalled, possibly reversed.
And the reason for that appears to be a slowing in rates of technological diffusion.

63
 Comin and Mestieri (2013a).
64
 Comin and Mestieri (2013b).
65
 Baddeley (2006).
66
 Comin and Mestieri (2013b).
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
If there has been a slowing in rates of technological diffusion across countries, is the same potentially true
within countries? Evidence using micro-level company data within countries suggests it is. Research by the
OECD suggests there has been a slowing in rates of technological diffusion across firms in a number of
countries.67
 Put differently, there is a long and lengthening lower tail of companies failing to keep pace with
the technologies used in frontier companies. Their productivity performance has fallen ever-further behind.
This widening dispersion in the performance of upper and lower tail companies appears to be a particularly
acute in the UK. To a greater extent than elsewhere, it appears that the UK is a tale (or tail) of two
companies. At the upper end, the UK seems to have as many high-productivity companies as its main
competitors (Chart 8). That fits with the UK’s standing as a global innovation hub, with world-leading
companies and universities and high indices of innovation by international standards.68
That is more than counter-balanced, however, by productivity performance in the lower tail. This tail appears
to be both materially longer and lower than the UK’s main international competitors (Chart 8). Moreover, this
lower tail has lengthened by more over the past decade than in those other countries.69
 The already slower
rates of technological diffusion in the UK, by international standards, have slowed further over the recent
past.
These diffusion dynamics are important to productivity and living standards. The wider dispersion and longer
lower tail of companies helps explain the UK’s roughly 30% productivity gap with, say, France and Germany.
And the widening dispersion in productivity performance between the upper and lower tails of companies
helps explain the UK’s relatively poorer productivity performance than these countries over the past decade.
These diffusion dynamics are also consistent, or at least are not inconsistent, with the dawning of a Fourth
Industrial Revolution. This will have had the effect of shifting outward the technology and productivity frontier
for pioneer firms. There is clear evidence of that having happening. Productivity among firms in the top 1%
of the distribution grew, on average, by 8% per year over the ten years to 2014. The most-productive 0.1%
of firms exhibited 12% annual growth.
70

As a point of comparison, the lower 99% tail of firms have experienced annual average productivity growth of
less than 1% over the same period. In other words, looking at a cross-section of UK companies, secular
innovation (for the upper 1% or 0.1%) has co-existed with secular stagnation (for the remaining 99%). The
two are engaged, not so much in a secular struggle, as a secular stand-off. Technological diffusion, and
hence productivity growth, has been the casualty.

67
 Andrews, Criscuolo and Gal (2016).
68
 Department for Business, Energy & Industrial Strategy (2017).
69
 Haldane (2017).
70
 Productivity figures refer to the non-financial business sector, but other sectors for which data are not available over the full ten-year
period are also omitted.
All speeches are available online at www.bankofengland.co.uk/speeches
20
20
There have been one or two recent studies which have been quoted as questioning the importance of this
“long tail” issue for the UK.71
 These pieces use the same data used here. The problem arises not from the
data but from the (mis)interpretation placed on them by these studies. All of these studies show, very
clearly, that the UK has a significant long tail problem. Appropriately defined, they also demonstrate that this
tail has lengthened during the course of the crisis.
To give one example, my Bank colleague Patrick Schneider recently published a blog post showing that a
good chunk of the slowing of productivity growth since the crisis could be accounted for by workers in the top
quartile of the productivity distribution.
72
 This is entirely consistent with the tale of two companies. Indeed, it
is evidence of the lower tail of companies with flat-lining productivity having lengthened during the crisis and
by more in the UK than in competitor countries, consistent with the long tail hypothesis.
Why might the UK’s long tail, and hence technological diffusion, problem be more acute than elsewhere?
Several reasons have been posited and probed.
73
 One interesting window on this issue is provided by
looking at the intensive and extensive diffusion margins, to see if these provide clues. Looking at a range of
ICT technologies, the data suggests that high and relatively consistent levels of technological adoption, but
low and variable rates of technological penetration, mimicking the international evidence.
Take internet access. Almost all UK businesses – in excess of 90% – now have internet access. But the UK
ranks poorly compared to other OECD countries in terms of broadband penetration (Chart 9).
74 That is true
not only for standard broadband connectivity, but also for more advanced infrastructure such as optical fibre
connections. Those low rates of penetration largely reflect large/small firms differences. As with other
OECD countries, broadband usage by large firms is almost 100%. But for smaller firms that falls to 93%.
Similarly, the share of UK businesses with a website is above the OECD average, at around 83%. But that
masks sizable large/small firm differences. Almost 100% of UK large firms use websites, which has been the
case for a decade. Only two-thirds of small firms had a website in 2007. One may have expected that gap
to close rapidly. Even today, however, only around four-fifths of small firms have a website.
E-commerce is another technology that has arguably been surprisingly slow to propagate through the UK
economy. In 2016, over 80% of UK individuals ordered goods or services online, more than any other
OECD country. One may have expected that to have been mirrored by a large share of UK businesses
selling online. In practice, the UK is only marginally above the OECD average. And between 2009 and
2016, the share of UK businesses making e-commerce sales increased by only 6pp, from 17% to 23%.

71
 Swinney (2018), Giles (2018).
72
 Schneider (2018).
73 Haldane (2017), Jacobs et al (2017), Swinney (2018), for instance.
74
 OECD data.
All speeches are available online at www.bankofengland.co.uk/speeches
21
21
If the story, then, is one of diffusion dynamics being more sluggish in the UK than elsewhere, what might be
done to improve matters? Institutions, and within that universities, may well have a role to play. At present,
a relatively small sub-set of the UK’s universities operate as innovation hubs. They conduct world-leading
research using world-leading researchers. And, in an increasing number of cases, they are doing so in
tandem with businesses in spin-off science parks, acting as incubators of innovation.
What the UK may lack is some diffusion spokes to accompany these innovation hubs – spokes that could
help spread new and existing technologies, new products and practices to the long and lengthening lower tail
of companies. To be successful, these spokes would need to be repositories of knowledge and expertise on
technologies and their application to companies. And, ideally, these repositories would have regional and
sectoral reach to enable them to touch the long tail.
The UK’s existing university network would be one institutional solution to this spoke problem. They have
regional and sectoral reach. They also have embodied expertise and knowledge. By working in partnership
with local businesses, they could serve as repositories of expertise on good company practices – in
management, organisation, logistics, robotics, AI and the like. A new set of business parks attached to these
universities could serve, not so much as innovation incubators for new companies, but as diffusion clinics for
existing ones.
This idea goes with the grain of the earlier proposal; it would broaden the scope and purpose of universities.
In future, these would develop a broader set of capitals – beyond human capital (in people) to include
physical and intellectual capital (in firms). And they would do so throughout the lifecycle of companies, not
just in their early years. As it happens, this would also be consistent with Newman’s original conception of
universities as diffusion-engines, every bit as much as innovation-engines.
In Germany, a network of applied research institutes, run jointly by universities and industry, has been in
place since 1949. The Fraunhofer institutes now total 72.
75
 Their extensive project work means they can
apply technological solutions quickly to a wide variety of firms. Each year, they engage in between
6,000-8,000 projects, large and small, with companies. The Fraunhofer have been found to play a vital role
as both innovation hubs and diffusion spokes, boosting productivity in the German economy.
76

Since they first become operational in 2011, the UK now has around 10 Catapult centres, modelled on the
German Fraunhofer. These are smaller in number and scale than their German counterparts and have far
less project experience. Their focus, to date, has primarily been on acting as innovation hubs rather than
diffusion spokes. This means the UK does not currently have the diffusion infrastructure for its companies
enjoyed by Germany. This is not unimportant in explaining its longer lower tail of firms. The UK’s university
network, repurposed, could potentially provide that diffusion infrastructure.

75
 https://www.fraunhofer.de/en/about-fraunhofer/profile-structure/facts-and-figures.html
76
 Comin, Trumbull and Yang (2011).
All speeches are available online at www.bankofengland.co.uk/speeches
22
22
Conclusion
The story of growth is a story with two “i”s – ideas and institutions. The Fourth Industrial Revolution will
expand the range of ideas, perhaps more than any of its predecessors. It may also expand the range of
workers who suffer its side-effects, perhaps more so than any of its predecessors. In the past, new
institutions have emerged to cushion this painful transition, limiting the recessionary hit to societies.
Historically, doing so appears to have held the key to sustainable growth.
If this time’s technological transition is as great as any previously, securing sustainable growth will need new
institutions to manage this transition and mitigate its societal side-effects. I have speculated on one area
where that next institutional wave might usefully break – universities like this one, as new centres of lifelong
learning and technological diffusion. In future, institutional innovation will be every bit as important as
technological innovation if that gift of growth is to keep on giving. 

Most countries around the world now have targets for inflation, often pursued by operationally-independent
central banks. Inflation-targeting requires central banks to assess the future path of inflation, and the risks
around it, often through published inflation forecasts. Monetary policy is then set to meet the inflation target
in expectation, subject to these risks. This is the operational essence of flexible inflation-targeting.1
The Covid crisis has resulted in an unusually high degree of uncertainty about the future path of inflation.
This is clear from the confidence intervals around the MPC’s two-year-ahead inflation projections, which are
currently twice as large as normal. In its latest forecasts in February, the MPC judged there to be a
one-in-three chance of inflation lying below zero, or above 4%, at the two-year policy horizon (Chart 1).
This high degree of two-sided uncertainty is understandable. Economies have taken a huge hit as a result of
the Covid crisis, contracting by almost 8% in the UK and 4% globally during 2020. This has widened output
gaps and depressed inflation. In response, policymakers have provided unprecedented degrees of policy
support – in a number of advanced economies, double-digit percentages of GDP in both Quantitative Easing
(QE) and fiscal stimulus. These actions should stimulate activity, shrink output gaps and boost inflation.
Chart 1: CPI inflation projection
Source: Bank of England.
Notes: Projection from February 2021 Monetary Policy Report, based on market interest rate expectations, other policies as announced.

1
 For example, Batini and Haldane (1999).
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
3
3
It is these heavy weights on either end of the inflationary see-saw which give rise to unusually large
uncertainties around future inflation. If the hit to activity were to prove larger or longer, even this degree of
policy stimulus may prove insufficient to return inflation to target. But if economies bounce-back as the
vaccination programme is rolled-out, policy stimulus could over-stimulate the economy and, with it, inflation.
There are few, if any, historical precedents to help judge the response of the economy to this scale of shock
and degree of policy stimulus. And the costs of getting these judgements wrong could be significant. Costs
for the economy, if policy is set either too tight (resulting in lost jobs and income) or too loose (resulting in
policy needing to be tightened more than expected, causing future losses of jobs and incomes). And costs
for central banks in damaged credibility, if inflation targets were to be missed persistently.
In that light, it is worth assessing the likely forces acting on inflation in the period ahead and possible
alternative inflation scenarios, both to the upside and downside, around the central path. In a
highly-uncertain environment, where robust policy may call for avoiding tail outcomes rather than optimising
around a central path, this scenario-based or risk-management approach seems particularly valuable.2
I begin by discussing the powerful disinflationary forces at work over the past several decades at the global
level. There is a reasonable chance these trends could persist, and indeed be amplified by the Covid crisis,
posing downside inflation risks. In the following section, I then consider whether those disinflationary forces
could abate, or even reverse, in the period ahead. If so, this would generate an upside inflationary scenario.
Given high degrees of uncertainty, it would be spuriously precise to assign probabilities to either scenario.
But given the width of the MPC’s fan charts both are clearly plausible paths, sufficiently so to weigh in
monetary policymakers’ risk-management considerations when setting policy, now and looking forward.
Friedrich von Hayek once referred to inflation control as akin to trying to catch a tiger by its tail.3 That
metaphor seems apt today. For many years, the inflationary tiger slept. The combined effects of
unprecedentedly large shocks, and unprecedentedly high degrees of policy support, have stirred it from its
slumber. In this environment, the tiger-taming act facing central banks is a difficult and dangerous one.
A Low Inflation Equilibrium
The inflationary backdrop in the recent and distant past has been a benign one, in the UK and globally.
Since the 1970s, global inflation has fallen steadily. Having averaged around 10% in the 1970s and 1980s, it
fell to around 5% in the 1990s, just below 3% in the 2000s and around 2% in the 2010s (Chart 2)). After the
Great Inflation of the 1970s, low inflation has, within a generation, become an entrenched norm among

2
 For example, Orphanides and Williams (2007).
3
 Von Hayek (1972).
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
4
4
developed and emerging market economies alike. And despite large shocks – the global financial crisis,
Brexit, Covid - inflation has remained stable at levels considered to be around optimal
Chart 2: Global Inflation
Source: OECD and Bank calculations.
While success has many fathers, a significant contributor to this fall in global inflation has been the
widespread adoption of inflation targets by central banks. Around three-quarters of countries globally have
now adopted them.4 In the main, and contrary to the expectations of many at the time they were introduced,
these inflation targets have been hit. Indeed, in some advanced economies, central banks have
“over-achieved”, with inflation consistently undershooting their targets over the past decade.
Chart 3 plots the price level in the US, euro-area and the UK over the period since 1997, when the UK
commenced inflation-targeting. It also shows the price level path implied by meeting a 2% inflation target. In
the US and euro-area, the price level has drifted below this path over the past decade, as inflation has
undershot a 2% target. Cumulatively, these deviations have been significant, with the level of prices in the
US and euro-area 6% and 10% below their 2%-consistent paths respectively.

4
 See BIS Annual Economic Report 2019.
0
2
4
6
8
10
12
14
16
1971 1976 1981 1986 1991 1996 2001 2006 2011 2016
per cent
OECD inflation rate 70s 80s 90s 00s 10s
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
5
5
Chart 3: Price level in US, EA and UK5
Sources: ONS, FRED, ECB Statistical Data Warehouse and Bank calculations.
Interestingly, the UK experience has been rather different. Over the period since inflation-targeting was
introduced, the UK price level has not deviated significantly or persistently from its 2% trajectory. Unlike in
the US and euro-area, the MPC has neither consistently over-achieved nor under-achieved on its inflation
objective. That is consistent with the UK’s inflation target being a symmetric one.
There is some evidence of underlying measures of UK inflation having been subdued, both absolutely and
relative to target.6 Chart 4 shows some measures of underlying UK inflation over the past decade. While
they have fluctuated cyclically, these have more often undershot than overshot target-consistent levels over
the past decade. UK inflation expectations have, nonetheless, remained stable at low levels (Chart 5).
A number of reasons have been given for subdued levels of inflation, in the UK and globally, over the past
decade in particular. Some of these factors are cyclical, demand-side factors, while others are structural,
supply-side factors. Let me discuss each in turn.

5
 26/02/21: These series have been corrected for a basing error in the original version. 6
 Tenreyro (2020).
100
110
120
130
140
150
160
170
1997 2001 2005 2009 2013 2017 2021
Jan 1997=100
2% trend US PCE price level EA HICP level UK CPI price level
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
6
6
Chart 4: Underlying UK inflation relative to target-consistent levels
Source: ONS and Bank calculations
Chart 5: UK inflation expectations
Source: Bank of England, HMT Survey of Professional Forecasters and Citi/YouGov Household Expectations Survey
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2012 2013 2014 2015 2016 2017 2018 2019 2020 2021
CPI ex VAT Core services CPI ex VAT Core CPI ex VAT
Trimmed mean core SPPI
pp
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
7
7
On the demand side, a disinflationary drag factor over the past decade has been the persisting effects of the
global financial crisis. The recovery from this crisis was significantly slower than expected, with the level of
activity in the UK consistently undershooting expectations (Chart 6). This resulted in a larger and more
persistent output gap, and hence a larger and longer downdraught on inflation, from the demand side of the
economy than was anticipated. The same was true internationally.
Chart 6: UK GDP growth and historic forecasts
Source: ONS and Bank of England.
The sluggishness of the recovery in many advanced economies from the global financial crisis – the
so-called Great Recession - was striking even by historical standards. For example, the level of activity in
the UK lies very significantly below its level at the equivalent stage following the 1930s depression (Chart 7).
The UK’s Great Recession was far-worse than the Great Depression.
A number of factors are believed responsible for the persistence of weak global demand. One often given
prominence was heightened risk-aversion among lenders, consumers and businesses, which in turn
constrained credit and spending growth in the economy. This so-called “psychological scarring” of risk
appetite was also a dampener of credit and aggregate demand following the Great Depression.7

7
 Malmendier and Nagel (2011).
-8
-6
-4
-2
0
2
4
6
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019
per cent
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
8
8
On the supply-side of the economy, several factors have contributed to subdued global inflation over recent
decades. These include the rise of global trade and value chains which offer access to lower-cost inputs,
lowering the price of final imported goods and services8; demographic trends which have added to the
economy’s labour supply, lowering wage costs9; and increased automation and flexibility in the jobs market
holding down wage growth and potentially flattening Phillips curves.10
Chart 7: UK GDP after the Great Depression and Great Recession

Source: ONS and Bank calculations.
These structural factors would, in theory, be expected to cause a downward shift in the price level rather than
in the rate of inflation. But because these factors have played out over a lengthy period, they may have
shown up in a persistent drag on measured inflation rates. For example, Faijgelbaum and Khandelwal
(2016) estimate that global trade integration has lowered the price of the typical household consumption
basket by between a quarter and two-thirds – a huge effect.
This subdued pattern for global inflation, driven by demand and supply-side factors, has occurred against a
backdrop of global monetary policies remaining highly accommodative, itself in part a response to these
disinflationary forces. For example, in the aftermath of the global financial crisis, central banks lowered

8
 Johnson (2018).
9
 Goodhart and Pradhan (2020).
10 Haldane (2018).
-10
0
10
20
30
40
50
0 t+1 t+2 t+3 t+4 t+5 t+6 t+7 t+8 t+9 t+10 t+11 t+12
Cumulative
percent change
Years
The Great Depression The Great Recession
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
9
9
official interest rates to close to zero and expanded their balances sheets progressively by around $10 trillion
or 13% of global GDP (Chart 8).

Chart 8: Central Bank Balance Sheets
Source: Respective central banks and Bank calculations.
Interestingly, this rapid growth in central bank money creation was not mirrored in money spending growth in
the economy. In the ten years prior to the Covid crisis, narrow money globally rose by over 150%, while
money spending globally rose by less than 50%. In the UK this contrast was starker still, with central bank
money rising by around 200%, while money spending rose by less than 50%.11
One way of reconciling these diffuse trends in the monetary and real sides of the economy comes from
looking at a different measure of money - commercial bank money. Measures of broad (commercial bank)
money globally rose by only around 85% in the decade following the global financial crisis and by even less

11 Global figures are based on data on OECD M1, while UK figures based on central bank reserves plus notes and coins in circulation.
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020
USD (billions)
Fed ECB BoJ BoE SNB Total
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
10
10
in the UK. Put differently, the money multiplier in the UK – the ratio of broad money to central bank money –
fell by around two-thirds following the global financial crisis (Chart 9).
Chart 9: Ratio of Broad Money to Narrow
Source: Bank of England and Bank calculations.
This should come as no surprise. The global financial crisis impaired financial intermediation, slowed the
growth in bank credit and money and thus contributed to the sluggish growth in money spending.
Interestingly, the relationship between broad money growth and money spending growth – the velocity of
circulation of broad money – remained relatively stable following the global financial crisis (Chart 10). The
velocity of circulation of narrow money, by contrast, fell dramatically.
The Covid crisis has added to these long-standing disinflationary pressures. While affecting both demand
and supply, most economists believe the hit to demand from Covid has been larger than to supply, opening
up output gaps. The IMF estimate the output gap among the G7 countries rose from around -0.4% in 2019
to 3.8% in 2020. In the UK, the MPC judge that the output gap has widened and will continue to widen in the
near term, peaking at 3% and lowering inflation by around ½% in 2021.
0
5
10
15
20
25
30
35
06 07 08 09 10 11 12 13 14 15 16 17 18 19 20
M4ex/(Reserves+
Notes and Coins)
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
11
11
Chart 10: Velocity of Broad Money
Source: ONS and Bank of England.
There are contrasting views among economists about the pace at which these output gaps will close. The
MPC’s central view is that the UK’s output gap will close this year, although uncertainty around the timing
and pace is considerable. A key factor, as after the global financial crisis and the Great Depression, will be
the extent and persistence of psychological scarring among lenders, households and companies.
The deeper and longer-lasting these psychological scars, the slower the likely pace of recovery, the more
persistent the output gap and the greater the disinflationary drag from the Covid crisis. For example, if the
level of output in the UK economy were to return to pre-Covid levels in 2023, rather than 2022 in the MPC’s
central case, this would lower inflation by around 60 basis points at the two-year horizon, leaving it
comfortably below its 2% target.
In summary, then, historically low rates of inflation driven by disinflationary demand and supply-side forces
have been given further impetus by the Covid crisis. This provides reasonable grounds for believing
inflation, in the UK and globally, could remain low in the period ahead. Indeed, there are reasons to think
there are significant downside risks to the inflation outlook, as the MPC’s fan charts illustrate clearly.12

12 Evans et al (2015).
0
0.05
0.1
0.15
0.2
0.25
0.3
06 07 08 09 10 11 12 13 14 15 16 17 18 19 20
Nominal
GDP/Broad money
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
12
12
A Higher Inflation Equilibrium
Inflation-targeting requires a judgement on future inflation, not an extrapolation of the past. That is pertinent
at present because there are several good reasons why future inflation trends may not match the past. The
Covid shock is fundamentally different from the global financial crisis. And some of the forces acting on
inflation in future, both cyclical and structural, may reverse direction. Let me discuss these factors in turn.
(a) The Covid Crisis and the Output Gap
The restrictions needed to contain the spread of Covid have had a profound impact on both demand and
supply in the economy. Given the nature of the shock and the restrictions, it is unclear in principle whether
we would expect the hit to demand to be greater than to supply. In other words, the implications of the Covid
shock for the output gap, and hence inflation, are unclear as a matter of theory.
It is clear restrictions forced large number of businesses to close and large numbers of workers to be
furloughed, constricting supply in the economy. The crisis clearly also had a large adverse impact on the
demand for certain goods and services – for example, social spending due to health concerns and travel
spending due to working from home. The net effect of these large forces of supply and demand on slack in
the economy is an empirical judgement.
Empirically, the unemployment rate typically provides the cleanest guide to slack in the economy.
Unfortunately, that is not the case at present. Statistical surveys of employment are less reliable than usual.
Official measures of unemployment from the UK Labour Force Survey (LFS) show a fall in employment of
around 500,000 over the past year, giving an unemployment rate of around 5.1%. But administrative data
from HMRC suggest job losses 1½ times that, implying an unemployment rate of over 6%.
A further complicating factor is the various government support schemes. By design, these have broken the
link between the demand for workers by firms and the unemployment rate. At its peak, the UK government
was paying the wages of around a quarter of the workforce (9 million people) and is still doing so for around
4 million workers. Judging the true “employment status” of these furloughed workers, in particular whether
they are actively seeking work, is difficult to judge. It is also difficult to gauge how many of these furloughed
workers will return to their jobs once the furlough scheme ends.
That leaves the current unemployment rate, the likely future unemployment rate and the equilibrium
unemployment rate (taking into account the natural frictions associated with people moving job) all highly
uncertain at present. This means measures of labour market slack in the economy are also highly uncertain.
The same is true, for many of the same reasons, of measures of spare capacity within firms.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
13
13
With quantities uncertain, one indirect measure of slack comes from looking at the prices of labour, goods
and services. These, too, have faced measurement problems and have been distorted by sharp shifts in the
composition of employment and spending.13 Nonetheless, most measures of underlying inflation and costs
have been surprisingly resilient over the past year, given the scale of the contraction in activity. For
example, core CPI inflation rates have fallen by only 0.2 percentage points over the past year, while private
sector average weekly earnings growth has been roughly unchanged.
One possible explanation of this resilience in underlying costs and prices is that demand and supply have
fallen largely in lockstep, leaving slack in the economy relatively little changed. The design of the furlough
scheme, which has supported household incomes and aggregate demand at the same time as labour supply
has fallen sharply, would support that interpretation. While output in the UK is likely to have fallen by around
8% during 2020, household incomes are likely to have been broadly flat.
An alternative explanation lies in the heterogeneity of Covid’s impact on certain sectors, lowering demand
sharply for some, raising it for others. If the price effects of the latter have dominated the former – for
example, because of capacity constraints on expanding supply in some sectors – this might explain the
relative robustness of aggregate cost and price measures.14 The sectoral pattern of demand and prices
during the Covid crisis is consistent with this explanation (Chart 11).
 We have also seen these price dynamics at work in other markets as demand has recovered, notably in
commodity markets. The price of oil has risen by around 65%, and of non-oil commodities by more than
20%, in the last four months. This reflation reflects rising (actual and expected) demand for these
commodities, alongside short-run constraints on supply.
The MPC’s judgements on slack are shown in Chart 12, together with estimates of the output gap following
the global financial crisis. Despite the much larger hit to activity, the peak output gap following the Covid
crisis, at 3% of GDP, is materially smaller than following the global financial crisis. The output gap is also
judged likely to be significantly less persistent, lasting only around 18 months.

13 These compositional effects were discussed in the February 2021 Monetary Policy Report. 14 Broadbent (2021).
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
14
14
Chart 11: Demand and Prices across Sectors (a)
Source: ONS and Bank calculations.
Notes: a) Data points refer to goods and services that have been predominantly affected by a demand shock. These are spending
categories where consumption and inflation have both increased, or where they have both decreased. Categories affected by changes
in energy prices and those where prices are regulated are excluded. Air travel is also excluded. Dashed line represents the line of best
fit.
(b) Changes in inflation rates between the three-month average to February 2020 and October 2020, measured as the number of
standard deviations from their 2012–19 average.
c) Consumption growth over the four quarters to 2020 Q3.
These differences in output gap profile seem plausible. The fall in output after the Covid crisis was atypically
sharp due to restrictions being imposed. So we might expect an atypically sharp recovery in activity as
restrictions are eased. Nonetheless, whether these judgements on the output gap come to pass depends,
ultimately, on the pace at which demand in the economy returns and the accompanying response of the
economy’s supply potential. Let me discuss those two issues in turn.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
15
15
Chart 12: Output gap – Global Financial and Covid Crises Compared
Source: Bank of England.
(b) Money and Money Spending
I will discuss in some detail the components of private demand, consumption and investment, at a future
event. But as after the global financial crisis, some useful insights into possible future paths for aggregate
demand and aggregate money spending is provided by looking at developments in the banking system.
Exceptional amounts of central bank money have once again been created during the Covid crisis to
maintain borrowing costs at low levels. This additional QE is running at around $5 ½ trillion so far and
rising.15 Globally, the amount of QE undertaken during the crisis is already rapidly catching-up with the
amount undertaken during the prior ten-year period. And in the UK, QE so far announced by the MPC, at
£450 billion, is already more than was carried out in the 10-year aftermath of the global financial crisis.

15 Based on Federal Reserve, ECB, Bank of England, Bank of Japan and Swiss National Bank.
-5
-4
-3
-2
-1
0
1
t=0 t+5 t+10 t+15
per cent
Quarters
GFC Covid (forecast)
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
16
16
One key difference from the global financial crisis is the state of the banking system. A decade ago, its
impairment led to slow growth in bank deposits and lending. With the financial system now repaired, bank
intermediation has increased sharply during this time’s crisis. Supported by exceptional monetary and fiscal
support, bank deposits and lending have grown rapidly. Annual growth of global M3 is currently running at
almost 20% (Chart 13), its highest level since 1988.
Chart 13: Growth in Global M3 (YoY)
Source: OECD.
Notes: Based on total OECD aggregate.
The same is true in the UK, where annual growth of commercial bank money (M4) currently stands at a little
over 14%. Annual growth of M4 lending is more modest, at just over 4%, though this is still around 2
percentage points above its average over the past decade. It has been buoyed by the support packages
extended to UK companies, with around 1.5 million business loans made totalling around £70 billion.
A decade ago, movements in broad money broadly tracked movements in money GDP – the velocity of
circulation of broad money was roughly stable. If the same is true now, we would expect the recent rapid
rise in broad money growth to translate, in time, into higher money spending growth. In fact, the projections
recently published by the MPC for UK nominal GDP growth over the next year are already in double-digits.
0
2
4
6
8
10
12
14
16
18
20
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
per cent
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
17
17
Another way of interpreting these rises in the money supply is as the financial counterpart of the excess, or
involuntary, savings accumulated by households and companies during the Covid crisis. These reflect the
combined effects of restrictions on spending at the same time as relatively stable incomes. In the US, these
excess savings total around $1½ trillion and in the euro-area almost $½ trillion, although these savings are
unevenly distributed across household types.
In the UK, the picture is much the same. Excess savings currently total around £150 billion for households
and over £100 billion for companies, with the lion’s share of these savings are in highly liquid bank deposits.
This leaves money growth, or accumulated liquid savings, in a very different position than at the time of the
global financial crisis (Chart 14).
Chart 14: M4 after the Global Financial and Covid Crises
Source: Bank of England and Bank calculations.
It is unclear what fraction of these savings will be spent. As elsewhere, these savings are highly unevenly
distributed across UK households. In its latest projections, the MPC assumed around 5% of savings in
aggregate would be spent. This is a conservative estimate. Milton Friedman famously described money as
90
95
100
105
110
115
t t+6 t+12 t+18 t+24 t+30 t+36
Start of
episode=100
Quarters
GFC Covid
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
18
18
a temporary abode of purchasing power.16 If that definition holds true, by the middle of this year as much as
£400 billion of savings by households and companies (around a third of annual GDP) will be seeking a new
home, whether physical or financial assets or goods and services. This would provide a very significant
degree of additional demand stimulus to an already rapidly-recovering economy.
(c) The Shifting Supply-Side
To what extent any rise in demand translates into higher inflation depends, ultimately, on the performance of
the supply side of the economy. Over recent decades supply-side improvements, including increased trade,
automation and population, have tended to relax supply-side constraints, dampening inflationary pressures.
But some of these global forces now seem set to slow, perhaps even to reverse.
In their recent book, Charles Goodhart and Manoj Pradhan discuss how, following a long period when global
demographic trends supported growth in the workforce, these trends are likely to reverse. 17 As the
population ages and workers retire, labour supply growth will weaken and the bargaining power of workers
will strengthen. This would cause both an inward shift, and steepening of, the Phillips curve, raising the price
level and increasing its responsiveness to upward demand pressures.
The UK has been a particular beneficiary of demographic trends over recent years. The UK working age
population has risen, on average, by over ½% per year over the past two decades. It has also experienced
rising rates of labour market participation, in particular among older workers and women.
The first, and possibly both, of those factors are now in reverse gear. The share of the total UK population of
working age has been falling since 2008. Globally, dependency ratios – the fraction of the population not in
the workforce – is forecast to start rising from around now for the first time since the 1970s (Chart 15). This
rise in dependency ratios is fastest in high-income countries, such as the UK.
Until recently, this effect had been more than offset by rises in the UK population. But that, too, is changing.
Recent estimates suggest the UK population may have shrunk by as much as 1.3 million people over the
past year.18 If these demographic trends continue – a smaller population fewer of whom are in work – this
would turn the demographic tailwinds of the past decade into a headwind looking ahead, constraining supply
and potentially amplifying inflationary forces.

16 Friedman (1961).
17 Goodhart and Pradhan (2020).
18 See O’Connor and Portes (2021). The authors themselves describe this estimate as an upper bound.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
19
19
Chart 15: Global Dependency Ratios
Source: United Nations, Department of Economic and Social Affairs, Population Division
Note: Dependency ratio is population aged 0-19 or 65+ per 100 aged 20-64
A second supply-side effect working in the same direction is globalisation. The world witnessed many
decades of under-interrupted growth in world trade and global value chains after the Second World War,
much of it associated with the rising role of China in the world economy.19 On average since 1983, global
trade volumes have risen by around 5½ % per year, roughly double the rate of growth of the world economy.
The Covid crisis has decisively broken those trends. Global trade volumes collapsed by around 15% at the
peak of the Covid crisis, compared to a fall in global activity of around 9% (Chart 16). The crisis saw the
fracturing of some global value chains, in part as a result of countries prioritising domestic over international
supply of some goods and services.
With global trade barriers tending to rise over recent years, and with Covid having provided a further impetus
towards localisation, it seems unlikely globalisation will remain as powerful a disinflationary force in the future

19 See, for example, the work of Richard Baldwin here and here.
0
20
40
60
80
100
120
140
160
1950
1955
1960
1965
1970
1975
1980
1985
1990
1995
2000
2005
2010
2015
2020
2025
2030
2035
2040
2045
2050
2055
2060
World High-income countries
 Middle-income countries Low-income countries
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
20
20
as it has been in recent decades. And it is certainly possible trends in globalisation could even go into
reverse in the period ahead, adding inflationary impetus.20
Chart 16: Global Trade and Growth
Source: IMF WEO, World Bank, Bank calculations.
On top of these long-standing supply-side forces came the Covid crisis. One of the casualties of this crisis,
from a supply-side perspective, is likely to be capital formation. The MPC’s projections assume around a
6½% hit to the capital stock from Covid, and a long-term scar on the UK economy’s supply-side of around
1.75%. This will also serve to tighten supply-side constraints as demand increases.
There are very significant uncertainties either side of these scarring estimates. On the one hand, the hit to
capital could be larger if firms’ risk appetite remains subdued or if the debts accumulated during the crisis act
as a drag on investment. For example, in the UK Office for Budget Responsibility’s (OBR) downside
scenario, scarring reduces output by as much as 6%.

20 Baldwin and Freeman (2020).
-20
-15
-10
-5
0
5
10
15
20
1983
1985
1987
1989
1991
1993
1995
1997
1999
2001
2003
2005
2007
2009
2011
2013
2015
2017
2019
per cent
World Trade World GDP
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
21
21
On the other hand, it is possible investment and the capital stock rebounds faster than in a typical recession
– for example, if there is catch-up investment or if firms seize the opportunity created by the Covid crisis to
improvement their digital estates and digital skills. This would reduce, but not eliminate, any scarring. The
balance of these effects on investment is something I will discuss in a future lecture.
(d) Fiscal Policy
After the global financial crisis, the major tool of macro-economic demand management was monetary
policy, which was loosened significantly in most countries. By contrast, fiscal policy in most countries was
loosened only modestly and in some, including the UK, was tightened. This fiscal response added to the
burden placed on monetary policy.
The fiscal policy response to the Covid crisis has been very different. Although monetary policy has once
again been loosened, fiscal policy has played the frontline role in stabilising the economy. Both the
fiscal/monetary mix, and the scale of the all-in supporting policy response, has as a result far exceeded that
following the global financial crisis.
Table 1 shows peak levels of the primary fiscal deficit in the UK during the global financial and Covid crises.
The degree of fiscal support has plainly been materially larger – more than twice as large. And relative to the
size of the economy’s output gap, which is smaller than at the time of the global financial crisis, this degree
of fiscal support has been larger still and multiplies of the output gap.
Table 1: Peak Fiscal Deficits in the Global Financial and Covid Crises
Peak primary balance (a) Peak output gap (b)
Global Financial Crisis -8.3 -4.2
Covid Pandemic -18.0 -3.1
Source: OBR and Bank of England
Note: (a) % of GDP. (b) % of potential GDP. Fiscal numbers for pandemic are based on OBR November 2020 forecast for FY 2019-20.
The appropriate degree of fiscal support to close the output gap has recently been a source of debate in the
United States.21 There, the fiscal response is prospectively larger than in the UK, and the output gap
smaller, meaning the upside risks to demand and inflationary are likely to be larger.


21 For example, see Olivier Blanchard here.
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
22
22
(e) Psychological Scarring
A final potential difference from the past concerns the degree of psychological scarring of risk appetite
among household and companies as a result of crisis. At the time of the global financial crisis, as after the
Great Depression, people’s appetite for risk was blunted for a sustained period. This led to long-term
scarring of both the demand and the supply sides of the economy, slowing the recovery.
I think the behavioural response to the Covid crisis will be different from those past episodes. Financial
crises are costlier and longer-lived because of their balance sheet impact. Holes in balance sheets need to
be filled, often through deleveraging, leaving lasting financial and psychological scars. By contrast, the
Covid crisis has resulted in stronger, not weaker, balance sheets, at least for the average household and
company. This is likely to increase, not reduce, their willingness to take risk.
Low risk appetite after the global financial crisis was a correction of the excessive risk-taking that took place
in the run-up – an overshoot followed by an undershoot. The Covid crisis has seen the exact opposite.
People’s appetite to spend and socialise has been artificially suppressed. This increases the chances of an
overshoot as restrictions are removed, as after previous episodes of suppressed animal spirits. Riskswitching is, for me, more likely than risk-scarring.
Conclusion
At present, inflation pressures in the UK and globally are subdued. In the UK, inflation remains well below its
2% target, as it is in the US and the euro-area. In all three countries, inflation expectations among
consumers, businesses and financial markets appear to be well-anchored around the inflation target,
although they have fluctuated recently.
The next 12 months will see a rapid change in measured inflation. In the UK, it is likely to return to its target
by the middle of the year, as the one-off effects of Covid wash out of annual inflation rates. Thereafter, the
MPC foresees UK inflation remaining around its target level. The risks around this projection are, in the
MPC’s judgement, large but balanced.
That these risks are large is fully justified by the high degrees of uncertainty that exist around the forces
driving inflation in future, as I have discussed here. But given likely trends in these factors, my judgement is
that the risks to inflation in the UK are skewed to the upside, rather than being balanced. Certainly, there are
good grounds for believing future inflation may behave very differently than in the past.
My judgement is that we might see a sharper and more sustained rise in UK inflation than expected,
potentially overshooting its target for a more sustained period, as resurgent demand bumps up against
All speeches are available online at www.bankofengland.co.uk/news/speeches and @BoE_PressOffice
23
23
constrained supply. Financial markets globally have begun pricing this possibility recently, with measures of
inflation expectations rising in the US and, to lesser extent, euro-area (Chart 17).
Chart 17: Tail Risk in US and Euro-area Inflation Expectations.
Source: Bloomberg Finance L.P and Bank calculations.
Note: Tail risk is the option-implied inflation tail risk, 5-years ahead measured at the 95th percentile.
Inflation is the tiger whose tail central banks control. This tiger has been stirred by the extraordinary events
and policy actions of the past 12 months. It is possible that, as vaccinations are rolled out and some degree
of normality returns, inflation will return to a stable state of rest. Indeed, if risks from the virus or elsewhere
prove more persistent than expected, disinflationary forces could return.
But, for me, there is a tangible risk inflation proves more difficult to tame, requiring monetary policymakers to
act more assertively than is currently priced into financial markets. People are right to caution about the risks
of central banks acting too conservatively by tightening policy prematurely. But, for me, the greater risk at
present is of central bank complacency allowing the inflationary (big) cat out of the bag. 

It is wonderful to be here at the University of Sheffield, my alma mater, to give the SPERI Annual Lecture. I
have been a great admirer of SPERI’s work over the years. It has played an important role in shaping the
conversation about public policy, economically and socially, locally and nationally. My topic today echoes
many of the themes SPERI has championed – the role of local perspectives and policies in better
understanding the economy and in shaping a better society.
Tip O’Neill, the former Speaker of the United States House of Representatives, famously said “all politics is
local”. Those words probably apply to politics everywhere. They have also probably been true at every point
in history. But at this particular point in history, with local or national identities becoming an increasingly
potent force shaping our economies and societies, those words have a particular resonance.
That set me thinking. Does the O’Neill hypothesis hold for our economies too? Is all economics local? Are
our economies better understood as a nation-state solid or as a set of loosely-linked local clusters? Might
this have implications for how we map, model and manage the economy? And might it even have
implications for central banks, despite their policies operating nationally rather than locally?
I want to explore these questions, tentatively and provisionally, first by drawing some simple maps of the
economy at different spatial resolutions. These maps provide some insight into how “local” economies really
are. How large are differences between-region? How do they compare with differences within-region? And
what do we know about flows of factors, including people, between and within regions?
This local lens poses some interesting questions about how the economy should be modelled. Existing
models of the macro-economy operate at a high level of aggregation. When is this telescopic lens on the
economy the most useful one? When might a microscopic, or local, lens offer additional insights? And how,
practically, can those microscopic perspectives be pieced together to give a macroscopic overview?
When it comes to managing the economy, this has been fertile ground for policy debate recently. Several
reports have made the case for rethinking local economic policies, including Michael Jacobs’ work as part of
the IPPR Commission on Economic Justice.1
 The Government has put “place” centre-stage in its own policy
agenda.
2
 I discuss ways in which analytical techniques and data might help in shaping those policy choices.
Some areas of public policy can only operate at the national rather than local level, such as monetary policy.
Even then, however, a local perspective can be important for better understanding the economic issues
people face and for building understanding and trust among those people. That is why the Bank of England
has recently augmented its own local initiatives, as I will discuss. Other central banks are following suit.

1
 IPPR Commission on Economic Justice (2018). See also Collier (2018).
2
 Department for Business, Energy and Industrial Strategy (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
In his recent book, Raghu Rajan blames neglect of the Third Pillar of society – community – relative to the
market and the state (the first two pillars) for rising societal disconnection and mistrust.
3
 Rajan is right.
Economic policymakers, including central banks, have a pivotal role to play in resurrecting that Third Pillar, in
making economic policy local, to better support our economies and societies. Here is how.
Mapping the Economy
Let me begin with some simple mapping of the economy. The most widely-used metric of economic success
is Gross Domestic Product (GDP).4
 In simple terms, this measures how much the average person has
available to spend on the good things in life. If asked, most people would say this is a decent, if partial,
proxy for their overall economic health, even if they express bafflement at the concept of GDP.
Aggregate GDP is far from being the only possible proxy for economic health. The lived experience of most
people depends on more than their income. A rich strand of literature has developed alternative measures of
people’s well-being. As well as economic and financial health, these measures include physical and mental
health, their families, friends and communities.
5
 This work tells us that people’s sense of well-being, if not
their GDP, is shaped by local factors, underpinned by the Third Pillar.
Another reason why aggregate GDP may not chime with most people’s lived experience is because, by
definition, most people are not “average”. Economists use the idea of the average or representative agent
as a convenient shortcut. But that agent is a fiction. People’s lived experiences often differ very materially,
even within a single country or region – spatially, socially and financially. For most people’s everyday lives
and everyday decisions, all economics is local.
These imperfections in GDP are well-understood. They certainly do not fatally undermine its usefulness as a
means of keeping economic score. They do, however, suggest it can usefully be complemented by looking
at the economy through different lenses. One of those different lenses comes from mapping the economy
bottom-up, rather than top-down, aggregating microscopic experiences into a macroscopic view.
In medicine, we use a variety of different tools, at different resolutions, to diagnose problems and when
prescribing solutions: thermometers, blood pressure monitors, X-rays, CT scans, ultrasound and blood tests.
Rarely does one of these measures provide all of the diagnostic answers. Using them in combination can,
however, help reach robust clinical conclusions. And that “micro-to-macro” approach is commonplace when
understanding other complex adaptive systems like the body, natural, physical and social.

3
 Rajan (2019).
4
 Coyle (2014).
5
 For example, Helliwell, Layard and Sachs (2019), Ngamaba (2017) and Commission on the Measurement of Economic Performance
and Social Progress.
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
Like our bodies, the economy is also a complex, adaptive system. As in medicine, it can be measured using
different tools at different resolutions. There are various metrics of societal health, in addition to GDP. For
example, there has been increasing interest recently in measures of physical and mental health.6
 A number
of statistical agencies, including in the UK, now gather direct and indirect metrics of people’s well-being.
Here we map a selection of those alternative metrics, based around health, wealth and happiness.
A different sort of lens comes from viewing these metrics at different spatial resolutions – regional, local
authority, postcode even. This allows a “micro-to-macro” jigsaw of the economy to be pieced together, as
with other complex systems. Patterns in these systems often self-replicate at different resolutions; they are
“fractal”. 7
 One interesting question is whether economic systems also exhibit self-replicating patterns. If so,
this has implications for how we understand and model them.
We consider these questions using a sequence of maps. Many of these maps are not new. For example,
Philip McCann from the University’s Productivity Insights Network (PIN) has published an excellent book
setting out a range of fascinating regional facts and maps. So too have others.8
 These maps provide useful
context when testing the Tip O’Neill hypothesis and when modelling and managing the economy.
To provide the clearest visual guide, these maps are scaled by economic rather than geographic size
– so-called cartograms.
9 Each region is initially scaled to have the same geographic area and then rescaled
in line with economic differences. This means some areas shrink (such as Scotland) while others expand
(such as London) even without differences between them. This helps when visualising local differences.
(a) Differences between regions
To fix ideas, Charts 1a and 1b plot a map and an accompanying cartogram of population by region in the UK
and, by way of comparison, Germany. It uses a resolution one-level below the national – administrative
“regions” in England and each of Scotland, Wales and Northern Ireland. Comparing the map and cartogram
suggests the UK has a significant demographic skew towards London and the South-East. Germany has
nothing like the same skew, with the cartogram and map little different.
10
A key focus of work among economic geographers is the importance of agglomeration effects – that is,
increasing returns to economic scale at the spatial level.11
 If those effects are powerful, and if workers are
one of their most important sources, we would expect cartograms to have similar properties when looked at
by population, income (per head) and wealth (per household). More should not only be merrier but wealthier.

6
 For example, Case and Deaton (2017).
7
 For example, Viscek, Shlesinger and Matsushita (1994).
8
 McCann (2016), Centre for Cities (2019), What Works Centre for Local Economic Growth (2018).
9
 See Dougenik, Chrisman and Niemeyer (1985).
10
 Put differently, the correlation between geographic area size and population in the UK is in fact slightly negative (-0.05) while it is
relatively high and positive (0.7) in Germany.
11
 Viladecans-Marsal (2004) and Rosenthal and Strange (2003), example.
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
Chart 1a: Map (LHS) and cartogram (RHS) of population in the UK
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
Chart 1b: Map (LHS) and cartogram (RHS) of population in Germany
Sources: Eurostat and Bank calculations.
Notes: Charts show population in 2018 for NUTS 1 regions. Cartograms resized by population.
As Charts 2 and 3 show, broadly-speaking that is the case. The income per head cartogram suggests a
shrunken North and a swollen South. The gap between richest and poorest region is around 150%. For
wealth per household, the North-South divide is also large, with a gap between richest and poorest of 130%.
These are striking differences. But are they unusual compared either with the past or other countries?
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
Chart 2: Cartogram of UK income per head
Sources: Eurostat and Bank calculations.
Notes: Purchasing power standard (PPS) per inhabitant for NUTS 1 regions.
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
Chart 3: Cartogram of Great Britain wealth per household
Sources: ONS Wealth and Assets Survey and Bank calculations.
Notes: Data refer to 2014-16 for NUTS 1 regions, re-sized and coloured by median household wealth. No equivalent
data available for Northern Ireland.
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
On the first of those, Chart 4 plots the distribution of income per head across UK regions on three dates:
1997, 2007 and 2016. All of the distributions are centred on mean levels of income in 2016. This suggests a
modest increase in the degree of regional income dispersion over time. The 2016 distribution has somewhat
fatter tails, lower and upper, than in earlier periods.
12
Chart 4: Distribution of UK income per head over time (mean-aligned)
Sources: ONS and Bank calculations.
Notes: Chart shows regional gross disposable income for different time periods at current basic prices, centred around
the 2016 mean. Observations for each region are weighted by the population of each region.
Chart 5 compares the regional distribution of income per head in the UK, France and Germany in 2016,
again centred on mean income levels in the UK in 2016. It suggests a materially wider regional dispersion in
incomes in the UK than in France and Germany. The gap between richest and poorest regions in the UK is
almost twice as large as in France and three-quarters larger than in Germany.13


12
 Comparing NUTS 3 regions.
13
 Figures refer to income differences at the NUTS1 regional level, using Purchasing Power Standard (PPS).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
Chart 5: Distribution of UK, France and Germany income per head in 2016
Sources: Eurostat and Bank calculations.
Notes: Purchasing power standard (PPS) per household in NUTS2 regions for 2016 centred around the UK mean.
Excludes NUTS region ‘FRY’ (‘Départements d'outre mer').
These regional differences in GDP are found in alternative metrics of economic health, such as pay
(Chart 6).
14
 They are also mirrored in medical health. Chart 7 plots one measure of medical health by region
– average life expectancy. This mirrors regional income differences. Richer people tend to live longer. The
gap between highest and lowest life expectancies regionally is around 3.4 years, or 4.2% of the UK average.

14
 Haldane (2019).
Oberbayern Inner London -
West
Île de
France
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
Chart 6: Cartogram of UK pay
Sources: ONS Annual Survey of Hours and Earnings (ASHE).
Notes: UK NUTS 1 regions re-sized and coloured by median gross weekly earnings (£) in 2018.
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
Chart 7: Cartogram of UK life expectancy
Sources: ONS and Bank calculations.
Notes: Life expectancy at birth by local area in the UK (years) for UK NUTS 1 regions, re-sized and coloured by mean
life expectancy in 2010-12.
If we turn to an alternative measure of health – psychological health – the picture changes dramatically.
Chart 8 plots a subjective measure of well-being on a regional basis.15
 This is the mirror image of the
income and wealth maps. Well-being is lowest in the richest region (London) and highest in a region where
incomes are low (Northern Ireland). Income may buy you many things, but happiness is not one of them.16

15
 This is based on answers to the question: “Overall, how satisfied are you with your life nowadays?” Other questions regarding wellbeing yield very similar conclusions.
16
 Easterlin (1974) and Easterlin (2013), for example, point out that while wealthier people tend on average to be happier, happiness
does not increase in line with GDP per head. Unemployment and debt arrears also have a large adverse psychological impact on
individuals, above-and-beyond their direct impact on income and wealth (Wildman (2003) and Taylor et al (2007)).
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
While simple, these maps suggest there is important information on economies and societies to be found by
peering through different lenses: lenses that operate at different resolutions, as the striking UK regional
differences in income, wealth and health demonstrate; and lenses that focus on different economic
attributes, as the striking differences between the UK’s regional income and well-being maps demonstrate.
Chart 8: Cartogram of UK well-being
Sources: ONS Annual Population Survey (APS) and Bank calculations.
Notes: Responses to the question, “overall, how satisfied are you with your life nowadays?” where 0 is ‘not at all
satisfied’ and 10 is ‘completely satisfied’ for 2016/17. UK NUTS 1 regions re-sized and coloured by mean response.
(b) Differences within regions
Given these striking regional differences in health, wealth and happiness across the UK, an interesting
supplementary question is how those differences compare within-regions. As we alter the resolution at
which we view the economy, do these differences narrow? Do economic patterns look different when
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
comparing them within and between regions? Or are these patterns self-similar at different resolutions, as
when we use a microscope to study the shoreline?
Chart 9 provides one visualisation of these patterns at different resolutions. These “violin” plots compare the
distribution of income per head at the regional (NUTS 1) level with the distribution at NUTS 2 and NUTS 3
level. If between-region income differences were much larger than within-region differences, we would
expect the violin to be much longer on its left than on its right hand side.
This is clearly not the case. The distributions of income within and between regions are in fact quite similar,
at both the NUTS 2 and 3 levels. If anything, the distribution of outcomes within each region appears to be a
little larger than the distribution between regions. If anything, income inequalities within a region appear to
be larger than income inequalities between regions.
Chart 9: ‘Violin’ plot of GVA per head
Sources: ONS and Bank calculations.
Notes: Regional gross value-added (GVA, income approach) per head of population at current basic prices for 2017 for
NUTS 1, 2 and 3, log-scale. Regional figures weighted by population. Dark bar shows interquartile range, dots show
median and circle shows mean.
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
Charts 10, 11 and 12 are violin plots for wages, health and well-being at higher resolutions. For health and
well-being, differences within a region or local authority are materially larger than differences between
regions. The standard deviation of health outcomes at the local authority level is almost 3 times larger than
at the regional level. The difference between the highest and lowest life expectancy among UK local
authorities is 12 years, or 15% of the average expected lifespan.
Chart 10: ‘Violin’ plot of wages
Sources: ONS Annual Survey of Hours and Earnings (ASHE) and Bank calculations.
Notes: Median gross wages in 2018. Regional figures weighted by number of jobs per region. Dark bar shows
interquartile range, dots show median and circle shows mean. No Northern Ireland wage data available at NUTS 3 level.
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
Chart 11: ‘Violin’ plot of health (life expectancy)
Sources: ONS and Bank calculations.
Notes: Life expectancy at birth for NUTS 1, 2 and 3 regions in 2010-12. Regional figures weighted by population in each
region. Dark bar shows interquartile range, dots show median and circle shows mean.
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
Chart 12: ‘Violin’ plot of well-being (life satisfaction)
Sources: ONS Annual Population Survey (APS) and Bank calculations.
Notes: Responses to the question, “overall, how satisfied are you with your life nowadays?” for NUTS 1, 2 and 3 where
0 is ‘not at all satisfied’ and 10 is ‘completely satisfied’ for 2016/17. Regional figures weighted by population in each
region.
Chart 13 plots health perceptions by electoral ward in four big UK cities: Belfast, Cardiff, Edinburgh and
(inner) London.17
 The differences are big, with nearly twice as many people perceived to be in bad or very
bad health in Belfast than Edinburgh. Even more striking are the differences in the same city, however, with
adjacent districts reporting bad health differences of between 3% and 25%.

17
 Data refer to percentage of people in the census rating themselves in “bad” or “very bad” health.
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
Chart 13: Heat maps of health in Belfast, Cardiff, Edinburgh and Inner London
Sources: 2011 Census.
Notes: Data show percentage of respondents declaring themselves to be in bad or very bad health at electoral ward
level.
I draw two conclusions from these micro-level visualisations. First, however striking the regional differences
in economic and societal health across the UK relative to historical and international standards, these
conceal even more striking differences in levels of health, wealth and happiness within regions. To the
extent there is an inequality issue in the UK, it is as much or more a local rather than regional one.
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
A second, nerdier, point is what this means for our understanding the economy. Many systems – social,
natural and physical – exhibit a pattern in which the distribution of outcomes does not converge at higher
resolutions; it remains constant or even expands. These systems have a fractal property with patterns that
are self-similar at different levels of resolution. Snowflakes and shorelines are the best-known examples.
The violins charts suggest economic systems may also exhibit some of these properties, with distributions
which do not converge at higher resolutions and may even expand. Outcomes in these systems are not
distributed like a bell-shaped, Normal distribution. Instead, outcomes are drawn from a Power Law
distribution, which has fatter tails and a much greater probability of extreme outcomes.18
Chart 14 plots the distribution of health, wealth and happiness across the UK. The straight line shows where
outcomes would lie if they were Normally-distributed. Deviations from that line, at its extremities, signify
fat-tails. For health and income there is strong evidence of fat tails and Power law distributions.19
Chart 14: Power Law tests
GVA per head Well-being

18
 Haldane (2012) discusses Power Law distributions as they apply to economic and financial variables.
19
 Haldane (2012).
All speeches are available online at www.bankofengland.co.uk/speeches
20
20
Wages Health (life expectancy)
Sources and notes: as per violin plots in Charts 9-12.
Fat-tails and Power Law distributions are associated with systems that are complex and adaptive. The best
definition of complexity is, for me, the one provided by Herbert Simon back in 1962: “one made up of a large
number of parts that interact in a non-simple way”.20
 That description fits our maps rather well. They are
certainly made up of a large number of distinct parts, even within geographically-concentrated areas.
If the economic system is complex, in the sense of Simon, this carries important implications for
understanding and modelling these systems. As with the body, to understand the dynamics of a complex
system it is crucial you study the micro-level moving parts. It is equally important to understand interactions
among these parts. What do we know about interactions among the economy’s moving parts?
(c) Movements between regions
In economic systems, interactions occur among all of the factors involved in production and consumption
decisions – people, money, goods and services. Capturing flows of these factors, and their interactions, is
crucial for understanding how a complex economic system will function. Below I consider flows of money,
goods and services. Let me start by considering flows of people.
Chart 15 plots migration in and out of various regions in 2016-17, expressed as a proportion of the regional
population. Even gross inflows and outflows are relatively modest, varying between 4% (London) and ½%
(Northern Ireland). Net flows of people are even more modest, in all cases bar London averaging less than
0.6%. One of the reasons regions are distinct may be because of their ring-fenced populations.

20
 Simon (1962).
All speeches are available online at www.bankofengland.co.uk/speeches
21
21
Chart 15: Migration inflows and outflows between different UK regions
Sources: ONS Local Area Migration Indicators and Bank calculations.
Notes: Data refer to mid-2016 to mid-2017, as a percentage of 2017 population.
Of course, people living in one region can still commute into another for work purposes. Table 1 looks at
commuting destinations across UK regions, as a percentage of each region’s population. With the exception
of London and the South-East, worker flows from outside of the home region are modest, averaging only 5%
of the working population. Even when looking at flows of workers, most regions are self-contained and local.
All speeches are available online at www.bankofengland.co.uk/speeches
22
22
Table 1: Commuting flows between UK regions (percentage of workers resident in each region)
Sources: Nomis and Bank calculations.
Notes: Percentage of each region’s workers commuting from each residential region.
These flow data provide a useful lens on local economic eco-systems. As an example, Chart 16 plots the
time to get to work (in minutes) against the distance to work (in km) across the UK’s regions. Most regions
outside London and the South-East are tightly clustered, with an average commute time of 27 minutes and
distance of 16 km. Consistent with other evidence, this suggests most regions are local economically.
All speeches are available online at www.bankofengland.co.uk/speeches
23
23
Chart 16: Distance vs. time to work
Sources: ONS Labour Force Survey, 2011 Census and Bank calculations.
Notes: Average distance travelled (km) and usual time (minutes) from home to work by NUTS 1 region in England and
Wales.
The situation is somewhat different in London. The average commute distance is materially shorter (11km)
but the average travel time almost doubles at (40 minutes). A cartogram of commute times makes this point
strikingly (Chart 17). Contrary to popular belief, London is life in the slow lane. As we know commuting often
detracts from people’s well-being, this cartogram perhaps explains why Londoners are richer but more
miserable.
21

21
 For example, Office for National Statistics (2014) and Novaco and Gonzalez (2009).
All speeches are available online at www.bankofengland.co.uk/speeches
24
24
Chart 17: Cartogram of UK travel time to work
Sources: ONS Labour Force Survey, 2011 Census and Bank calculations.
Notes: UK local authorities re-sized and coloured by mean travel to work time (minutes).
All speeches are available online at www.bankofengland.co.uk/speeches
25
25
Modelling the Economy
How can these micro-level data help when modelling the economy? Economists and policymakers typically
use highly-aggregated models which operate at an economy-wide level, often comprising a small number of
“representative” agents. In their most aggregated versions, there is a single representative household and
company. The most widely-used is the Representative Agent New Keynesian (RANK) model.22

RANK-type models have real virtues. They are underpinned by an optimising view of individual human
behaviour and, in this sense, can be said to be “micro-founded”. They are explicitly identified, in the
statistical sense, and parsimonious. These characteristics mean these models can often offer deep insight
into comparative static questions about the impact of changes in the deep parameters of an economy, such
as shifts in risk-aversion among households or in levels of technology among firms.
23

There are also, of course, costs of simplicity. The most serious is the inability of these models often to fit the
macro-economic facts at business cycle frequencies, particularly at times of economic stress.24
 RANK
models fared poorly in capturing the dynamics of the economy either side of the global financial crisis. At the
very time it was most interesting and important, these models were found wanting as a guide to the fortunes
of the economy and economic policy.
One response has been to develop models that relax the representative agent assumption. So-called
Heterogeneous Agent New Keynesian (HANK) models comprise distinct agents with distinct behaviours. As
importantly, these models capture interactions among agents. 25
 While it is heterogeneity that is often
emphasized, it is interactions between agents that is most important for replicating real-world dynamics.
Consider the dynamics of two very simple systems: a single and double pendulum. The single pendulum is
a representative agent model. Once disturbed, its motion is regular and its distribution normal
(Figure 1, LHS). The double pendulum involves two identical representative agents which interact. Its
dynamics are complex and highly non-linear and it is Power Law distributed (Figure 1, RHS). Even simple
interactions among representative agents generate complex system dynamics.

22
 For example, Galí (2015).
23
 For example, Kydland and Prescott (1982).
24
 Haldane (2016), Haldane and Turrell (2018).
25
 For example, Kaplan, Moll and Violante (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
26
26
Figure 1 : Single and double pendulum
Single pendulum Double pendulum
Sources: https://codepen.io/anon/pen/XQLNaZ
HANK-type models do a better job of capturing heterogeneity across agents than their RANK cousins. The
number of interactions among their internal moving parts is nonetheless heavily constrained. Certainly, they
do not come close to capturing the granularity, spatially or sectorally, that exists in real-world economic
systems. Are there alternative modelling approaches that capture more of these moving, interacting parts?
Agent-Based Models (ABMs) use a larger array of distinct agents and permit a far-larger scale of interaction
among them.
26
 They have been used extensively to model complex systems – natural systems (rainforests
and oceans), physical systems (electricity grids and war zones) and social systems (swarms of birds and
bees).
27
 These are systems where it is recognised that an understanding of interactions among the internal
moving parts is crucial for capturing behaviour of the system as a whole, in line with Simon’s dictum.
For economists used to dealing with highly-aggregated models, the very dimensionality of ABMs is striking.
The largest in physics can now simulate interactions of up to 400 billion distinct particles at any one time.
28

That is around 50 times as many agents as there are humans on the planet. By way of comparison, the
largest-scale macro-models would typically have fewer than 10 distinct agent types.
ABMs have been used to study the economy, albeit not very extensively.29
 The Bank has recently used
them to model the housing and corporate bond markets, both of which exhibit non-linear dynamics due to
complex interactions among agents.
30
 These models have also been used to run hypothetical policy
simulations, in particular for macro-prudential policies.31
 These models are no free lunch. They pose difficult
choices, such as where to strike the balance between realism (complexity) and parsimony (simplicity).

26
 Farmer (2019) discusses agent-based models in the context of complexity economics.
27
 For example, Ge et al (2018), Ge and Polhill (2016) and Heppenstall et al (eds.) (2012).
28 https://arxiv.org/abs/1112.1754
29
 For example, Cristelli, Pietronero and Zaccaria (2011) and Napoletano, Gaffard and Babutsidze (2012).
30
 Baptista et al (2016) and Braun-Munzinger, Liu and Turrell (2016).
31
 Baptista et al (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
27
27
We typically define models as simplified abstractions of reality, partly for reasons of practicality given
humans’ limited capacity to track too many moving parts. But computers these days can keep track far
better than any human. If the largest ABMs have a dimensionality greater than our population, why not
model the economy at its highest possible resolution? Why be abstract if you can be exact? That would be
a very different conception of the term “micro-founded”.
This may sound like science fiction. Yet we already have practical examples of just that, starting at the
highest level of resolution – the firm. In 1956, Jay Wright Forrester, an electrical engineer, switched from
devising systems for detecting incoming Soviet nuclear bombs to designing systems for understanding
production problems in companies. He developed the first computer models of how companies operated. In
so doing, he founded the discipline that would become “system dynamics”.32
In line with Forrester’s initial insight, one of the most interesting developments recently when understanding
the dynamics of companies is the creation of so-called “digital twins”.33
 These are a DNA-level digital clone
of a company, tracing the interactions among its moving parts. A digital twin is a high-resolution, dynamic
model of a firm-level micro-economy that can be used both to understand and to simulate a company’s
operations, however complex. Their use among frontier companies is increasingly commonplace.
The data necessary to calibrate these digital twins is considerable, but also increasingly commonplace.34

Twenty years ago, the large American retailer Walmart created a system allowing them, and their suppliers,
to monitor their complex supply-chain in close to real time.35
 This transformed inventory management at
Walmart and its suppliers. Twenty years on, these real-time data systems are rapidly becoming the norm.
At the next level of resolution, could a digital twin be created at a town or city level? Having started with the
firm, Jay Wright Forrester’s next project was to simulate a city, in an attempt to understand and help solve
the US’s “urban crisis” of the 1960s. Forrester developed DYNAMO, a computer-simulated model of a city
comprising 150 equations and 200 parameters. His book Urban Dynamics, published in 1969, set out how
these computer models could be used to understand the evolution of cities.36
Reading Forrester’s book in the 1980s, video-game developer Will Wright had an idea for creating a game
involving building a dynamic model of a city. The result was SimCity (Figure 2). This quickly became one of
the most popular video games of its era. Computer-simulated video games quickly came to be used to solve
real-world city planning problems. Subsequently, ABM-like models of simulated cities have found
widespread use for urban planning and development.37


32
 For example, Karnopp, Margolis and Rosenberg (1990).
33
 Shaw and Fruhlinger (2019).
34
 Bolton et al (2018) have discussed the possible guiding values for a national digital twin framework as part of work on behalf of the
Centre for Digital Built Britain.
35
 Hardgrave et al (2006).
36
 Forrester (1969).
37
 For example, Motieyan and Mesgari (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
28
28
Figure 2: SimCity
Sources: Electronic Arts, available at: https://help.ea.com/en-us/help/simcity/simcity-buildit/all-about-regions-in-simcitybuidlit/
If creating a digital twin is possible at the firm and city-region level, might it be feasible economy-wide? An
economy-wide map constructed at the microscopic level, and an accompanying digital twin tracking flows
among the moving parts, is an intriguing vision of how a future economic model might be constructed. Paul
Clarke, Chief Technical Officer at Ocado, has recently set out a vision of smart cities and communities, with
tracking and modelling of flows through drones, robots and smart infrastructure, which is in this same spirit.
38

Such a model is not without precedent. In 1949, the New Zealand economist A W Phillips built a machine in
his landlady’s garage in Croydon using liquid and tanks to illustrate the circular flows of income and
interactions in an economy (Figure 3). This early analogue computer was not for show; it was used to
understand the economy and simulate policy before digital computers existed. Working versions still exist.
The model I have sketched is a high resolution, digital version of the Phillips machine.

38
 Clarke (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
29
29
Figure 3: Phillips machine
Sources: Image available at https://en.wikipedia.org/wiki/William_Phillips_(economist)
Building a microscopic-level model of the economy is an enormous undertaking if it were to be attempted
top-down. But that traditional top-down approach to model-building may itself need to be inverted. Clarke’s
suggested approach is to build this model bottom-up. The map would be crowd-sourced and then stitched
together, a Wikipedia-style or micro-to-macro approach to model-building.
An economy-wide digital twin would, in a very literal sense, be micro-founded, but in a diametrically-opposing
way to existing models. The foundations of this digital twin are constructed not on micro theory, but micro
fact. If you were looking for clues on the feasibility and desirability of such an endeavour, consider the way
in which scientists have gone about understanding and simulating another complex, adaptive system which
affects everyone’s everyday lives – the weather.
Weather systems are the archetypical complex, adaptive system, the very origin of the concept of chaos.39

Yet despite their chaotic properties, our understanding of weather systems, globally and locally, has
improved dramatically over recent decades. Errors in forecasting the weather have halved in a generation, a
remarkable improvement. The reason for that improvement is not because of a great leap forward in
weather theory; it is because of improvements in micro-level weather facts.
Meteorologists piece together their models of weather systems from very high resolution readings. It is this
improvement in data resolution that has held the key to improvements in weather forecasting. These
readings come from a range of meteorological agencies, as well as from private companies and citizens.
These data can be pooled and stitched to give a global map. Weather data is crowd-sourced,
Wikipedia-style, and then centrally stitched, a micro-to-macro approach.

39
 Lorenz (1963).
All speeches are available online at www.bankofengland.co.uk/speeches
30
30
As these data are collected in close to real-time, weather simulations are available in close to real-time. This
also allows us to study weather systems at a wide variety of resolutions – globally, nationally, regionally,
down to the local village. This not only helps people better understand the environment facing them.
Importantly, it improves the decisions they make, nationally and locally, governments and citizens.
Weather is by no means a unique example. Micro-level data, digital twins and massive-scale simulations are
being used to better understand all manner of other systems at all manner of resolutions – from the
sub-atomic scale when seeking the Higgs-Boson, to the global scale when modelling flows across oceans,
the world wide web and galaxies. All have benefitted from drawing on mass micro-level data, crowdsourced, centrally-stitched, micro-to-macro.

Until recently, fewer such high-resolution data existed when it came to tracking flows of goods and services,
people and money through our economy. That is changing. These data increasingly exist in companies,
whose management systems capture activity in close to real time. They exist through now-ubiquitous
sensor, camera and satellite technology. And they exist in governments, local and central, in the form of
administrative data on people, goods and money flows.

Increasingly, these data are being put to use to understand the economy.40
 In the UK, the Office for National
Statistics recently started publishing a range of high frequency economic indicators, based on such sources
as administrative data on VAT returns and geospatial data on traffic flows. The Bank has made increasing
use of these sorts of data in its own real-time assessment of the economy. Let me give a couple of
examples.
One topic attracting attention recently has been the economic consequences of a disruptive Brexit. One of
the key channels through which this economic disruption might take place is the inability of lorries to move
seamlessly through UK ports. To track this disruption in the event of a disorderly Brexit, the Bank put in
place a system for monitoring traffic flow around the UK’s main ports using geospatial data from Google
Maps. This allowed close to real-time monitoring of potential port bottlenecks.
As an illustration, Chart 18 plots traffic times into and out of Dover during February, March and April this
year. The first highlighted mini-spike in traffic times is the Friday evening and Saturday morning before the
school holidays. The second is Easter weekend. A disruptive Brexit would potentially have pushed these
needles off the Richter scale. There is huge potential for more extensive use of geospatial data such as
these to track goods and service flows across the economy, in normal as well as abnormal times.
41


40
 Haldane (2018a).
41
 A geospatial commission has recently been set up, under Sir Andrew Dilnot, to look at precisely these questions. See Geospatial
Commission (2019) for a discussion of its work to date and longer-term priorities.
All speeches are available online at www.bankofengland.co.uk/speeches
31
31
Chart 18: Traffic times into and out of Dover
Sources: Google Maps API and Bank calculations.
A second example concerns the housing market. This, too, is a local market. That is why the most popular
house-buying show on TV is called “Location, Location, Location”. Two colleagues at the Bank, Fergus
Cumming and Al Firrell, have recently used a database on all mortgages granted in the UK since 2005 to
explore house-moving patterns.42
 The results are preliminary, but let me share some of the early findings to
demonstrate what might be feasible.
Chart 19 plots the frequency of house moves at different distances in 2018. The modal distance of a house
move is less than 10km. This is twice as likely as the second most popular distance – 10-20km. In other
words, a gravity (distance) model of house moves fits the data, with around one-third of all house moves
within 50km. Most house moves are local. Most, but by no means all. The median move is a little below
100km, the mean 130km. There is a longer tail of long-distance house moves than I would have predicted.

42
 Although this may sound simple, it involves a complex matching algorithm to identify housing chains, similar to Chakraborty,
Gimpelewicz and Uluc (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
32
32
Chart 19: Distribution of house moves by distance (mortgage-to-mortgage movers in 2018)
Sources: Product Sales Database and Bank calculations.
Part of the explanation lies in regional and demographic differences. People outside London and the
South-East – for example, in Scotland – are much more likely to move greater distances (Chart 20). Older
people (over 46) are more likely to move larger distances than younger people (under 33) (Chart 21). And
richer people are less likely to move larger distances than poorer ones (Chart 22).
All speeches are available online at www.bankofengland.co.uk/speeches
33
33
Chart 20: Distribution of house moves by distance in different regions (mortgage-to-mortgage
movers in 2018, moving to South-East England and Scotland)
Sources: Product Sales Database and Bank calculations.
Chart 21: Distribution of house moves by distance for different ages (mortgage-to-mortgage movers
in 2018, upper and lower age quartiles)
Sources: Product Sales Database and Bank calculations.
All speeches are available online at www.bankofengland.co.uk/speeches
34
34
Chart 23: Distribution of house moves by distance for different income groups
(mortgage-and-mortgage moves in 2018, upper and lower income quartiles)
Sources: Product Sales Database and Bank calculations.
Take Yorkshire and Humberside. A third of all moves are within-region. East, West, Home’s Best. Some do
move to the West (around 15%) and some to the East (7%). Although some head South-East for life in the
slow lane (7%), a larger number move North to Scotland (around 10%). This only covers moves among
those with a mortgage, so will not capture first-time buyers, students, renters or those retiring without a
mortgage. It nonetheless offers a new lens on the housing market, locally and nationally.
It is not difficult to see how these micro-level flow data could be augmented. Data from payments systems
can, and are, being used to track flows of money. Companies’ management information systems can, and
are, being used to track flows of workers, goods and services. And administrative data can, and are, being
used to track people, money and goods. These micro-level data that could, with time, be pieced together to
provide a macro – or micro-to-macro – model of the economy. Or that, at least, is the promise.
Managing the Economy
If we can map and model the economy at a high resolution how, if at all, might this affect how economic
policy is set? Can analysis of this granular type help when thinking about appropriate forms of policy
intervention? Might it help in defining what might be a sensible geographic area for an economic strategy?
And might it even help in defining what that economic strategy might be?
All speeches are available online at www.bankofengland.co.uk/speeches
35
35
There is already a deep and rich literature on these topics. One strand looks at optimal currency areas.43

Another looks at optimal fiscal areas.44 And a third looks at optimal political or policy areas.45
 This literature
identifies a rich range of factors important for determining the optimal spatial dimension of policy. From that
long list, let me highlight three factors which are commonly found to play a crucial role:
(a) Heterogeneity and Specialisation
Other things equal, the greater are the differences in the economic characteristics of an area, the stronger is
the case for recognising these differences in the setting of economic policy. These economic characteristics
include: the cultural and political preferences of citizens; economic fluctuations over the business cycle;
and structural features of the economy, such as industrial composition and policy transmission.46

A related aspect is the degree of specialisation or comparative advantage in an economic area. The greater
this degree of specialism or comparative advantage, the stronger the case for having it recognised and
supported in the setting of economic policy. Specialisation is one of the ways in which an economic area can
harvest the benefits of economies of scale and scope through the agglomeration multiplier.
47
On the face of it, there is a considerable degree of heterogeneity across the UK, including in terms of
income, wealth, health and happiness. Other things equal, this would support locally-targeted policy
interventions. At the same time, those differences are often as large locally as they are regionally. That
suggests a very decentralised setting of policy would be needed, which may come with costs.
(b) Diversity and Risk-Sharing
One of the most important of those costs is the loss of risk-sharing. A diverse economic area allows
aggregate fluctuations to be smoothed-out, in the same way a diverse portfolio of assets smooths out returns
to an investor. And by smoothing-out aggregate fluctuations, diverse economic areas can help smooth out
local fluctuations by transferring resources between regions to insure against region-specific shocks.
Another set of factors weighing in the same direction are economies of scale and scope. The smaller the
geographic area, the lower the potential for agglomeration effects. Some of these agglomeration benefits
may themselves arise from having a diverse economic eco-system, as this enables positive spillovers
between sectors and skills within an area.
48

43
 Mundell (1961).
44
 For example, Berger, Dell’Ariccia and Obstfeld (2018).
45
 Alesina, Tabellini and Trebbi (2017).
46
 For example, Alesina, Tabellini and Trebbi (2017), Carlino and DeFina (1996), Cumming (2018), Dow and Montagnoli (2007) and
Fratantoni and Schuh (2003).
47
 Krugman (1991) and Krugman and Venables (1996).
48
 Izraeli and Murphy (2003).
All speeches are available online at www.bankofengland.co.uk/speeches
36
36
(c) Factor Mobility
A third sector of factors influencing the choice of optimal policy area concerns the degree of factor mobility.
Easy movement of factors between regions can smooth-out differences between regions. For example,
people moving from low to high unemployment areas can smooth out regional differences in incomes. In this
way, factor mobility can serve as a risk-sharing device between areas, even without policy intervention.
The other side of this coin is that, people and money do not always flow in this direction. Factor mobility can
sometimes increase, rather than smooth out, differences between regions. For example, people or firms
may move to areas which offer strong existing incomes and jobs. If so, this will tend to amplify, not
ameliorate, regional differences.
That is the theory. The optimal policy strategy involves balancing these factors, based on the structural
characteristics of an economy, its degree of diversity versus specialism and the flows of factors within it. A
digital twin of the economy, of the type outlined earlier, would embody these factors and flows and would
provide a quantitative test-bed for making policy choices.
In the absence of such a model, let me discuss two examples where existing data and analytical tools could
be used to inform policy choices. In line with theory, the first looks at flows of factors within a region, the
second at structural economic characteristics across regions. These data and techniques can be used to
shed light on policies which might boost economic growth, locally or nationally, in line with the UK
Government’s industrial strategy.49
My first example draws inspiration from recent work by Tom Forth at the Open Data Institute in Leeds, on
behalf of the Productivity Insights Network (PIN) here at the University.50 This uses data on flows of people
to assess local growth prospects. The story starts with the observation that many UK cities appear to punch
below their demographic weight in income terms. Put differently, a number of UK cities do not appear to
benefit as much as their international counterparts from agglomeration effects.
To demonstrate, Chart 22 plots (log) city population size against GDP per head in 563 cities in advanced
economies around the world, including a number of UK cities. It also shows a line of best fit. This is
upward-sloping and statistically significant, consistent with agglomeration effects. Moving from a city the size
of Oxford (population 535,000) to one double its size such as Sheffield (around 1.2 million) should be

49
 Department for Business, Energy and Industrial Strategy (2017). In a personal capacity, I chair the Government’s Industrial Strategy
Council which evaluates its progress.
50
 Forth (2019).
All speeches are available online at www.bankofengland.co.uk/speeches
37
37
expected, on average, to boost GDP per head from £27,300 per head to £28,100 per head, or around
2.6%.51
Chart 22: City population and GDP per head
Sources: OECD Metropolitan Areas Population and Bank calculations.
Notes: GDP per capita in US$ in 2015 and log scale used for metropolitan area population.

In fact, GDP per head in Sheffield is around 44% lower than in Oxford. This is not just a North-South divide.
Income per head in Sheffield is little different than in Doncaster just down the road (population 305,000).

51
 Data set underlying Chart 22 refers to GDP per capita in US$ in 2015. Numbers which convert to sterling in the text are based
around average $/£ bilateral exchange rate of 1.34 in 2018.
All speeches are available online at www.bankofengland.co.uk/speeches
38
38
And it is not just Sheffield. Many large UK cities sit below the line: Belfast, Glasgow, Leeds, Manchester,
Cardiff, Birmingham, as well as Sheffield. Agglomeration multipliers seem to be consistently smaller in a
number of UK cities. Why?
Forth’s work suggests transport networks are a large part of the explanation. By analysing commuting times
into Birmingham at different times of the day, he provide a measure of the “effective” working population. At
peak times, the effective working population of Birmingham is perhaps 50% smaller than its measured
population due to poor transport infrastructure. This helps explain why Birmingham punches below its
demographic weight.
This is a terrific example of how high-resolution data can be used to understand local patterns of economic
growth. The same data could be used to analyse the “effective” working population in other UK cities,
perhaps helping explain their under-performance. It could also be used to answer industrial strategy
questions, such as what impact new transport routes have on the effective working population of the UK’s
major cities and hence on their incomes through agglomeration effects.
Imagine a transport strategy whose aim was to raise the effective working population of those UK cities
currently operating below the line. Using Chart 22, doing so would raise UK GDP per head by around 10%
or around $4,100 (£3,100) in today’s money. Those annual benefits could be compared with the one-off
costs of investment to determine the desirability of such a policy, as part of the UK’s industrial strategy.
My second example draws inspiration from recent work at the University of Cambridge by Penny Mealy,
including by my colleague on the Industrial Strategy Council Diane Coyle.52
 This looks at the structural
economic characteristics of different regions and provides a way of summarising and ranking them. The
particular way it does so is by measuring their “economic complexity”, following the work of Hidalgo and
Hausmann (2009) in the context of international trade.
Without going into the technicalities, the complexity index can loosely be thought of as weighing the two
structural characteristics of an economic area which theory would suggest are crucial for economic success.
The first is the degree of specialism or comparative advantage in a particular product or industry: the greater
this specialism, the greater an area’s complexity. The second is the degree of diversity in this specialism
across products and industries: the greater this diversity, the greater an area’s complexity.53

Though the construction of an Economic Complexity Index (ECI) is itself complex, the principles underlying it
are not. An economic area exhibits complexity when it has a diverse set of highly-specialised industries. An

52
 Mealy and Coyle (2019).
53
 This description sets out a stylised interpretation of ECIs as they were originally conceived by Hidalgo and Hausmann (2009).
Recent work by Mealy, Farmer and Teytelboyn (2019), however, has re-assessed the interpretation of ECIs and found that diversity is
mathematically orthogonal to ECIs. In this new interpretation, ECIs instead provide a useful way to understand place-based similarly in
industrial specialisations.
All speeches are available online at www.bankofengland.co.uk/speeches
39
39
area lacking complexity has few or no industries or products which are specialised. ECIs have been
constructed at the nation-state level and, more recently, at the local level in the US and UK.54
One of the interesting features of these empirical ECIs is that they appear to match the theory: the greater
the degree of specialisation and diversity, and the higher the ECI, the better the economic outcome. One
rationale for this finding, proposed by Hidalgo and Hausmann, is that ECIs serve as a proxy of the degree of
knowledge embedded in an economic area, knowledge that comes courtesy of specialisation and diversity.
Charts 23 and 24 plot local authority level ECIs against measures of income per head and hourly earnings in
the UK. They show a positive, statistically significant relationship. The same has been found at national
level and among US states. The embodied-knowledge effect is, on the face of it, an empirically strong one.
In a bivariate regression, ECIs account for around half of the regional variation in level of incomes and pay.
Chart 23: Local authority-level ECIs and income per head
Sources: ONS Regional Gross Value-Added, ONS Business Register and Employment Survey (BRES) and Bank
calculations.
Notes: ONS regional gross value-added (balanced) by local authority in the UK for 2016 against economic complexity
indices calculated using BRES. City of London and Westminster excluded from chart.

54
 Hidalgo and Hausmann (2009) for international comparisons and Mealy, Farmer and Teytelboym (2019) for regional comparisons.
R² = 0.4799
0
20
40
60
80
100
120
140
-0.1 -0.05 0 0.05 0.1 0.15 0.2 0.25
GVA per head,
£000s
ECI
All speeches are available online at www.bankofengland.co.uk/speeches
40
40
Chart 24: Local authority-level ECIs and hourly earnings
Sources: ONS Annual Survey of Hours and Earnings (ASHE), ONS Business Register and Employment Survey (BRES)
and Bank calculations.
Notes: Data available for Great Britain only (i.e. excluding Northern Ireland). Mean hourly earnings excluding overtime
against economic complexity indices calculated using BRES.
Chart 25 plots a cartogram of local authority level ECIs for the UK, at the NUTS 3 level. It shows a
remarkable degree of difference across areas of the UK. This matches regional differences in income and
pay. These metrics could help when designing the Local Industrial Strategies currently being developed by
Local Enterprise Partnerships (LEPs) and the devolved nations in Scotland, Wales and Northern Ireland.
Table 1: Top 10 local authorities by ECI
City of London
Tower Hamlets
Southwark
Westminster
Islington
Camden
Hammersmith & Fulham
Kensington & Chelsea
Hackney
Cambridge
Sources: ONS Business Register and Employment Survey (BRES) and Bank calculations.
R² = 0.4466
10
15
20
25
30
35
-0.1 -0.05 0 0.05 0.1 0.15 0.2 0.25
Hourly earnings,
£ per hour
ECI
All speeches are available online at www.bankofengland.co.uk/speeches
41
41
Chart 25: Cartogram of local-authority level ECIs
Sources: ONS Business Register and Employment Survey (BRES) and Bank calculations.
Notes: UK NUTS 3 regions re-sized and coloured using estimates of economic complexity.
All speeches are available online at www.bankofengland.co.uk/speeches
42
42
Table 1 lists the top ten local authorities by ECI across the UK in 2017, while Table 2 shows the bottom ten.
There is a high degree of inertia in both rankings. The ECI framework could be used to help assess which
industrial strategy interventions might boost the degree of specialism and diversity, and hence the income
benefits, both in already-successful and struggling regions.
Table 2: Bottom 10 local authorities by ECI
Dudley
South Derbyshire
Wrexham
Aberdeenshire
North-East Lincolnshire
Moray
Hartlepool
Carmarthenshire
Breckland
Telford and Wrekin
Sources: ONS Business Register and Employment Survey (BRES) and Bank calculations.
Economic complexity can also be assessed across industries using a product complexity index (PCI). This
proxies the embedded knowledge in an industry. Table 3 plots the top-20 UK industries by PCI against their
sector shares. Financial services, the creative industries, and higher education exhibit the highest economic
complexity and thus potentially generate the highest value-added. PCI metrics could be used to identify
industries where “sector deals” could benefit most the economy, as part of an industrial strategy.
Table 3: Top 20 UK industries by PCI and share of employment
Reinsurance 0.01%
Fund management activities 0.16%
Television programming and broadcasting activities 0.08%
Trusts, funds and similar financial entities 0.03%
Motion picture, video & television programme activities 0.34%
Sound recording and music publishing activities 0.03%
Legal activities 1.09%
Translation and interpretation activities 0.02%
Software publishing 0.04%
Creative, arts and entertainment activities 0.32%
Activities auxiliary to financial services, except insurance
and pension funding
0.66%
All speeches are available online at www.bankofengland.co.uk/speeches
43
43
Publishing of books, periodicals and other publishing
activities
0.35%
Accounting, bookkeeping and auditing activities; tax
consultancy
1.29%
Advertising 0.42%
Computer programming, consultancy and related activities 2.43%
Leasing of intellectual property and similar products,
except copyrighted works
0.01%
Market research and public opinion polling 0.14%
Organisation of conventions and trade shows 0.08%
Higher education 1.61%
Sources: ONS Business Register and Employment Survey (BRES).
Notes: Dates for 2017. Only industries with 1000+ employees included in table.
These are partial, though informative, pieces of micro-level evidence, though no substitute for an
economy-wide digital twin. This could be used as a general equilibrium test-bed for formulating, and
evaluating the impact, of industrial strategy interventions, nationally and locally. Those policy simulations
could be complemented by controlled trials of policies, the like of which have been successfully pioneered by
the What Works Centre for Local Economic Growth.
55

The economy is a complex, adaptive system. Behaviour in those systems is difficult to predict ex-ante,
especially at times of policy change; it is emergent, just as a hurricane or tornado is emergent. Often, our
policy intuition about complex systems is simply wrong. No model, however micro-founded or data-rich, is
proof against those uncertainties. But one that embodies complex, micro-level dynamics is more likely to do
so than one without them. A complex systems framework can make for robust policy choice.
That was Jay Wright Forrester’s insight in the 1950s. We know it to be true from more recent experience.
The real-time, supply-chain data system developed by Walmart twenty years ago transformed inventory
management, at Walmart and elsewhere. The real-time micro-level models developed by meteorologists
transformed weather-forecasting at around the same time. Today, digital twinning is transforming processes
across companies. These are microcosms of what might be achievable at the macroscopic scale.

55
 For example, What Works Centre for Local Economic Growth (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
44
44
Can a Central Bank be Local?
Finally, what are the implications of these local economic perspectives for central bank policy? On the face
of it, these might seem few. Central banks are national agencies. Their policies, such as interest rates, can
only be set at the national level. It is impossible for the Bank of England to set separate interest rates for
Aberdeen and Aberystwyth, Belfast and Birmingham. A central bank, by definition, cannot be local.
Or can it? I would argue that not only is this feasible but essential. I have spoken previously about the “twin
deficit” problem facing central banks: the deficit in public understanding and the deficit in public trust.
56

Boosting this understanding, and restoring trust, among the public is among the most pressing issues facing
central banks. Reconnecting communities with institutions helps strengthen Rajan’s Third Pillar.
But how is this best done? The public’s understanding of the economy and policy is enhanced when
messages are relevant to their lives and locality.57
 A central bank’s understanding of the economy is
enhanced if it can tap into the lived, local experiences of companies and citizens making up the economy.
And a better mutual understanding between communities and central banks would, in turn, help build trust
between them. Constructing a Third Pillar shrinks the twin deficits of public trust and understanding.
This is not a new point. Despite being headquartered in London since 1694, the Bank of England has had a
regional presence for much of its history. After the UK banking crisis of 1825, the Bank set up a network of
15 regional branches to provide commercial banking facilities.
58
 Though their purpose was operational, it
quickly became clear that the intelligence gathered by the Bank’s branches could help in understanding the
economy. This intelligence was soon being fed back to Head Office.
During the 20th century, this intelligence-gathering role was expanded. It has grown steadily, in scale and
importance, since then. Today, the Bank’s 12 Agencies across the UK have around 9,000 regular company
contacts providing real-time intelligence on the economy. Structured surveys from the Agents first appeared
in 1997. Agents’ National Scores were first introduced in the mid-1990s and first published in 2006. Agent’s
Company Visit Scores were first developed in 2007 and first published in 2015. 59
Brexit provides as good an illustration as any of the ways the Agents’ high-frequency, high-resolution
intelligence can improve the Bank’s understanding of the economy. A key question recently has been
whether and how companies have been preparing for Brexit.60
 The Agents’ network was used not only to
track how prepared companies were over time, but what form this contingency planning was taking
– stock-building, seeking alternative suppliers, building cash reserves etc (Chart 26).

56
 Haldane (2017).
57
 For example, Agerström et al (2016).
58
 Bank of England (1963).
59
 Ellis and Pike (2005).
60
 Bank of England (2019).
All speeches are available online at www.bankofengland.co.uk/speeches
45
45
Chart 26: Contingency planning by firms for Brexit
Sources: Bank of England and Bank calculations.
Company contacts also helped the Bank’s Brexit scenario planning. Companies were asked about the
consequences for their businesses of a “no deal, no transition” Brexit. The results are shown in Chart 27.
They suggest a marked slowing in output, employment and investment under this scenario, consistent with
the Bank’s in-house macro-economic models. This is micro-to-macro in practice.
Chart 27: Firms’ expectations for economy under a no-deal, no-transition Brexit
Sources: Bank of England and Bank calculations.
Recently, the Bank has used its regional network to augment its local engagement. Two years ago, I began
a programme of “Townhall” visits to some of the UK’s most poorly-performing regions, often organised in
conjunction with local charities, faith or community groups including Age UK, Citizens’ Advice, the Board of
All speeches are available online at www.bankofengland.co.uk/speeches
46
46
Deputies of British Jews, the Muslim Council of Britain, Oxfam and the YMCA. These Townhalls engage the
public directly on issues around the economy and finance. They, too, are a micro-to-macro approach.
So far, I have undertaken 14 Townhall visits covering all parts of the UK.61
 They are a direct conversation
between the Bank and the public, typically on local economic and financial issues. They draw on the sorts of
listening and facilitation techniques more familiar from anthropology and sociology. Intelligence-gathering
means extracting economic narratives, rather than facts, about the forces shaping people’s lives and
decisions. The anthropologist Clifford Geertz calls this approach “deep hanging out”.62
Deep hanging out is harder than it sounds. A day’s talking leaves me tired. A day’s listening leaves me
exhausted. Entering a room as the least knowledgeable person, at least on local issues, can be a culture
shock. But it is those very things that make intelligence from the Townhalls distinctive and valuable.
Statistical surveys cannot capture narrative in a way conversations can. The Townhall visits have been one
of the most enlightening things I have done in my almost 30 years at the Bank of England.
The Bank has recently expanded and deepened its range of local initiatives. One of these is Citizens’
Panels.63
 After a couple of trials last year, the Bank recently began rolling-out Citizens’ Panels across each
of its regional agencies. These panels have an independent external chair, drawn from the local region, to
facilitate discussion. Each is attended by a senior Bank policymaker. Messages from the panels will be
drawn together periodically and published externally.
Complementing Citizens’ Panels, the Bank’s direct engagement with community groups is now being
expanded through a set of “Community Forums”, working in partnership with local charities and community
groups. The Governor recently launched these forums with events in Tower Hamlets and Glasgow. Other
Governors and Directors have their own Community Forums planned through this year and beyond.
The Bank’s education programme also has strong local presence. The educational materials developed by
the Bank for 11-16 year olds, econoME, have been taken up by a third of schools across the UK. Last year,
staff ambassadors visited close to 300 schools across the UK to deliver talks on the economy and economic
policy. These are examples of a central bank acting locally.
Central banks around the world seem themselves to be moving in this same direction, with greater local,
citizen-level engagement. Other central banks to have recently launched community-based initiatives
include the US Federal Reserve’s “Fed Listens” events. These involve roundtables with the general public,
involving local leaders, policy experts and academics from across the United States.
64

61
 https://www.bankofengland.co.uk/outreach
62
 Geertz (1998).
63
 Patel, Gibbon and Greenham (2018) and Haldane (2018b).
64
 https://www.federalreserve.gov/monetarypolicy/review-of-monetary-policy-strategy-tools-and-communications-fed-listens-events.htm
All speeches are available online at www.bankofengland.co.uk/speeches
47
47
These initiatives are a good foundation for central banks, for constructing a Third Pillar. More of these types
of local, citizen-based engagements may be needed in the years ahead, if policy institutions are to shrink the
twin deficits and strengthen that Third Pillar. There is scope to decentralise more of central banks’ activities,
giving them greater reach, voice and engagement locally. That would certainly get my personal vote.
Conclusion
Our economies, like our politics, are local. Like the seashore, the more you magnify an economy, the
greater its richness, complexity, self-similarity. Like our bodies, understanding our economic health means
taking readings at many resolutions. It means understanding the moving body parts, and their interactions,
in microscopic detail. It calls for new data, at a higher frequency and higher resolution, and new ways of
stitching it together. It means making micro-to-macro a reality.
Our global weather systems, oceans, information networks, supply chains, solar system, galaxies and even
our universe can these days be mapped and modelled in microscopic detail in close to real time. We cannot
yet do so for our economies. I believe our economic policies would be better able to serve the public, and
better understood by them, if we could do so. 

The topic for this panel is the link between developments in product markets and monetary policy. It is a
great one. A lot of attention has been paid by central bankers over recent years to the relationship between
labour markets and monetary policy (for example, Yellen (2014) and Constâncio (2017)). And rightly so.
The relationship between monetary policy and product markets has, by comparison, been the road less
travelled.
1,2
Labour markets have been subject to big structural shifts over recent years, including the secular fall in the
degree of worker unionisation in a number of industries (for example, Schnabel (2013)), the emergence of
the so-called “gig economy” (for example, Taylor (2017) and Katz and Krueger (2017)) and secular rise in the
degree of globalisation and automation in the workplace (for example, Brynjolfsson and McAfee (2014) and
Acemoglu and Restrepo (2018)). Each of these shifts has led to a change in employment patterns and
tenures and in workers’ bargaining power.
These structural shifts have been used to help explain the secular fall in labour’s share of national income
and the recent weakness of wage growth across a number of advanced economies (for example, Dao et al
(2017) and Abdih and Danninger (2017)).
3
 They have also been used to justify potential shifts in the position
and/or the slope of the Phillips curve (for example, Blanchard (2016) and Kuttner and Robinson (2010)).
Each of these potentially has a bearing on the setting of monetary policy.
Yet, over the same period, structural shifts in the product market have been no less profound. They include
the emergence of highly-integrated global supply chains, increasing the degree of specialisation of product
markets (Baldwin (2016)); the blossoming of companies benefitting from global network economies of scale
and scope, who acquire “superstar” status (Autor et al (2017)); and the rapid emergence of e-commerce and
price-comparison technology (Cavallo (2017)).
The associated shifts in market power, too, might plausibly have altered some of the key macro-economic
relationships in the economy (De Loecker and Eeckhout (2017)). They may have influenced the pricing and
provision of goods and services in the economy and hence the Phillips curve. And they may have influenced
the amount of investment and innovation undertaken by firms and hence the aggregate demand curve
(Aghion et al (2005)). They, too, might thus have a bearing on the setting of monetary policy.
These structural shifts in product and labour markets may, in some cases, have had common cause. For
example, network economies of scale and scope could potentially have increased some companies’ market
power both over their labour inputs (through monopsony effects) and product outputs (through monopoly
effects). This could show up in both a falling labour and a rising profit share, with potential macro-economic
implications for activity, costs and prices (Autor et al (2017), Barkai (2017)).

1
 As Blanchard (2008) said, “How mark-ups move, in response to what, and why, is however nearly terra incognita for macro.”
2
 Some notable papers that discuss the impact of product market developments on the macro-economy include Cacciatore and Fiori
(2016) and Eggertsson, Ferrero and Raffo (2014).
3
 Unlike many other advanced economies, it is worth noting that the UK labour share has not been on a downward trend.
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
To explore these issues, we start by discussing briefly recent empirical evidence on market power and its
potential macro-economic explanations and implications. We then explore its effects on monetary policy,
using counter-factual policy simulations and adaptations of a simple New Keynesian model. Taken together,
this evidence suggests that an increase in market power and mark-ups could have potentially important
consequences for the economy and policy. These are summarised, in stylised terms, in Figure 1.
To the extent a secular rise in mark-ups reflects a set of trade-off inducing shocks, that would shift outwards
the output/inflation variability (policy possibility) frontier (from A to B). It may steepen the Phillips curve,
causing the policy possibility frontier to rotate clockwise (B to C). And it may also potentially alter the optimal
weights placed on output and inflation stabilisation by the policymaker, shifting the point of tangency
between the policy possibility frontier and policymakers’ loss function (C to D).
Taken together, the net effect of increased market power could be a potentially significant rise in inflation (but
less so output) variability, relative to the counterfactual case of stable and static mark-ups (A to D). As for
monetary policy, the fact that these are trade-off inducing shock places limits on its stabilisation capacity.
The (level and variability) of the optimal interest rate path is, as a result, less affected by increased market
power, despite significant shifts in policy possibility frontiers and policymaker preferences.
There are two ways in which the path of monetary policy might potentially be affected to a greater degree by
increases in market power. When companies have a significant degree of market power, the level of output
produced is likely to be below the social optimum, creating an incentive for monetary policy to try to offset
that by running the economy hotter (“inflation bias”). And if market power lowered companies’ investment
rates, this could reduce the economy’s neutral rate of interest. Neither, however, at present has a strong
empirical basis. To the extent these channels do operate, they reinforce the institutional case for
independent central banks charged with pursuing well-defined inflation targets.
At the same time, actual inflation across advanced economies has of course been relatively low and stable
over recent decades. So while the micro-economic evidence – a secular increase in mark-ups – is striking, it
is not easily reconciled with the macro-economic evidence on measured inflation, including on the impact of
mark-ups on inflation (for example, Smets and Wouters (2007)). Reconciliation of the two strands of
evidence – micro and macro – means that some combination of the following would have to be true.
First, the micro-economic firm-level evidence may not accurately describe how economy-wide mark-ups
have evolved since the 1980s. Second, other macro-economic factors may have more than offset the impact
of rising mark-ups on the behaviour of inflation. Third, the theoretical macro-economic framework we use
here – a New Keynesian model with monopolistic competition – may not be appropriate to analyse firm-level
changes in mark-ups. The apparent puzzle between the micro-economic and macro-economic evidence
deserves further research, given its potential impact on inflation dynamics and monetary policy.
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
Market Power – Evidence and Implications
There is a rich micro-economic literature that assesses the impact of market power on pricing and other firm
decisions (for example, Tirole (1988)). There has been rather less evidence linking the industrial
organisation of firms to developments in the wider macro-economy. That has changed recently, with a
number of papers exploring the empirical evolution of (firm, sectoral and national) measures of market power
and their implications for the macro-economy (for example, De Loecker and Eeckhout (2017, 2018) and Díez
et al (2018)).
Perhaps the simplest way of capturing market power is through measures of market concentration, such as
Herfindahl-Hirschman Indices (HHI) (Hirschman (1964)) or concentration ratios (the share of sales that
accrues to the largest firms within an economy or sector). Evidence suggests that market concentration,
measured either through HHIs or concentration ratios, may have increased in the US over recent decades,
across a broad range of sectors (for example, Autor et al (2017)). This pattern is not uniform, however, with
concentration among European companies showing no such trend (Gutiérrez and Philippon (2018)).
The evidence on industry concentration in the UK suggests it occupies a mid-Atlantic position. Chart 1 plots
the turnover share of the largest 100 UK businesses since 1998 (i.e. concentration ratio).
4
 This ticks up in
the lead-up to the financial crisis, although this pick-up is more modest than in the US, from 20% to around
28%.5 Concentration has flattened-off in the period since the crisis, however, in line with other European
countries.
Turning to measures of concentration within the financial services industry, the international pattern is
somewhat more uniform. Chart 2 plots the largest five banks’ share of total banking assets in the US, euro
area and the UK. Levels of banking concentration started fairly high, averaging around 30%. They drifted
further upwards in the run-up to the crisis, although this drift was again fairly modest. Since the crisis,
however, measures of banking concentration have flat-lined and, in the UK, have fallen slightly.
Concentration indices have their limitations, though, and need not always be associated with market power.
Some firms may be able to exercise market power in setting prices even without having a large share of a
market if, for example, there is brand loyalty. And in a world of differentiated products, concentration
measures such as HHIs or concentration ratios no longer correlate closely with market power (Bresnahan
(1989)). With non-homogenous goods and non-Cournot competition, a better measure of market power is
often provided by firms’ mark-ups – the ratio of their price to their marginal cost (De Loecker and Eeckhout
(2018)). The larger the mark-up, the greater the degree of market power, whether at the firm, sector or
national level. Mark-ups also have the benefit of being the relevant measure of market power in the
workhorse models of the macro-economy used by policymakers.

4
 Excluding financial services firms.
5
 See, also, Bell and Tomlinson (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
In that spirit, a number of recent papers have estimated measures of mark-ups based on individual company
accounts data. These cover a wide range of companies, sectors, countries and time periods (for example,
Díez et al (2018)). The findings from these studies are, in macro-economic terms, both quite striking and
quite strikingly uniform in the broad trends they reveal.
For example, De Loecker and Eeckhout (2018) have recently calculated mark-ups for around 70,000 firms
across 134 countries over almost four decades.6
 Since 1980, they estimate that the sales-weighted mark-up
for the average firm across countries has risen by a remarkable 50 percentage points.
7
 Table 1 shows their
mark-up measures for the G7 economies over the period. Though there is cross-country variation, average
mark-ups have risen significantly in every G7 country, by between 30 and 150 percentage points.
Taken at face value, the macro-economic implications of these shifts in mark-ups could be very large. The
most direct and immediate impact would be on measured inflation rates. According to Table 1, mark-ups will
have been adding, on average, over one percentage point each year to measured inflation rates across the
G7 countries between 1980 and 2016, other things equal. As context, over the same period average G7
inflation rates have fallen by over 10 percentage points.
8
A second potential macro-economic impact of higher mark-ups is on sales. Higher mark-ups will, other
things equal, have pushed down on aggregate demand and generated a deadweight loss of consumer
surplus (“Harberger triangle”). Baquee and Farhi (2018) estimate the size of this effect and find that
eliminating mark-ups entirely would raise aggregate US total factor productivity (TFP) by as much as 35%.9

To better understand some of the drivers of higher mark-ups, it is useful to look at more granular data. Using
a similar approach to De Loecker and Eeckhout (2017, 2018), we draw on data for around 3,500 unique
UK-listed companies from the late-1980s to construct around 33,500 firm-year mark-up estimates.10
 Using
that methodology, Chart 3 plots a sales-weighted measure of mean mark-ups for UK-listed companies since
1987. It shows a striking rise, from 1.2 to around 1.6, over the period. This broadly mirrors international
trends.
Although they capture subtly different dimensions of market power, there is a weakly positive relationship
between measures of mark-up and market concentration at the sector level (Chart 4), which is statistically

6
 De Loecker and Eeckhout (2018) use a dataset that largely includes publicly-traded companies, but there are also some privately held
firms.
7
 Similar estimates have recently been provided by Díez et al (2018).
8
 OECD data.
9
 De Loecker and Eeckhout (2018) estimate that mark-ups in the US are a little larger than in the UK, so the boost to UK TFP from
eliminating them would be smaller than the 35% estimate for the US in Baquee and Farhi (2018), albeit of a similar order of magnitude.
10
 The methodology is explained in the Appendix. The data include around a little over 1000 firms, on average, per year. These firms
account for around one-third of UK employment. Their sales are equivalent to around one-third of UK turnover and around two-thirds of
UK nominal GDP. We exclude financial sector firms and, having estimated mark-ups, trim outliers, i.e. those firm-level mark-ups that
are below the 1st percentile and above the 99th percentile of the firm-level mark-up distribution in a given year,
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
significant at the firm level.
11
 The same has been found among companies in other countries (Díez et al
(2018)). This gives some degree of reassurance that the rise in measured market power has been a
genuine one.
If we slice the mark-up data for non-financial companies on a sectoral basis, this suggests this rise has been
reasonably broad-based (Chart 5). All but two of the ten sectors have seen mark-ups rise since 1987,
although some are volatile. Six of the ten have seen them rise by more than 30 percentage points. Among
the largest rises have been in manufacturing (70 percentage points), professional, scientific and technical
(62 percentage points) and transport and storage (57 percentage points). This broadly mirrors the
international evidence.
12
One apparent exception is the banking sector. Chart 6 plots a measure of banks’ net interest margins
(NIMs), as a proxy for mark-ups, in the UK, US and euro area since 1996. NIMs appear to have been
broadly flat in these countries over recent decades. If anything, they may have fallen over the past decade.
The latter is potentially the result of the low levels of official interest rates, constraining the ability of banks to
lower their deposit rates in order to protect margins (for example, Claessens, Coleman and Donnelly (2017)).
Another way of slicing the data is to ask how much of the rise in mark-ups is due to a compositional shift over
time towards sectors whose mark-ups are already high and how much reflects a generalised rise in mark-ups
within each sector. Chart 7 shows this decomposition for UK-listed firms. Compositional effects do not
explain any of the rise in mark-ups in the UK; and even if we do the same exercise at the firm level,
compositional shifts towards firms with high mark-ups cannot explain the rise. Rather, the rise in mark-ups
appears to be reasonably generalised across sectors.13

Although relatively broadly-based across sectors, the rise in mark-ups need not necessarily be broadly
based within sectors. One way of showing that is by looking at the evolution of the distribution of mark-ups
over time (Chart 8). This suggests the increase in mark-ups is heavily concentrated in the upper tail of the
distribution – companies whose mark-ups are in, say, the top quartile. Mark-ups among firms in this upper
quartile of the distribution have, on average, increased by a remarkable 50 percentage points since 1987.
By contrast, mark-ups among firms in the bottom three quartiles of the mark-up distribution have scarcely
risen over the period. This distributional effect can also be seen from the large and widening gap between
mean and median mark-ups (Chart 9). In 1987, this gap was 7 percentage points. By 2016, it had reached
44 percentage points. This strongly suggests that the rise in aggregate mark-ups over the past 30 years can
largely be accounted for by a subset of high mark-up firms raising their mark-ups and/or market share.

11
 While the positive unconditional correlation between average firm-level mark-ups and concentration at the sector level shown in Chart
4 is not statistically significant, a regression of individual firm-level mark-ups on market concentration in their sector (at the two-digit SIC
level) shows a statistically significant positive relationship when we include firm and time fixed effects.
12
 For example, Díez et al (2018) find that the majority of industries in the US have seen mark-ups rise since 1980.
13
 Díez et al (2018) find that the increase in US mark-ups since 1980 is also relatively broad-based across sectors.
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
This fattening of the upper tail of the mark-up distribution is not uniform across sectors. Chart 10 plots a
measure of the skew of the mark-up distribution across different sectors over time. The fattening of the
upper tail of the distribution is most pronounced in the ICT, transport and storage and manufacturing sectors,
each of which is associated with higher average levels of mark-up.
In understanding the characteristics of these firms, one revealing cut comes from taking into the account the
extent to which UK-based firms’ sales are domestic or foreign-focussed (Chart 11). While both categories
have seen their mark-ups rise somewhat, this has been far larger among firms selling predominantly into
foreign markets (almost 60 percentage points) than domestic markets (around 15 percentage points).14

Within that, this rise in mark-ups among foreign relative to domestic sales-focussed firms is largest in the
manufacturing and ICT sectors.
Taken together, this evidence is consistent with a story of rising mark-ups being concentrated among
internationally-operating firms, who perhaps benefit disproportionately from global network economies of
scale and scope. These firms tend to occupy the fat and fattening upper tail of the mark-up distribution.
These are firms that might legitimately be termed “superstars” (Autor et al (2017)).
Given this diagnosis, what impact might the rise in mark-ups have had on the macro-economy? One aspect
is what impact increased market power may have had on firms’ incentives to invest and innovate and hence
on firms’ productivity. With investment and productivity each having under-performed over recent years, the
relationship with market power has been subject to increased academic scrutiny recently (for example,
Eggertsson, Robbins and Wold (2018)).
The relationship between mark-ups and productivity is vital in understanding their macro-economic effects
(Van Reenen (2018)). On the one hand, if highly productive ‘superstar’ firms, benefitting from network
economies of scale and scope, have become more dominant, then higher mark-ups could be the side-effect
of a positive supply shock in the economy. On the other hand, if mark-ups are the counterpart to increased
market power and reduced competitive pressures, that would suggest a negative supply shock.
These effects are not mutually exclusive. For example, Aghion et al (2005) develop a model which
generates a concave relationship between competition and investment. Within some range, increased
market power raises rents and acts as a spur to investment, innovation and productivity. But beyond a point,
those forces go into reverse. Market power is associated with a fall in innovation and investment incentives,
with knock-on negative effects for productivity.
There is some empirical support for such a relationship. Jones and Philippon (2016) and Gutiérrez and
Philippon (2017) suggest increased market power may have reduced investment among US companies.

14
 Our results are consistent with De Loecker and Warzynski (2012) who find that exporters charge, on average, higher mark-ups and
that firm mark-ups increase upon export entry.
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
De Loecker and Eeckhout (2017) document a negative relationship between mark-ups and the capital share
among global companies. And Díez et al (2018) identify empirically a concave relationship between
mark-ups and investment, in line with Aghion et al (2005).
We can re-run the Díez et al (2018) investment equations using the panel of UK-listed firms. This also finds
a concave relationship with mark-ups (Table 2, column 1). The same relationship holds between mark-ups
and R&D expenditure (Table 2, column 2). Chart 12 plots the estimated investment curve. It suggests that
firms with mark-ups above around two tend to be associated with lower investment rates, in line with Díez et
al. With estimated firm-level mark-ups having risen secularly in a number of countries, this is potentially a
cause for concern.
It is important, however, not to overstate the likely impact of this rise in mark-ups on aggregate investment,
innovation and productivity. The rise in mean mark-ups in the UK over the past 30 years would still leave
them below the levels at which investment rates start falling. The same is true among global firms. Indeed,
among our panel of UK-listed companies, the shift in average UK mark-ups since the late-1980s would,
using the estimated investment equation, be expected to have raised average investment rates by around
1 percentage point.
Finally, there is the question of whether any potential negative effects of increased market power on
investment and R&D translate into a negative effect on productivity. Evidence suggests there could be an
effect. De Loecker and Eeckhout (2017) argue that, once account is taken of the rise in mark-ups, it is
possible to account for the slowdown in US productivity growth after 1980. And Díez et al (2018) find that
the greater the distance to the technological frontier, the lower a firm’s investment – a “reverse catch-up”
effect.
If we look at the relationship between productivity and mark-ups across UK-listed firms, there is evidence of
a positive relationship with TFP but no significant relationship with labour productivity (Table 2, columns 3
and 4). If anything the relationship with TFP may be convex, with higher mark-up firms being associated with
proportionately higher levels of total factor productivity. There is some evidence of “reverse catch-up”
effects, but only at high levels of mark-ups.
Overall, then, while the theoretical and empirical evidence suggests it is possible higher market power and
mark-ups may have come at some cost in lower investment and innovation, the evidence is not
overwhelming and certainly would not imply that the aggregate effect is large.
A second relationship explored recently is between market power and the labour share.15
 Autor et al (2017)
find a negative empirical relationship using measures of market concentration among US companies. And
Díez et al (2018) and De Loecker and Eeckhout (2018) identify a weakly negative relationship between

15
 The relationship between labour, capital and profit shares is discussed in Barkai (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
mark-ups and the labour share. If we run regressions similar to those in Díez et al for UK-listed companies
(Table 3), we also find a negative relationship between mark-ups and the labour share.16

The analysis presented here, and much of the recent literature, is based around estimates that suggest a
secular rise in firm-level mark-ups. Some caution is advisable when drawing conclusions from these results.
First, some macro-economic evidence points to falling, rather than rising, company mark-ups/margins (for
example, Chen, Imbs and Scott (2009, 2004)). Second, some mismeasurement may be at play in the
estimation of mark-ups (for example, Traina (2018)). These uncertainties in the measurement of mark-ups
should be borne in mind in interpreting what follows.
Market Power and the Macro-Economy – A Simulation Approach
Having assessed some evidence on the evolution, and macro-economic implications, of increased mark-ups
and market power, the next question is what implications these may have for the setting of monetary policy.
This does not appear to have been an extensively examined area of research, whether among academics or
policymakers.17
 What follows is an initial exploration of some of the potential channels.
One simple way of beginning to gauge how a rise in mark-ups might affect the economy and monetary policy
is to simulate their impact using a macro-economic model. For this purpose, we model the economy using
the Bank of England’s in-house DSGE model, COMPASS.18
 Monetary policy is assumed to follow a simple
Taylor rule, with interest rate smoothing.19
 The simulations are shown for a variety of different values of the
relative weight policymakers place on output and inflation deviations from target in the Taylor rule.
Chart 13 considers the impact on inflation, the output gap and monetary policy of a temporary mark-up shock
that delivers a one percentage point increase in annual inflation. The dynamics of the economy are largely
as we would expect following an adverse supply shock. The inflation rate rises, and real GDP usually
contracts, in both cases temporarily. Although temporary, these disturbances are often material and always
persistent, despite monetary policy acting to damp these fluctuations.
The reason monetary policy struggles to damp these fluctuations is because a mark-up shock is trade-off
inducing. Monetary policy is caught between loosening to return output to potential and tightening to return
inflation to target. Which wins out depends, crucially, on the relative weight placed on these twin objectives
in the policy rule. When inflation deviations are given greatest weight, monetary policy tightens materially.
When output deviations are given greatest weight, monetary policy scarcely tightens at all.

16
 Unlike Díez et al (2018), we use the reported data on staff costs in our firm-level dataset when calculating the labour share.
17
 A recent exception would be the work of Mongey (2018).
18
 Burgess et al (2013).
19
 Taylor (1993).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
With monetary policy facing this trade-off, it follows that an increased prevalence of mark-up shocks would
leave policymakers somewhat constrained in their ability to smooth the economy. Put differently, a
sequence of trade-off inducing mark-up shocks would tend to worsen the trade-off between output and
inflation variability, for a given monetary policy rule. The “Taylor curve” frontier of policy possibilities would
be expected to shift outwards.20

To illustrate that, we can conduct a counterfactual simulation of the effects of mark-up shocks on the course
of output, inflation and interest rate variability. Chart 14 shows the variability of inflation and the output gap
(the black dot) generated by the model. It also shows the variability of output and inflation when the
economy is re-simulated having “switched-off” the shocks to firm mark-ups identified by the model (the red
symbols). Policy is again assumed to follow a Taylor rule, with varying weights on output and inflation.
Mark-up shocks have a material impact on output and especially inflation variability, even with monetary
policy cushioning their effects. The variance of inflation is reduced by around a quarter, and variance of the
output gap by around 10%, when mark-ups shocks are switched-off. The policy possibility frontier of
output/inflation variabilities is shifted outwards materially by the presence of mark-up shocks.21

The scope for monetary policy to cushion these shocks is relatively limited. Chart 15 plots the variability of
interest rates alongside output variability, for the same set of policy rules. Mark-ups shocks affect interest
rate variability relatively modestly.
22
 And the variance of interest rates is reduced by only around 5% when
mark-up shocks are switched-off. This tells us that trade-off inducing shocks to mark-ups leave the (path
and variability) of interest rates less affected than inflation.
Clearly, this simulation places an upper bound on this shift as it effectively removes shocks to mark-ups. In
practice, the evidence on how the variability of mark-ups may have evolved is mixed. On the one hand,
macro evidence suggests a fall in the variability of both output and inflation in many countries recently, a
finding that has been attributed by some to a lower incidence of mark-up shocks (for example, Smets and
Wouters (2007) and Kapetanios et al (2017)). On the other, direct micro-level evidence on mark-up
behaviour over recent years suggests a potential pick-up in their trend and variability.
To the extent that the historical evidence is consistent with a sequence of larger mark-up shocks, it would be
expected to have made the task of monetary policymakers somewhat harder. Both output and inflation will
have deviated more significantly and persistently from their long-run values. And although interest rates will
have been adjusted somewhat more often in response, the trade-off inducing nature of these shocks places
constraints on the degree of stabilisation monetary policy can achieve.

20
 Taylor (1979).
21
 The larger proportionate reduction in inflation than in output gap variability arises because mark-up shocks account for a larger
proportion of the historical variance of inflation than output.
22
 The interest rate smoothing term in the COMPASS Taylor rule will also have a role to play here in limiting the extent of the policy
response to the mark-up shock.
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
The limitations of this simulation approach need also to be borne in mind. First, the simulations consider
only the effects of temporary mark-up shocks, whereas in practice shocks may have been repeated and
persistent, as well as large. Second, more fundamentally, these simulations take the underlying model of the
economy as given – a strong, and probably unrealistic, assumption.
The competitive structure of the product market is one of the “deep parameters” in most standard
macro-economic models. So we would not expect the relationships embedded in those models necessarily
to be invariant to a rise in market power and mark-ups. Nor do these models typically take into account any
of the potentially macro-economic side-effects of a shift in market power – for example, on productivity.
Market Power and the Macro-Economy – A Theoretical Approach
To explore those questions, we draw on the textbook New Keynesian model of the macro-economy, the type
of which is often used to justify and assess the effects of flexible inflation targeting (Clarida et al (1999),
Woodford (2003)). Specifically, we use the well-known textbook New Keynesian model in Galí (2008) to
consider how changes in market power might affect the setting of monetary policy. We analyse the case
where higher mark-ups are effectively the result of reduced competitive pressures, rather than the rise of
highly productive ‘superstar’ firms.
The model takes the following generic form:
𝜋𝑡 = 𝛽𝐸𝑡𝜋𝑡+1 + 𝜅𝑥𝑡 + 𝑢𝑡 (1)
𝑥𝑡 = 𝐸𝑡𝑥𝑡+1 − 𝜎(𝑖𝑡 − 𝐸𝑡𝜋𝑡+1 −𝑟𝑡
∗
) (2)
where 𝜋 is inflation, 𝑥 is the output gap, 𝑖 is the policy rate, while 𝑢 is a cost-push shock.
The first equation is a New Keynesian Phillips curve, the second a forward-looking IS curve. This
two-equation system contains three key structural parameters: the slope of the Phillips curve, 𝜅; the interest
elasticity of demand, 𝜎; and the long-run neutral rate of interest, 𝑟
∗
. We discuss in turn how each might
potentially be affected by the degree of market power in product markets and higher mark-ups.
(i) Phillips curve
In the New Keynesian model, firms operate in imperfectly competitive product markets defined by
monopolistic competition (Blanchard and Kiyotaki (1987)), with nominal rigidities (Calvo (1983)). As first
described by Chamberlin (1933), and formalised by Dixit and Stiglitz (1977), monopolistic competition is an
environment in which a large number of firms each face a downward-sloping demand curve for their
respective differentiated product. This typically takes the functional form:
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
𝑌
𝑑
(𝑖) = (
𝑃𝑡
(𝑖)
𝑃𝑡
)
−𝜖
𝑌𝑡 (3)
𝑌
𝑑
(𝑖) is the demand for good 𝑖, whose price is 𝑃𝑡
(𝑖). 𝑃𝑡 and 𝑌𝑡 are the overall price level and aggregate
demand at time 𝑡, respectively, while 𝜖 is the elasticity of substitution between the monopolistically
competitive products.
Individual firms are assumed to be small enough that a price change by one firm has a negligible effect on
the demand faced by other firms. By implication, there is no strategic interaction between firms. Firms can
set prices above marginal costs because of a finite elasticity of substitution between individual goods in
consumer preferences, regardless of the fact that their share of the total market is small.
There are clear limitations of this product market formulation when addressing issues of market power.
Market power, in this setting, is captured by the capacity of firms to charge prices in excess of marginal
costs, rather than by their capacity to build-up ever-larger market shares. Market power means higher
mark-ups, but not higher degrees of market concentration. Different competitive settings might generate
quite different pricing behaviour and Phillips curves (for example, Rotemberg (1982)).
In this setting, the monopolistically competitive firm maximises profits by setting its price (𝑃
∗
(𝑖)) as a mark-up
over nominal marginal costs (𝑀𝐶):
𝑃𝑡
∗
(𝑖) = 𝜇𝑀𝐶𝑡 (4)
𝜇 =
𝜖
𝜖 − 1
(5)
where 𝜇 is the mark-up defined in terms of the elasticity of substitution.
This tells us a firm increases its mark-up as the demand for its good becomes more inelastic. That might
arise for a variety of reasons. Customer loyalty or brand might be one reason. Network economies of scale
and scope might be another. In either case, the implication is that prices are being set above (and sales
below) their socially optimal value – that is to say, their value under perfect competition when price equals
marginal cost (𝜖 tends towards infinity).
When prices are sticky, firms set prices in a forward-looking manner to get as close as possible to the
desired mark-up over time. The forward-looking Phillips curve in (1) summarises that price-setting behaviour
in the economy at large. The slope of the Phillips-curve, 𝜅, is a composite of deep structural parameters in
the model. In the baseline specification in Galí (2008), the slope is:
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
𝜅 =
(1− 𝜃)(1− 𝛽𝜃)
𝜃
1 − 𝛼
1 − 𝛼 +𝛼𝜖 (𝜎 +
𝜑 +𝛼
1 −𝛼
) (6)
𝜃 is the degree of price stickiness, 𝛽 is the discount factor in household preferences, 𝛼 is the degree of
decreasing returns to labour in production, 𝜑 is the inverse of the elasticity of labour supply with respect to
real wages (holding marginal utility of consumption constant), and 𝜎 is the inverse of the elasticity of
intertemporal substitution in consumption.
Significantly, the elasticity of demand, 𝜖, and hence the mark-up, 𝜇, affect the slope of the Phillips curve.
The higher the degree of market power and higher the mark-ups firms charge, the steeper the slope of the
Phillips curve (or, equivalently, the smaller the sacrifice ratio). We can roughly gauge the scale of this effect
by calibrating the model. Chart 16 looks at the relationship between the slope of the Phillips curve, 𝜅 and
mark-ups, 𝜇, for a given parameterisation of the model.
The relationship is, as we expect, a positive one. As a thought experiment, consider the scale of increase in
mark-ups seen by the average firm globally since 1980, from around 1.1 to around 1.6.
23
 Other things equal,
this would be expected on this calibration to have steepened the slope of the Phillips curve from just under
0.1 to around 0.2. This is a significant change in the parameterisation of a key macro-economic relationship
for the setting of monetary policy.
One way of explaining the intuition behind this steepening of the Phillips Curve is that market power reduces
the degree of strategic complementarity in price-setting. In a product market closer to perfect competition,
firms will be reluctant to raise prices fearful that, with other prices in the economy sticky, demand would
fall-away sharply. By reducing the elasticity of demand, market power reduces this risk and thus gives rise to
greater flexibility in prices – and hence a lower sacrifice ratio (for example, Ball and Romer (1990)).
Let us now put this model into a stochastic setting, by assuming desired mark-ups fluctuate around a trend
level according to some stationary stochastic process, log 𝜇𝑡
. These would now appear as cost-push
disturbances in the Phillips curve:
𝑢𝑡 =
(1 − 𝜃)(1 − 𝛽𝜃)
𝜃
1 − 𝛼
1 −𝛼 + 𝛼𝜖
log 𝜇𝑡 (7)
This tells us that, when mark-up shocks strike, a greater degree of market power will mean that a larger
fraction of the shock will fall on prices than activity. A rise in market power, through its effect on the slope of
the Phillips curve, will cause the Taylor output/inflation variability (policy possibility) frontier to rotate
clockwise, with more of the burden following a shock felt by inflation than by output variability.

23 De Loecker and Eeckhout (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
(ii) Impact on IS curve
The second way in which a shift in market power could potentially influence macro-economic outcomes is
through the IS curve. One structural parameter in that relationship is r*, the neutral rate of interest. In the
baseline model, the path of r* is determined by the path of shocks to households’ marginal utility of
consumption (𝑧) and firms’ TFP (𝑎):
𝑟𝑡
∗ = (1 − 𝜌𝑧
)𝑧𝑡 −
1 − 𝜌𝑎
𝜎
1+ 𝜑
𝜎
−1(1 − 𝛼)+ 𝜑 + 𝛼
𝑎𝑡 (8)
In steady-state, the equilibrium real rate is determined by household discount rates (determining saving) and
firms’ trend productivity growth (determining investment).
𝑟
∗ = −log 𝛽 + ∆𝑎 (9)
This suggests that, if there is any impact of market power and mark-ups on productivity, this could in turn
have an impact on r*.
24
 Consistent with that, and taken at face value, there is a positive correlation between
estimates of the US natural rate of interest and (the inverse) of global mark-ups since 1980 (Chart 17).
But whether higher mark-ups might in practice have contributed significantly to the global slowdown in
investment and productivity, and hence r*, is far from clear. The micro-economic evidence discussed earlier
suggests these effects are difficult to detect and, to the extent they do exist, might be relatively modest in the
contribution they have made to slowing aggregate investment and productivity growth. And if higher
mark-ups have, in fact, been the counterpart to a rise of ‘superstar’ firms, benefiting from network
economies, that would, in principle, raise productivity growth and r*.
A second structural parameter in the IS curve is the interest elasticity of aggregate demand, 𝜎. In the
simplest baseline model, this is determined by (the inverse of) the intertemporal elasticity of substitution. In
more general settings, with credit constraints, it may depend additionally on the balance sheet characteristics
either of borrowers or lenders or both (for example, Kashyap, Stein and Wilcox (1993)). A rise in market
power could, in principle at least, affect the balance sheets of borrowers and/or lenders in ways which could
influence 𝜎.
For borrowers, a rise in market power might raise equilibrium profit rates and market valuations. It may thus
reduce companies’ collateral constraints and their reliance on external sources of finance (for example,
Bernanke, Gertler and Gilchrist (1999)). For lenders, a rise in market concentration could in principle reduce
the speed of pass-through of policy rates to retail deposit and lending rates (Gerali et al (2010)). Each of

24
 Equation (9) is specific to the Galí model presented here, but equation (10) is a more general feature of macro-economic models,
where r* is a function of the household discount rate and productivity growth.
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
these would tend, therefore, to reduce the interest elasticity of investment demand, 𝜎. Whether these effects
are significant at the macro-economic level is, however, far from clear.
Market Power and Monetary Policy
Taking this evidence together, we now summarise the various channels we have discussed through which
monetary policy might prospectively be influenced by a rise in market power and mark-ups.
First, if shocks to firms’ mark-ups have increased over recent decades, this would tend to shift outwards both
output and especially inflation variability. Because these shocks are trade-off inducing, monetary policy is
constrained in its capacity to cushion them. In consequence, the path of interest rates (both its level and
variability) is affected only modestly by a rise in the incidence of mark-up shocks.
Second, a rise in market power also has the potential to alter the slope of the Phillips curve, as discussed
above. This, too, may affect the setting of monetary policy. To see that, consider the period loss function for
monetary policymakers in the textbook model. This can be derived from the utility function of the
representative household (Galí (2008)). It takes the form:
𝐿𝑡 = 𝜋𝑡
2 + 𝜆𝑥𝑡
2
(10)
where the weight on the output gap is:
𝜆 =
𝜅
𝜖
(11)
Under optimal discretionary monetary policy, the policymaker minimises this loss function each period,
subject to the Phillips curve, taking expectations as given. The optimal targeting rule for monetary policy is:
𝜋𝑡 = −
𝜆
𝜅
𝑥𝑡 (12)
This “Golden Rule” of monetary policy strategy simply states that the policymaker should let inflation absorb
more of the adjustment, after a trade-off inducing shock, either when the relative weight on output in the loss
function is high (𝜆) or when the sacrifice ratio is low (𝜅). If 𝜆 is set to its welfare-optimising level, this gives a
refinement of the “Golden Rule”:
𝜋𝑡 = −
1
𝜖
𝑥𝑡 (13)
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
This tells us that, once the dust has settled, the degree of market power in steady state is all that matters for
monetary policymakers in our simple framework when choosing the optimal trade-off between inflation and
the output gap.
Specifically, the greater this degree of market power, the steeper the slope of the optimal targeting rule: the
policymaker should let inflation absorb more, and the output gap less, of a trade-off inducing shock. The
intuition here is simply that market power increases the degree of price flexibility in the model and lowers the
sacrifice ratio. This makes it optimal to do a greater amount of (now less costly) output-smoothing in the face
of trade-off inducing shocks in the optimal policy rule.
It is possible to calibrate this parameter in the optimal targeting rule. Chart 18 plots the relationship between
it and mark-ups. The rise in average mark-ups globally since 1980 would, on this calibration, be expected to
raise the trade-off parameter in the Golden Rule (
1
𝜖
) from around 0.1 to 0.4. This is a reasonably significant
shift in the terms of trade between inflation and output variability.
Chart 19 seeks to brings all of these points together graphically; it is an empirically-calibrated version of
Figure 1. Point A is the starting equilibrium, before any rise in the (trend and variability) of mark-up shocks,
with the policy possibility curve tangent to the policymaker’s loss function. A rise in the (trend and variability)
of mark-ups then has three distinct effects.
First, an increase in mark-up volatility causes an outward shift in the policy possibility frontier (A to B),
calibrated here to be equivalent in scale to the outward shift from the counterfactual simulations. Second,
there is the clockwise rotation of the Phillips curve, and hence in the policy possibility frontier (B to C),
calibrated to be equivalent to the mark-up rise among firms globally since 1980. And third, there is the shift
in the relative weight placed on output stabilisation by the policymaker, calibrated in line with the mark-up
shift (C to D).
The net effect of increased market power is a significant rise in inflation variability but relatively less change
in output variability. Chart 20 uses the same calibration, but looks at the relationship between inflation and
interest rate variability. Despite the significant shifts in macro-economic relationships and in policymakers’
preferences arising from a rise in market power, the net effect on the (level and variability) of the optimal
interest rate path is more modest. At root, that is because of the trade-off inducing nature of mark-up
shocks.
Third, the presence of market power and higher mark-ups also has implications for the level of output in the
economy which could provide additional incentives to generate inflation. Monopolistic competition implies
that output is inefficiently low relative to its (perfectly competitive) social optimum. For sufficiently small
deviations from the steady state, the policymakers’ loss function can be re-written to reflect that inefficiency:
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
𝐿𝑡 = 𝜋𝑡
2 + 𝜆𝑥𝑡
2 − Ʌ𝑥𝑡 (14)
where
Ʌ =
1
𝜖
2
(1 − 𝜃)(1 − 𝛽𝜃)
𝜃
1− 𝛼
1 −𝛼 + (𝜍 + 𝜑)𝜖
(15)
The optimal targeting rule is also then altered to become:
𝜋𝑡 =
1
𝜖
2
(𝜎
−1 +
𝜑 + 𝛼
1 − 𝛼
)
−1
−
1
𝜖
𝑥𝑡 (16)
The additional constant term reflects the well-known inflationary bias under discretionary policy, first
articulated by Barro and Gordon (1983). Output at a sub-optimally low level increases the incentives of a
discretionary policymaker to run looser monetary policy to push output towards its social optimum, thereby
generating an inflation bias. In steady-state, this inflation bias is:
𝜋𝑡 =
Ʌ𝜅
𝜅
2 + 𝜆(1 − 𝛽)
(17)
This inflation bias is clearly bigger, the greater is the degree of market power. Chart 21 plots the relationship
between mark-ups and the inflation bias implied by the model. Using our simple model, the rise in mark-ups
by global firms since 1980 might have added as much as 20 percentage points to the inflation bias!
This calibrated effect is implausibly large. Nonetheless, the qualitative point remains: a rise in market power
may, by constraining demand, generate an added incentive to run loose monetary policy. That makes
institutional arrangements which resist those temptations – such as independent central banks charged with
meeting an inflation target – more important than ever (Rogoff (1985), Svensson (1997)).
Finally, while we find little evidence of mark-ups having a material effect on investment, market power also
gives rise to at least the possibility of lower productivity growth, and hence, r*. There are a number of other
structural forces currently lowering productivity growth (for example, Gordon (2012), Andrews et al (2016)).
And there are a larger number still of structural factors bearing down on r* (for example, Monetary Policy
Committee (2018)). To the extent market power is another, this increases the chances of those falls in
productivity growth and r* proving long-lasting.
The empirical link between market power and productivity is not, at present, well-defined. But the
implications of a persistently lower r* for the setting of monetary policy are reasonably well understood.
They include the fact that the probability of the zero lower bound constraint binding is likely to be materially
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
higher than it has been historically. Recent simulation studies have suggested this probability may be as
high as around one-third, if r* remains around current levels (for example, Kiley and Roberts (2017)).
Implications for Future Research
The link between the competitive structure of product markets, the macro-economy and the setting of
monetary policy is a relatively under-researched area. This paper has only scratched the surface of this
important topic. But trends in concentration and market power have clear potential to impact on the
macro-economy and monetary policy, justifying ongoing research on the topic. To that end, we conclude
with a few reflections on potentially fruitful future research avenues.
First, the framework used here to understand the macro-economic implications of increased market power
assumes a particular competitive structure – monopolistic competition. This has limitations, assuming as it
does that no one firm is sufficiently large to have a significant bearing on others’ behaviour. In practice,
strategic interactions between firms are likely to be important in many markets, especially network markets
(for example, Bramoullé, Kranton and D’Amours (2014)), with potentially important implications for pricing
and the Phillips curve.
Second, the framework developed here also sidesteps questions about the competitive structure of the
market for inputs, especially labour inputs. Dominant firms may exercise monopsonistic power over workers,
in ways which have implications for profit and labour shares. Consistent with that, there is some empirical
evidence linking market concentration to a lower labour share (Autor et al (2017)). How monopsony power
influences wage growth and the slope of the Phillips curve are important areas to consider further.
Third, there is further work to be done in understanding the balance sheets and decision incentives of
so-called “superstar” firms. This includes their choice of debt versus equity, distributing versus reinvesting
profits and intangible versus tangible sources of capital. These choices might imply quite different incentives
and behaviours – for example, about the level of investment and its interest elasticity. These are yet to be
fully explored, at a micro and macro level.
Fourth, the emergence of a set of firms with significant degrees of market power clearly raises big questions
about the appropriate stance of competition policy (Gutiérrez and Philippon (2018)). These policy issues are
clearly outside of the remit of central banks. Nonetheless, how these anti-trust issues are tackled could
have implications for the structure and dynamics of the economy and hence for the setting of monetary
policy. This, too, is an area ripe for further research.
Finally, the apparent puzzle between the secular rise in mark-ups at the firm level on the one hand, and
relatively low and stable aggregate inflation on the other hand, could usefully be reconciled. Current
estimates of mark-ups using firm-level data may suffer from mismeasurement. Or other macro-economic
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
factors may have more than offset their impact in order to keep inflation stable. How the evidence is
reconciled could have important implications for inflation dynamics and the setting of monetary policy. 

As someone whose family hails from the North-East, and who themselves was born in the North-East, it is
wonderful to be here in Redcar. Regional visits like this are crucial for the Bank of England. They enable us
to take the temperature of the local economy by speaking to a broad cross-section of businesses, charities,
social enterprises, schools, further and higher education institutions and trade unions.
Here in the North-East, it is clear the temperature of the local economy is not as high as either you or I would
wish. Despite having come down rapidly over recent years, the unemployment rate, at over 6%, is still
higher than for any other region in the UK. And the closure of the steelworks down the road a little over a
year ago was a further blow to Teesside, with the loss of 2,200 jobs.1
Yet it is not all doom and gloom. There are more people in work in the North-East today than at any time in
its history. Over 48,000 more people have found jobs over the past 12 months. And the North-East alone
has contributed 20% towards the overall fall in national unemployment over the last year. Or, if you like, the
increase in employment over the last year is 7 times that at Nissan’s Sunderland plant alone. Despite the
media headlines, the North-East has been one of the most dynamic areas of job creation in the country
recently.
Underpinning this above-average jobs performance has been above-average productivity growth in the
region. So it is particularly appropriate to be here today at the Materials Processing Institute, which has been
a global centre for research on the iron and steel industry since 1945. The Institute has contributed
importantly to Teesside having a high concentration of high value-added industries, such as chemicals and
processing, which have boosted jobs and productivity in the region.
Today, I want today to discuss the economy, at both a national and regional level. I want to discuss what
monetary policy can and, as importantly, cannot do to support the economy, nationally and regionally. And I
will also touch on what role other policies might play in supporting growth, here in the North-East and
nationally, in particular when it comes to boosting skills, innovation and productivity.
A Two-Car Economy
I want to set the scene by describing an imaginary economy. Now I know what you are thinking – typical
economist, head in the clouds, aimlessly theorising without any grounding in the real world. But bear with
me on this one, as I think my stylised economy might help in making sense of the real world to which I
promise I will then return.
This imaginary economy comprises, for simplicity sake, two regions - Red Car region and Blue Car region.
These two regions have quite different economies. The economic make-up of their firms and households is

1
 BBC News October 2015, ‘SSI Redcar Steelworks to be shut’.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
3
3
different in terms of their production processes and spending patterns. So too, as a result, is the level of
activity, income and productivity in the two regions.
Red Car region comprises a set of firms who make red cars. To do so their production processes rely on few
machines and lots of workers. Because there are few machines, the workers employed do not need
especially high skills. And because firms employ lots of these lower-skilled workers, their levels of
productivity – the number of cars produced per worker – is relatively low.
Households in Red Car region earn income from working in red car firms. Because these firms have low
productivity, they tend to pay low wages to these workers. And because, as a result, household incomes are
low, workers can only afford to buy one red car and often having to borrow money from the bank. In other
words, households in Red Car region have lower net wealth.
Blue Car region is, in some respects, its mirror image. Firms produce blue cars using lots of machines and
relatively few workers. To operate these machines, workers require high skills. Given this mix of machines
and skilled workers, blue car firms have high levels of productivity. And as high levels of productivity
translate into higher profits, blue car firms have less need to borrow from the bank than red car firms.2
Meanwhile, households in Blue Car region benefit from higher wages, courtesy of their higher level of skills
and productivity. These high incomes enable them to own two blue cars, often without needing to borrow
from the bank at all. In other words, households in Blue Car region have higher net wealth than in Red Car
region.
The Role of Monetary Policy
Now let’s hit this hypothetical economy with a nasty surprise – say a shock to the banking sector, which
reduces its willingness to lend to households and firms, or increases the costs of doing so. This tightening of
credit conditions affects households and companies in both regions, as both rely on banks to some degree.
Firms whose credit conditions have tightened face a profits squeeze and cut their production of (red and
blue) cars. Households facing the same tightening in credit conditions face an income squeeze and cut their
consumption of (red and blue) cars. As a result, aggregate activity or GDP in the economy – the sum of red
and blue car production – falls in this hypothetical economy. There is a recession.
While both regions are affected, the impact of this recession is not felt equally. Red firms might borrow more
than Blue ones and Red consumers more than Blue ones. This means the tightening of credit conditions

2
 In practice borrowing behaviour is likely to be driven by several other factors, such as expectations of future earnings, access to
finance and the amount of collateral with which to borrow against.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
4
4
has a larger impact on Red Car than Blue Car region - its recession deeper, the impact on unemployment
greater. This further widens the income gap between Red Car and Blue Car regions.
What can be done? Enter a central bank with a mandate to support the whole economy. They can do so in
two ways. First, they can cut the interest rate at which they lend to banks which, in turn, is reflected in banks’
lower cost of lending to companies and households. Second, they can buy assets – which, in this
hypothetical economy, means cars – increasing demand for them and hence their price.
These actions lower the cost of borrowing and raise the price of assets in the two-car economy. Both
support the production of cars by firms and the consumption of cars by households. In this way, aggregate
activity in the economy is boosted, at least in the short run. By lowering interest rates and purchasing
assets, a central bank can help meet its mandate of supporting the economy as a whole.
If the aggregate impact of looser monetary policy on the economy is positive, what about its distributional
impact across regions? In this simple setting, lower interest rates boost the incomes of net borrowers in
Red Car region, while reducing the income of net savers in Blue Car region. As the income of Red Car
consumers is lower than Blue Car consumers, this shrinks the distribution of incomes across the economy.
The effect of asset purchases is to boost the wealth of all car owners, with the bigger boost felt by those
whose car wealth is largest. So although both regions are better off in net wealth terms, Blue Car region
benefits more than Red Car region. Put differently, the distribution of wealth across regions may widen as a
result of asset purchases. Or that at least is what this simple, stylised economy would imply.3
The Real Economy
Now let’s return to planet earth, from the hypothetical to the real world. In fact, as you might have guessed,
these two worlds are not as unalike as might first appear.
Chart 1 plots a map of the UK by region, shaded according to levels of nominal GDP per head prevailing in
2014, the latest date for which we have data. There are clearly significant differences across region. The
GDP per head of the richest region (London) is around £45,000, 2.4 times higher than the poorest parts of
the UK (Wales and the North East) at around £18,000 per head. These are, if you like, the Blue and Red
Car regions.4
 Indeed, regional differences in UK living standards are among the highest in Europe.5

3
 I am simplifying here considerably. Lower interest rates will also boost asset prices, altering the distribution of wealth. And asset
purchases will also influence borrowing costs, affecting savers and borrowers. This story also only considers the direct financial effects.
Monetary policy can also have several wider indirect macroeconomic effects by boosting activity and employment which can also have
distributional impacts. For further details see Draghi (2016).
4
 See Haldane (2016), CBI (2016), Institute for Public Policy Research (2016).
5
 According to data from Eurostat (2016), the gap in GDP per head between the richest and poorest regions in the UK ranked fourth in
a sample of 20 EU countries on a comparable PPP adjusted basis in 2014.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
5
5
The vast majority of national income – around three-quarters - is paid to workers in the form of wages.6
 This
means there is a close correspondence between the regional distribution of GDP per head and wages per
household (Chart 2). The average weekly wage across the UK in the first half of 2016 was £540. In the
highest-wage region, again London, it is £670 per week. That is around 40% higher than in the lowest-wage
region, the East Midlands, at £480 per week. These are, if you like, the red and blue car households.
These regional differences in household incomes are mirrored in regional patterns of household spending
and saving – spending on durables like houses and cars and savings in pensions and financial assets.
Chart 3 shows the regional distribution of net wealth, based on the ONS Wealth and Asset survey of around
30,000 households. There is considerable dispersion, with the highest net wealth region (the South-East at
£340,000 per household) holding assets 2.3 times larger than the lowest wealth region (the North East at
£150,000).
Turning from saving to spending, Chart 4 shows the regional pattern of car ownership. This also shows wide
dispersion, broadly mirroring the pattern of regional wages. The highest car-owning region is the
South-East, with 1.3 cars per household. The lowest, excluding London, is the North-East, at around 0.9
cars per household.
So what explains these large regional variations in levels of economic activity, income and spending?
Plainly, many factors are at work, economic, geographic, historic, social. There is a large literature on the
determinants of growth, which suggests each of these factors is important.7
 But let me focus on one which is
I think key to understanding regional differences - levels of productivity among firms.
A few years ago, the US economist Paul Krugman noted: “productivity isn’t everything but in the long run it
is almost everything”.8
 Krugman was talking about the determinants of growth, and hence living standards,
in the economy over the longer-run. He assigns productivity a central role. The raw numbers, looked at over
long spans of history and large cross-sections of countries, suggest Krugman is right.
Take a simple example. Since 1850, inflation-adjusted national incomes in the UK have risen around
20-fold.9
 This is a massive rise in living standards. How much of that was the result of higher productivity?
Well, in the absence of productivity growth, UK living standards today would only be around double their
1850 value – in other words, at late-Victorian levels. In the UK, productivity has not been everything but it
has been almost everything. The same is true elsewhere around the world.

6
 ONS Quarterly National Accounts 2016 Q2, available here
7
 See World Economic Forum (2016) for a summary.
8
 Krugman (1994).
9
 Hills et al (2016).
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
6
6
What is true across time is also true, by and large, across regions. Chart 5 looks at a regional map of
firm-level productivity, where productivity is measured as output per hour worked. This map is built up from
several sources of data on output and employment collected by the ONS using surveys covering tens of
thousands of firms across all regions in the UK and all sectors of the economy.10
 It paints a pretty
comprehensive picture of the regional workings of the UK corporate sector.
This productivity map suggests, perhaps unsurprisingly, a wide degree of regional dispersion. The most
productive region (London) has firms with levels of productivity almost 60% higher than those in the lowest
productivity part of the UK (Northern Ireland). This map matches closely the regional pattern of GDP per
head and wages, consistent with Krugman’s conjecture. Much of the regional variation in economic activity
and household income derives, it seems, from variations in productivity among firms.
These regional maps suggest a simple but coherent story about regional variations in living standards. The
more productive a firm, the higher the wage it pays. If levels of regional productivity differ greatly, so too will
regional levels of economic activity and household income. So Red and Blue Car regions comprise low and
high productivity firms, and thus low and high income households, respectively. This is not an imaginary
world; it is the real world. This is not a hypothetical story; it is the UK story.
If levels of productivity among companies hold the key to explaining regional variations in incomes, this
naturally begs the question why does productivity differ by region. Past empirical studies have highlighted a
range of factors - economic, geographic, technological, social – in explaining productivity.11
 From that
potentially long list, let me highlight two factors: physical capital and human capital.
One important determinant of a firm’s efficiency is the amount it invests in machines and technology - its
physical capital stock. Measuring the capital stock is difficult even at an aggregate level and there are no
reliable data on regional capital stocks. There are data, however, on regional patterns of business
investment and research and development spending – the flow rather than stock of physical capital.
Chart 6 shows regional spending on R&D per head of the working age population. This shows a wide
regional divergence, with the most R&D-intensive region (the East of England) having levels of investment
four times those in the least R&D-intensive region (the North East). This map has a close correspondence
with regional measures of productivity, consistent with capital-intensive regions having higher levels of
productivity.12
A second determinant of firm-level productivity is the skill level of workers, so-called human capital.

10
 See ONS ‘Sub-regional labour productivity’, available here.
11
 See Barnet et al (2014), IMF (2015), Haldane (2015), OECD (2015).
12
 See CBI (2016)
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
7
7
The greater those skills, the more productive workers are likely to be. Measuring human capital, as with
measuring physical capital, also has its problems. But Chart 7 shows one measure of skills – the proportion
of the workforce with at least NVQ Level 4 qualifications, roughly-speaking undergraduate degree level.
It points to sharp regional differences. The percentage of the workforce with higher-level qualifications is
20 percentage points higher in London than in Northern Ireland, Yorkshire and Humberside and the North
East. These regional skill differences have, predictably, a high correlation with regional productivity. Human,
as well as physical, capital accumulation appears to be a significant part of the regional productivity story.
One downstream consequence of lower levels of skills and investment is likely to be higher levels of
unemployment. Chart 8 shows that this is true, with higher rates of unemployment in regions with lower
levels of productivity and skills. This relationship appears to be particularly strong for levels of youth
unemployment among those aged 16-24 (Chart 9). More generally, higher levels of productivity are often
associated with higher levels of employment, not lower.
It is not just standard economic metrics, such as incomes, that show a high degree of regional variation.
So too do measures of social well-being, such as mortality rates. The mortality rate among working age
males in London is around half that of those living in Scotland and the North-East (Chart 10). Male life
expectancy in London and the South East (at around 80 years) is 2 to 3 years longer than in these parts of
the UK.13
So this is a story of wide regional differences in incomes and well-being sourced, to a significant extent, in
regional differences in firm-level productivity. Those differences are themselves rooted in differing levels of
investment in machines and people, in technology and skills, in physical and human capital. We have red
and blue firms hiring red and blue workers, resulting in red and blue car levels of income and productivity.
Which is the North-East? Levels of GDP per head, wages and productivity in the North-East are,
respectively 25%, 8% and 11% below the UK average. Levels of physical and human capital accumulation
are also lower. Levels of unemployment are currently 1.3 percentage points higher and 4.4 percentage
points higher for young people. North-Eastern households own half as many assets and fewer cars than
South-Eastern households. Redcar is a Red Car region.
The Role of Monetary Policy
These maps lay bare both some of the challenges, and the opportunities, facing the UK economy.

13
 See ONS ‘Life Expectancy at Birth and at Age 65 by Local Areas in the United Kingdom’, available here.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
8
8
The challenges include the fact that growth in income and productivity at an aggregate level has recently
been slower than in the past - the economic pie has not risen rapidly. They also include the fact that the
distribution of income and productivity across regions has been uneven: the pie has been unequally sliced.
The opportunities flow from these challenges. Given these sharp regional differences, if a way could be
found of raising the productivity of lower-performing regions, this could have a material impact on aggregate
productivity and hence national income. For example, if all of the regions in the lower part of the productivity
distribution were lifted to the UK average, this would raise average levels of UK productivity by around 6%. If
all regions were lifted to the average level of London and the South East, this would raise the average level
of UK productivity by 20%. Levels of UK productivity would then rival those in France and Germany.
But how is this to be achieved? At least over the longer run, it is reasonably clear that monetary policy is
ill-equipped for the task of either expanding the economic pie or altering the way it is sliced. Over those
horizons, nominal things like money and monetary policy cannot affect real things like productivity, skills and
investment. In the language of economics, money and monetary policy are neutral in their impact.14
You do not have to be an economic theorist to see why. If printing money or setting interest rates had a
durable impact on activity, incomes and living standards, economic policy would be simple and we would
have long since cracked it. Raising living standards could be achieved by simply keeping the printing
presses running. Countries that have done so have found this tends to end, not in triumph, but disaster.
Monetary policy is also unable to influence the distribution of outcomes across the economy over
medium-term horizons. The reason is that central banks cannot set different interest rates for different
sectors or regions, for different households or companies, or for borrowers and lenders.15
 Monetary
policymakers are, at least in normal times, one club golfers.
None of this is to suggest that monetary policy is entirely unimportant in its impact on the size of the
economic pie, and its distribution, over shorter-run horizons. As our two-car economy example illustrated, it
is perfectly possible for monetary policy to influence both aggregate income, and its distribution, in the
shorter run. That being the case, it is worth assessing the evidence on that over recent years.
(a) Aggregate Impact
Starting with the impact of monetary policy on aggregate activity, since the dawn of the global financial crisis
monetary policy has been loosened significantly in a large number of countries around the world, using a
combination of lower official interest rates and, in some cases, asset purchases or QE. These measures
have aimed to cushion the impact of the financial crisis on the economy and on jobs.

14
 Not all economists agree on this, for example see Aghion and Howitt (1998).
15
 In addition, there is limited information on regional consumer prices and how much variation there is in regional inflation rates.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
9
9
Here in the UK, interest rates were lowered from 5% to 0.5% between September 2008 and March 2009 by
the Bank’s Monetary Policy Committee (MPC). This took interest rates to their lowest levels in the Bank of
England’s over 300-year history. This was augmented with £375billion of asset purchases between 2009
and 2012, almost exclusively in government securities. In interest-rate equivalent terms, this amounted to a
monetary stimulus worth around 7 percentage points.16
 Exceptional times called for exceptional measures.
What impact have these exceptional measures had? One simple way to begin to answer that question is to
look at how aggregate activity has evolved in the period since. Chart 11a looks at the growth in
inflation-adjusted GDP per head across the UK regions since the MPC’s monetary stimulus was put in place.
All regions have experienced a rise in nominal GDP per head over that period, averaging 12%. Not all
regions have experienced an inflation-adjusted increase, with growth strongest in London (a rise of 6.6%)
and weakest in Northern Ireland (a contraction of 4.6%).
Unemployment rates rose sharply during the financial crisis. But they have since fallen across every region
in the UK since 2011 and by roughly similar amounts, on average 3 percentage points (Chart 12a). To
complete the picture, net wealth has also risen across the majority of regions since 2008, on average by 13%
(Chart 13a). In only three regions (the North East, East Midlands and West Midlands) has net wealth fallen.
These patterns are broadly as we would expect if monetary policy has loosened credit conditions, boosted
asset prices and wealth and thereby supported domestic demand, at least in the short-run.
That said, this is a rather imperfect experiment. Factors others than monetary policy are also likely to have
contributed to movements in GDP, unemployment and wealth (in particular housing wealth) over that period.
To pinpoint the specific contribution made by monetary policy, we need a model of the economy. By using
the Bank of England’s suite of macro-economic models, we can quantify the specific role played by monetary
policy in explaining movements in incomes and jobs, albeit rather imperfectly given that all models are
imperfect.
Charts 11b, 12b and 13b plot the change in real GDP per head, unemployment and wealth respectively
since 2008, under the hypothetical assumption that monetary policy had not been loosened over the period
but had remained at levels prevailing in 2008.17
 This paints a stark picture of what might have been in the
UK.

16
 This is set against a trend decline in global real interest rates and what economists refer to as the ‘neutral’ level of interest rates.
This means that it is very unlikely that Bank rate will return to the levels seen immediately before the crisis in the near future.
See Haldane (2015) and Broadbent (2016) for further details.
17
 These simulations take no account of regional variations in the monetary policy transmission mechanism. They assume that Bank
rate would have stayed constant, and there would have been no quantitative easing programme. This stylised simulation is estimated at
the aggregate level, and applied uniformly across the UK. All regions are affected by the same percentage change in each variable. Net
wealth is assumed to move in line with nominal GDP, and no distinction is made between the various different types of wealth (such as
housing or financial wealth). It also assumes that the monetary regime does not change and agents do not alter their view of the MPC’s
reaction function, despite observing extremely large economic shocks.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
10
10
Without the support of looser monetary policy, all regions would then have experienced a contraction in real
GDP per head since 2008. Every part of the UK would have been a Red Car region. This contraction would
have been large, averaging 7%, equivalent to an annual average income loss of £1,600 per person. Here in
the North-East, the loss in annual income for the average household would have been similar in magnitude.
The picture is similar if we turn to unemployment. In the absence of a monetary policy easing,
unemployment would have risen further after the crisis in every region in the UK, by an additional
4 percentage points. Around an extra 1.5 million people would have found themselves out of work. Here in
the North-East, the unemployment rate today would have been over 10%, or an extra 50,000 people out of
work. That is the equivalent to losing nearly seven Nissan factories.
Absent a monetary policy loosening, net wealth would also have contracted across all UK regions, on
average by around 20% or around £35,000 per household. Here in the North-East, that decline of wealth
would have been around £60,000 for the average household or more than three times annual GDP per head.
Without a monetary policy loosening, almost every town and city in the UK would have been painted red.
Of course, it is possible these model estimates overstate the impact of monetary policy in boosting demand
and asset prices. These estimates also make no explicit allowance for the fact that different regions might
be affected differently by a given monetary policy stance. Nonetheless, it is clear monetary policy has played
a material role in lifting all boats since the financial crisis broke. Indeed, without it, it is plausible to think the
vast majority of regional boats would have sunk.
(b) Distributional Impact
Even if monetary policy has lifted all boats, and could plausibly do so again if needed, that does not mean it
has done so equally. In particular, concerns have been expressed about the potential distributional effects of
monetary policy. In the simple two-car economy described earlier, monetary policy did have some potential
short-run impact, potentially shrinking the distribution of income but raising the distribution of wealth. What
does the real-world evidence suggest?
Several of my colleagues on the MPC have recently presented evidence which speaks to this issue. For
example, Ben Broadbent has looked at evidence on the Gini coefficient – a measure of the degree of
inequality - for UK households’ income and wealth over recent years. He finds no evidence of these
inequality measures having increased over the period since the UK’s monetary policy loosening.18
Jan Vlieghe has recently looked at how monetary policy may have affected the fortunes of, among others,
savers, pension funds and pensioners. The empirical evidence does not suggest these cohorts have been

18
 Broadbent (2016).
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
11
11
disadvantaged to any significant degree by the monetary policy stance. For most members in each cohort,
the boost to their asset portfolios and the improved wages and profits due to a stronger economy more than
offset the direct loss of income from lower rates.19
If we look at the regional dispersion of wages, unemployment and wealth over the period of the UK’s
monetary policy easing, the picture is roughly as we might expect. Comparing Charts 2 and 15, there is little
correlation between levels of wages and the change in them over recent years. In other words, the regional
distribution of incomes does not appear to have changed materially during the period of monetary loosening.
The same is true of unemployment rates (Charts 9 and 12a). There is some evidence of the higher-wealth
regions benefitting more than the lower-wealth regions over the period since the crisis (Chart 3 and 13a).
(c) Recent Monetary Policy
In August this year, in the light of the EU referendum result, the Bank’s MPC undertook a further round of
monetary policy easing. Interest rates were lowered further, to 0.25%. And the MPC agreed to undertake a
further £70 billion of asset purchases, of which gilts comprised £60 billion and corporate bonds £10 billion.
The Bank also introduced a Term Funding Scheme (TFS) to facilitate the pass-through of lower official
interest rates to the retail interest rates facing households and companies.
It is too early to assess what impact this further monetary policy loosening will have on activity and
unemployment. Estimates from the Bank’s model suggest it could increase activity by around 0.5% over a
three year horizon and support around an extra 100,000 jobs. In an environment of heightened uncertainty
about future growth and jobs, these gains are not to be sniffed at.
In the period since the MPC’s easing in August, there have been two pieces of material news about the
performance of the economy. The first is that growth in the immediate aftermath of the referendum has been
stronger than the Bank, and other major forecasters, had expected. For example, growth in the third quarter
of 2016 is currently estimated to be 0.5%. This compares with 0.1% growth in the third quarter expected at
the time of the Bank’s August Inflation Report.
This is welcome news. The main source of this upside news to activity has been the buoyancy of
consumers. They appear to have kept calm and carried on spending on the high street, in the car
showrooms and in the estate agents. The Bank’s latest survey of UK households explains why. It suggests
UK households’ perceptions of income and job prospects have, at least so far, been largely unaffected by
Brexit. That is true too in surveys of consumer sentiment which remain around historical average levels.

19
 Vlieghe (2016).
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
12
12
Spending by companies shows a more mixed picture. For example, according to the Bank’s new company
Decision Maker Panel20, just over half of all firms anticipate no material impact of Brexit on their investment
plans over the coming year. That leaves just under a half of all firms expecting some impact on investment,
consistent with surveys of gently falling investment intentions by the CBI, BCC and the Bank’s Agents.
The second piece of significant news since August has been around the exchange rate. This is currently
around 12% lower than immediately prior to the EU referendum. Indeed, uncertainty about the outcome had
already caused the exchange rate to depreciate ahead of the vote, such that it is now 17% lower than a year
ago. The Bank’s contacts in financial markets attribute this fall in sterling to expectations that the UK’s
future trading arrangements, and hence income-earning capacity, will be materially less favourable than at
present.
Whatever the precise explanation for sterling’s fall, its near-term impact on inflation and growth is likely to be
significant. It will provide some support for net exports in the short-term. But it will also cause a sharp rise in
import prices, and in time consumer prices, over the course of the next few years. That will in turn tend to
depress inflation-adjusted household incomes, and hence consumer spending, over this period.
It is unclear at present how the disconnect between the relative optimism of households and some
companies, on the one hand, and the relative pessimism of financial market players and the other half of
companies, on the other, will be reconciled. Is this a case of skittish financial market players over-sensitive
to economic risks that may never materialise? Or sluggish consumers insensitive to the weaker economic
prospects that lie ahead? Only time will tell.
In the meantime, however, the MPC needs to form its own judgement on prospects for inflation and output.
These were set out in the November Inflation Report. The projections for inflation were the highest ever
published by the Bank, while the forecasts for output growth were the lowest ever published. In other words,
these projections highlighted the difficult trade-off facing the MPC in keeping inflation close to target, while
supporting output and jobs growth in the economy.
In the MPC’s judgement in November, managing this trade-off was best achieved by maintaining the current
monetary policy stance, with a neutral bias on the direction of the next move in interest rates. Consistent
with this neutral bias, the MPC judged the risks to inflation and output to be broadly symmetric and, given
heightened uncertainty since the referendum, larger than usual.
My own personal judgement on the appropriate monetary policy stance is close to the MPC consensus. In
August, I believed the risks to inflation and output lay to the downside, even after the stimulus measures.

20
 See http://www.bankofengland.co.uk/research/Pages/onebank/decisionmakerpanel.aspx.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
13
13
That is why I supported forward guidance on a further lowering of interest rates in the run-up to the end of
the year, provided the economy evolved in line with the MPC’s projections for inflation and output at the time.
In the event, output has out-performed those expectations, while expectations of inflation have picked up,
largely as a result of sterling’s depreciation. That configuration now leaves me comfortable with the current
stance of monetary policy, with no bias on the direction of the next move in interest rates. Consistent with
that, my subjective distribution of risks around these projections is now symmetric.
On the upside, the fall in sterling has had a quite pronounced and, as importantly, persistent impact on
financial market and, to a lesser extent, consumer measures of inflation expectations. Financial market
measures of expected inflation five years ahead over the following five years have risen by around 60 basis
points from their mid-year low-point. There has also been a tick-up in equivalent measures in the US and
euro-area over the past few months of around 50 and 30 basis points respectively (Chart 16).
In each case, this does no more than take inflation expectations back to around where they were at the start
of the year. In other words, recent adjustments have removed the downside skew to inflation expectations
which had emerged. In that sense, the recent adjustment in inflation expectations is benign and helpful.
Nonetheless, a further large upwards shift in expectations from current levels would, I think, be less benign
and possibly unhelpful, especially if it were expected to persist.
On the downside, I think there is still a material chance that growth could under-perform relative to
expectations during the course of next year – for example, if the squeeze on household incomes prompts a
larger adjustment in household spending. In that event, the upside pressures on inflation would be weaker
than expected. In that event, the chances of the zero low bound constraint on interest rates coming into play
would increase, potentially limiting somewhat the effectiveness of any monetary policy loosening.
Bank staff have conducted simulations to assess the probability of this zero lower bound constraint binding in
future, based on the likely constellation of future shocks to the UK economy. Given the MPC’s existing set of
tools, this puts this probability broadly in the range 15-40%. So this is a material risk, for monetary policy
and for the economy, which needs to be weighed in the setting of policy. My personal view is that this
provides grounds for not proceeding too hastily with any tightening of the monetary policy stance.
The Role of Industrial Policy
If monetary policy can provide only short-term support for economic activity, and can do little to shape the
distribution of activity across regions, sectors and individuals, what other policy options are available?
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
14
14
The short answer is policies which act to improve skills, investment and productivity at source. This is one of
the key aims of the Government’s Industrial Strategy and underlines its importance, nationally and
regionally.21

Last week’s Autumn Statement outlined a number of measures to boost investment, skills and productivity.
For example, a significant increase in spending on infrastructure was announced, through a new
“National Productivity Investment Fund”. This will result in an extra 0.1% of GDP in infrastructure spending
in 2017/18, rising to 0.3% of GDP by 2021-22. Over five years, the incremental spending amounts to an
extra £23 billion. This fund will be focussed on transport, housing, telecoms and research and
development.22

Just yesterday, the CBI published a report on differences in regional productivity performance and outlined a
sequence of measures to help boost productivity and narrow regional differences. These measures included
developing regional scorecards to enable productivity hotspots (and coldspots) to be identified; increased
investment in skill-building through improved school and vocational education; increased infrastructure, in
particular transport, spending; and improvements in managerial practices.
The work of the “Productivity Commission”, chaired by Sir Charlie Mayfield, has identified a long tail of
companies across all sectors in the UK whose productivity performance is falling short.23
 The Commission
are developing, among other things, a tool which would enable firms to benchmark themselves relative to
others in their sector along several key business dimensions. This could then serve as a prompt for action,
enabling firms to boost their productivity performance through targeted action.
I think this micro-level assessment of productivity is a useful way to formulate plans which support
productivity, and narrow productivity differences, regionally and sectorally. For example, recent work by the
OECD has looked at the changing distribution of productivity across firms over time.24
 It suggests a widening
– or bifurcation - of this distribution, with a small set of frontier firms whose productivity growth continues
apace but a long tail of laggard firms whose productivity has effectively stagnated.
The UK broadly matches that picture. Chart 17 plots some percentiles from the distribution of productivity
(output per worker) at the firm level, based on a sample of 30,000 companies from the ONS Annual
Respondent Database (ARDx) since 2002. This database combines data from the ONS Annual Business
Survey and employment surveys, which are used to inform the aggregate National Accounts and Labour
Market statistics, and allows us to analyse what is going on at the firm level. Several features are striking.
First, it is clear that at least three-quarters of all firms in the sample have seen productivity flat-line over the
past 15 years. There is a long tail of companies who have, at least in efficiency terms, stood still.

21
 See May (2016), Clark (2016) and House of Commons (2016).
22
 CBI (2016), HMT (2016).
23
 Mayfield (2016).
24
 OECD (2015).
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
15
15
Second, it is only firms in the upper echelons of the productivity distribution that have seen any growth and
only those in the top 1% or above who have seen rapid growth. And this growth has been rapid, with the
productivity of the top 1% of companies increasing by on average around 6% per year since 2002. These are
the frontier firms. It is clear that, at the same time as the long tail of companies have been stagnating, they
have been sky-rocketing. Or put differently, the distribution of UK firm-level performance has itself been
widening or bifurcating over time.
This pattern of productivities goes some way towards explaining, at least in an arithmetic sense, the large
and widening divergence between regional levels of productivity. High-productivity regions have more of the
frontier firms, and fewer of the long tail, than low-productivity regions. Or, put differently, high-productivity
regions have greater numbers of companies from sectors with high productivity – for example, high-valued
added chemicals companies, like those in Teesside.
Charts 18 and 19 plot the estimated distributions of labour productivity, measured as output per employee, at
the firm level across different regions and different sectors using the ONS ARDx dataset. What is striking is
that differences in productivity within-region and within-sector are far larger than the differences
across-region and across-sector. Every region and every sector has a range of both high and low
productivity firms, with huge overlaps across regions and across sectors.
There are, of course, some regions, such as London, and some industries, such as Information &
Communication and Professional & Scientific services, which have a larger number of frontier firms in the far
right-hand side of the distribution. But the North-East clearly also has its own set of productivity pioneers.
This underscores the value of the Mayfield Commission’s benchmarking work. It also begs the question of
what is preventing the diffusion of processes and technologies used in one firm to other firms operating in a
similar region or sector.
Are there barriers to technological diffusion, legal or competitive, which are stymieing productivity
trickle-down? Are first-mover advantages and economies of scale simply larger for frontier firms these days,
entrenching their advantages? Could more be done by frontier companies to promulgate their practices to
companies lower down the supply-chain? I do not know the answers. But finding them strikes me as key for
unlocking the growth and productivity potential of the long tail of companies and hence of UK PLC.
Conclusion
The UK, perhaps more than any other country in the European Union, is an agglomeration of quite different
micro-economies – micro-economies of high and low incomes, high and low skills, high and low investment,
high and low productivities, Red and Blue Car regions. Closing these gaps would not only boost the size of
the aggregate economic pie, but would lead to a more equitable distribution of its slices.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
16
16
As much as anywhere in the UK, the North-East exemplifies some of those challenges. Incomes and
productivities are towards the bottom of the UK league table and unemployment rates towards the top.
In many ways it could be seen as a Red Car region. With these differences so large, it would be easy to
view the UK glass, economically and socially, as half-empty.
But Teesside also gives grounds for optimism. Productivity and jobs-wise, it is one of the fastest growing in
the country. With support for skills and technology, the like of which is being provided here at the Materials
Processing Institute, that progress can continue and those differences in productivity and unemployment can
be closed. In the North-East, there are good grounds for believing the glass is half-full.
If that could progress here in Teesside could be replicated nationally, the UK’s fortunes really could be
transformed. Those regional micro-economies would converge, their colours would merge, their glasses
would fill. Economically and socially, the prize could scarcely be larger.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
17
17
Annex
Chart 1: Nominal GDP per head (2014)
Source: ONS Regional GVA; Bank calculations.
Chart 2: Average weekly pay (2016)
Source: ONS Annual Survey of Hours and Earnings (ASHE);
Bank calculations. Pay is measured as median gross weekly
earnings of full-time employees.
Chart 3: Median household net wealth (2012/14)
Source: ONS Wealth and Asset Survey; Bank calculations.
Chart 4: Car ownership (2014)
Source: Eurostat (2016); ONS; Bank calculations. The chart
shows the number of registered passenger cars by region
divided by the number of households.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
18
18
Chart 5: Labour productivity (2014)
Source: ONS Regional Labour Productivity; Bank calculations.
.
Chart 6: Research and Development spending
per head (2013)
Source: Eurostat (2016); ONS; Bank calculations.
Chart 7: Percent of population with at least an
undergraduate degree (or equivalent) (2015)
Source: ONS Annual Population Survey; Bank calculations.
Chart 8: Unemployment rates (2016)
Source: ONS Labour Market Statistics; Bank calculations.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
19
19
Chart 9: Youth (16-24) unemployment rates
(2016)
Source: ONS Labour Market Statistics; Bank calculations.
Chart 10: Working age mortality rates, males
(2014)
Source: ONS Mortality Figures for England and Wales, Scotland
and Northern Ireland; Bank calculations.
Chart 11a: Real GDP per head growth
(2008 to 2014)
Source: ONS; Bank calculations.
Chart 11b: Scenario - Real GDP per head growth
(2008 to 2014), without monetary stimulus
Source: ONS; Bank calculations. Notes: See footnote 16.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
20
20
Chart 12a: Change in unemployment rates
(2008 to 2016)
Source: ONS Labour Market Statistics; Bank calculations.
Chart 12b: Scenario – Change in unemployment
(2008 to 2016), without monetary stimulus
Source: ONS Labour Market Statistics; Bank calculations. Notes:
See footnote 16.
Chart 13a: Growth in median net wealth
(2006/8 to 2012/4)
Source: ONS Wealth and Asset Survey; Bank calculations.
Chart 13b: Scenario - Growth in median net
wealth (2006/8 to 2012/4), without monetary
stimulus
Source: ONS Wealth and Asset Survey; Bank calculations.
Notes: See footnote 16.
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
21
21
Chart 15: Growth in average weekly pay
(2008 to 2016)
Source: ONS Annual Survey of Hours and Earnings (ASHE);
Bank calculations. Pay is measured as median gross weekly
earnings of full-time employees.
Chart 16: UK, US and Euro area 5-year, 5-year
forward inflation swaps
Source: Datastream; Bank calculations.
Chart 17: Firm level productivity percentiles
Source: ONS; Bank calculations. Notes: This chart plots various percentiles of the distribution of labour productivity (real GVA per
employee) across Great Britain. This work contains statistical data from ONS which is Crown Copyright. The use of the ONS
statistical data in this work does not imply the endorsement of the ONS in relation to the interpretation or analysis of the statistical
data. This work uses research datasets which may not exactly reproduce National Statistics aggregates.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
2004 2006 2008 2010 2012 2014 2016
UK (RPI)
US (CPI)
Euro area (HICP)
Per cent
0
50
100
150
200
250
300
350
2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014
10th percentile 50th percentile
75th percentile 90th percentile
99th percentile
GVA per head, £000s
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx
22
22
Chart 18: Distribution of firm level labour productivity across regions in 2013/4
Source: ONS; Bank calculations. Please see footnote under Chart 19.
Chart 19: Distribution of firm level labour productivity across industries in 2013/4
Source: ONS; Bank calculations. Notes: This chart plots the kernel density distribution of labour productivity (real GVA per employee)
across Great Britain. This work contains statistical data from ONS which is Crown Copyright. The use of the ONS statistical data in this
work does not imply the endorsement of the ONS in relation to the interpretation or analysis of the statistical data. This work uses
research datasets which may not exactly reproduce National Statistics aggregates.
0
0.005
0.01
0.015
0.02
0.025
-50 0 50 100 150 200
North East North West
Yorkshire & Humberside East Midlands
West Midlands East of England
London South East
South West Wales
Scotland
GVA per employee, £000's
Density
0
0.005
0.01
0.015
0.02
0.025
-40 -20 0 20 40 60 80 100 120 140 160
Manufacturing
Construction
Wholesale and retail trade
Transportation and storage
Information and communication
Professional, scientific and technical
activities
Density
GVA per employee, £000's
All speeches are available online at www.bankofengland.co.uk/publications/Pages/speeches/default.aspx

It is great pleasure to be here at the Acas Annual Conference whose theme this year is the “Future of Work”.
It is a particular pleasure to have Brendan Barber as Chair. As a member of the Bank of England’s Court
between 2003 and 2012, Brendan’s insights on the labour market and wider economy were crucial in helping
the Bank steer a policy course through a period of first macroeconomic calm and then extra-ordinary
macroeconomic storm following the global financial crisis.
It is the labour market that I want to discuss today. We have seen an unusual pattern emerge here over
recent years. Jobs growth has been strong, with over 2 million new jobs created since the end of 2012. But
pay growth has remained weak by historic standards, averaging around 2% annually. This pattern has been
replicated across a number of other advanced economies. As we reach the anniversary of what has become
a “lost decade” for inflation-adjusted pay in the UK, it is a good time to take stock.
What explains this puzzling pattern of rich jobs but poor pay growth? Some of the reasons for weak pay
have been cyclical. The financial crisis caused large job losses. A significant pool of unemployed workers
emerged and job insecurities rose, depressing pay growth. Recently, the UK’s economic recovery has
shrunk the pool of unemployed workers and reduced somewhat job insecurity. Had these cyclical factors
been the only ones at work, we might have expected a stronger recovery in pay.
The reason we have not is because longer-term, structural forces have also been holding back pay growth,
notably weak productivity. Over the medium-term, productivity is the single most important determinant of
the national income pie. Productivity growth pays for pay rises, at individual firms and for the economy as a
whole. Over the past ten years, productivity has barely grown in the UK. That second “lost decade” goes a
long way towards explaining the lost decade in pay.
Productivity is not, however, the only structural factor at work in the labour market. The world of work is
being reshaped in many advanced economies by the secular fall in the degree of unionisation and collective
bargaining, by changes in employment contracts and working patterns and by rises in the degree of
concentration and automation in the company sector. By reducing workers’ “pay power”, they too have
depressed wage growth, actually and prospectively.
I will start by discussing recent pay developments and the factors responsible for driving them, cyclical and
structural. I will then discuss the implications of these cyclical and structural factors for domestic cost
growth, inflationary pressures and hence for monetary policy in the period ahead.
Jobs and Pay
I do not need to tell this audience that the past decade has been a strikingly weak one for pay growth. Ten
years ago, the mean weekly wage in the UK was around £435 per week. A decade on, it has risen to £520
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
per week, an average annual rise of less than 2% in money terms. This makes it the weakest decade for
growth in money wages for British workers since the 1930s.
That is not the end of the story, of course. Over the same period, consumer prices in the UK have risen by,
on average, 2.2% per year. That means inflation-adjusted, or real, pay has fallen by around 3.7%
cumulatively over the past decade. That weekly pay packet in 2008 has, in purchasing power terms, fallen to
be worth only around £420 per week.
This makes the recent period very unusual by historical standards (Chart 1). Since as far back as 1870,
there have been only two episodes when the real pay of workers has fallen over a ten-year period. The
other episodes were associated with seismic shocks in the labour market, often wrenching technological
change or sharp cyclical downturns, which raised levels of unemployment and job insecurity.
The past decade has bucked that historical trend, with a boom in job creation accompanying weak pay
growth. The employment rate in the UK has risen to over 60%, and the unemployment rate has fallen to 4%,
respectively their highest and lowest levels since the mid-1970s. The vast majority of these new jobs,
around 75%, have been full-time. And this boom has persisted, with around 830,000 job vacancies currently
being advertised – the highest since records began.
This picture of weak pay and strong employment has been broadly-based, spanning all regions and sectors.
Real pay has fallen across every region of the UK since 2008 (Figure 1) and in all three major sectors of the
economy (Chart 2). Meanwhile, unemployment has fallen steeply across every region of the UK, by at least
2.5 percentage points.
The high-level picture of the UK labour market, then, is a jobs-rich but pay-poor recovery. This pattern is
broadly replicated in other countries. For example, there is a striking correlation between the pattern of
unemployment and wage growth in the UK and the US (Charts 3 and 4). In the US, unemployment has
fallen to its lowest levels since 1969, while pay growth has remained modest.
Given the strength of jobs growth, the weakness in pay has surprised many people, including the
Bank of England. Chart 5 plots the Bank’s wage forecasts at annual intervals since 2012. Over this period,
there has been a sequence of negative forecast errors, averaging around one percentage point per year.
The same has been true of external wage forecasts, for the UK and elsewhere. These surprises suggest
that the recent pattern in pay and unemployment has been unusual by historical standards.
In the 1950s, A W Phillips uncovered a negative empirical relationship between pay growth and
unemployment: pay grows faster when unemployment is lower. The Phillips curve was born (Phillips
(1958)). This relationship has since become a central pillar of macro-economic theory and policy. Part of
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
the attraction of the Phillips curve lies in the fact that it tells a simple story: the tighter the jobs market, the
greater the pressure on pay.
Exactly 60 years on, the Phillips curve is still widely used, and widely debated, by economists and
policymakers. But with pay undershooting expectations, and with unemployment touching generational lows,
some big questions are being posed of it. Has the Phillips curve died of old age, or is it merely sleeping?
Has it changed shape or location? The juries (or coroners) have yet to reach a definitive verdict.
There have been a number of recent re-examinations of the evidence.
1
 Using different datasets and
techniques, these have tended to conclude that the Phillips curve has a pulse and remains alive and kicking.
As an illustration, Charts 6 and 7 plot the relationship between wage growth and unemployment in the UK
and the US over three sample periods dating back to the 1970s. They also plot some simple regression
lines of best fit.

All of the regression lines are negatively-sloped and, for the UK, statistically significant. Across the three
samples, the estimated Phillips curves are similarly-sloped.2
 These curves have, however, shifted
downwards significantly over time. Taken together, this evidence is consistent with the Phillips curve having
a pulse that is beating at a similar rate to the past, despite the patient having moved hospital.
Explaining the Pay Puzzle
To explore the behaviour of pay in greater detail, we can use a slightly more sophisticated model which takes
account of factors in addition to unemployment. Table 1 provides some econometric estimates of pay
relationships over the period 1992 to 2018. The factors determining pay growth can be grouped under three
headings, each of which is important in explaining its evolution over the (distant and recent) past.
One key factor affecting pay growth is inflation or expectations of inflation.3
 This makes intuitive sense. In
the end, workers are interested in the purchasing power of their pay, not its money amount. They bargain
over real wages. So inflation, or its expectation, affects pay with a coefficient close to one. Put differently, in
the course of wage bargaining, employers insure workers against inflationary shocks, at least on average.
Movements in inflation have been a key driver of pay growth historically. They are the main reason why the
Phillips curve shifted downwards significantly as the Great Inflation of the 1970s gave way to the Great
Moderation of the late 1990s. That period saw inflation expectations ratchet down to target levels where they
have remained anchored since (Chart 8). Pay growth has, in response, fallen pretty much one-for-one. This
explains the first downwards shift in the Phillips curves shown in Charts 6 and 7.

1
 For example, Coibion and Gorodnichenko (2015), Gordon (2013), Vlieghe (2018), Saunders (2017) and Tuckett (2018).
2
 In statistical terms, you cannot reject the null that their slopes are the same.
3
 Both measures have a statistically significant impact on wage growth. The regressions in Table 1 use inflation expectations as they
give a slightly better empirical fit.
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
A second key determinant of wage growth is unemployment or, more precisely, the gap between
unemployment and its longer-run trend (sometimes called the NAIRU or Non-Accelerating Inflation Rate of
Unemployment).
4
 The larger this unemployment gap, the weaker the upward pressures on pay growth, as
A W Phillips had first postulated 60 years ago.
In econometric wage equations, the unemployment gap is a (statistically and economically) significant
determinant of pay (Table 1). A 1 percentage point unemployment gap drags on wage growth by around
0.8 percentage points. That means the 2 percentage point narrowing in the UK unemployment gap since
2012 would be expected to have raised wage growth by 1.6 percentage points, a significant tailwind.
The NAIRU itself is unobservable. It is also moveable, depending on structural features of the labour market.
This means the NAIRU needs to be estimated and these estimates are apt to shift over time. Over recent
years, the Bank has revised down its estimate of the NAIRU in the UK, by just under a percentage point, to
around 4 ¼%. These downwards surprises to the NAIRU (upwards revisions to the unemployment gap) help
account for the persistent overshoot in the Bank’s wage forecasts.
There are a variety of alternative, complementary measures of slack in the jobs market. For example, the
number of hours a person works, relative to their desired hours, is also relevant to gauging the degree of
spare capacity in the labour market. A number of research papers, theoretical and empirical, have found that
such “under-employment” may also have a significant bearing on the setting of pay.
5
There are various measures of under-employment, including the proportion of the workforce working
part-time involuntarily and the gap between actual and desired hours worked. In practice, these two
measures exhibit a very similar pattern (Chart 9). They also track the unemployment rate closely. This
suggests the incremental information contained in measures of under-employment, relative to measures of
unemployment, might be relatively modest.
Econometric estimates confirm that impression. Replacing the unemployment gap with a measure of the
under-employment gap produces a very similar wage equation (Table 1).6
And in a pairwise comparison,
under-employment is statistically insignificant if the unemployment gap is included. Under-employment is
important for pay, but does not appear to much improve the explanation of recent wage dynamics.
A third factor determining pay pressures is productivity. It is through gains in productivity that individual
firms, and the economy at large, can finance increases in workers’ pay. Econometric estimates confirm that
intuition. The productivity of the labour force is a (statistically and economically) significant determinant of
pay (Table 1). On average across firms, a 1 percentage point rise in productivity raises aggregate wage
growth by around 0.8 percentage points.

4
 Espinosa-Vega and Russell (1997) provide a history and review of the NAIRU concept.
5
 For example, Bell & Blanchflower (2018).
6 Specifically, the involuntary part-time share of employment.
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
Productivity growth in the UK has consistently surprised to the downside since the crisis, averaging 0.5% per
year since 2010 or around 1.5 percentage points below its historical average. We would expect weak
productivity in turn to have been reflected in lower pay growth. Consistent with that, the fall in productivity
growth since the crisis largely explains the (second) downward shift in the Phillips curve (Charts 6 and 7).
Since 2012, the Bank’s forecasts have consistently over-estimated productivity growth, on average by just
under 1 percentage point per year (Chart 10). This suggests a sizable fraction of the Bank’s negative
forecast errors for wages might plausibly be explained by downside surprises to productivity, alongside the
effects of a lower than expected NAIRU. Had the Bank known a decade ago how weak UK productivity
growth (and how much lower the NAIRU) was likely to be, its forecast errors for wages would have been
smaller.
We can confirm this by re-running our pay equations substituting in the actual outcomes for inflation, the
unemployment gap and productivity growth, in place of the forecasts for these variables at the time
(Chart 11). The fitted values for wage growth now track actual wage growth reasonably closely. With perfect
foresight about its determinants (productivity, unemployment and the NAIRU), there would have been far
less systematic over-estimation of pay growth during the lost decade.
This suggests the UK’s lost decade for real pay can, to a significant extent, be accounted for by
macro-economic developments: cyclically, the large pool of unemployed that built up immediately after the
crisis, generating a drag from the unemployment gap; and structurally, from lower levels of productivity and
a lower NAIRU than in the past. With those accounted for, there is far less of a pay puzzle left to explain.
The Wage Bargaining Game
A more granular look at the labour market suggests, however, that it is not behaving identically to the past.
Tectonic shifts in the labour market - cultural, contractual, technological, organisational – have come thick
and fast recently. Because this structural change is slow-moving, it is difficult to detect in monthly pay
movements. On a longer fuse, however, empirical evidence suggests these seismic shifts in the jobs market
are influencing pay significantly.
The Phillips curve plots the relationship between the stock of unemployed and wage growth. Another strand
of the labour market literature has looked instead at flows of people into and out of work and the associated
process of job-matching between employers and employees.7
 In these models, pay is determined by flows
of workers between jobs, and by the relative bargaining strength of employees and employers, as well as by
fluctuations in labour demand and supply.

7
 For example, Moscarini and Postel-Vinay (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
Flows of workers and their bargaining power are, in turn, affected by structural features of the labour market.
A rise in job insecurity will reduce flows of workers between jobs and their bargaining strength. A fall in the
degree of unionisation and collective bargaining in the labour market, and a rise in the degree of automation
and concentration in the company sector, will also reduce workers’ pay power. We discuss each in turn.
If these structural forces were reducing workers’ bargaining power, we might expect them to receive a
declining share of the income pie – a fall in the “labour share”. Internationally, there is strong evidence of the
labour share having fallen across a large number of developed countries.8
Structural factors, such as rising
levels of automation and falling rates of unionisation, are typically found to have been an important
contributor to those falls in international labour shares.
9
At face value, the UK appears to have bucked those trends. Chart 12 plots the UK labour share since 1955.
Over the past two decades, it has been broadly flat. The UK labour share in 1990 was 50%, largely
unchanged from today. In the UK, there appear to be fewer signs of any significant shift in the bargaining
power of workers having resulted in them taking away a smaller slice of the income pie. I will discuss some
of the reasons for that below.
But we also need to be cautious when interpreting the labour share. First, looked at over a longer horizon, it
is clear there has been a fall in labour’s share of income in the UK over the past 60 or so years. Since 1955,
the labour share has fallen by around 10 percentage points, falling sharply during the 1970s and 1980s.
Moreover, the level of the UK labour share today, at 52%, is similar to advanced economies as a whole
(around 51%), many of which have experienced falls more recently.
10
Second, looking at the sources of labour income is also important. These comprise not only wages and
salaries, but also defined-benefit pension contributions from companies and social contributions. While
technically-speaking these are all income, they are not always equivalent when it comes to thinking about the
wage bargaining process and spending patterns. The distribution of these different sources of income may
also be uneven across cohorts of workers.
Take defined-benefit pension contributions. For those still in work, these are deferred income that cannot
easily be spent today. With most defined-benefit schemes having closed, this income also only accumulates
to a subset of (mostly older) workers. As these pension contributions have risen recently as a share of total
income, the resulting boost to labour income will have been spread unevenly across generational cohorts
and the increase in the average worker’s true purchasing power may have been exaggerated. If we strip out
back payments made by firms to fill pension deficits, the labour share shows greater signs of falling, if only
gradually, over time (Chart 12).

8
 IMF (2017).
9
 IMF (2017).
10
 IMF (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
A more stripped-down measure of the labour share, which better tracks workers’ purchasing power, is wages
and salaries (Chart 12). That measure has fallen by around 1 percentage point since 1990. This picture is
more striking still if we look at different cohorts. For workers aged over 55, their share of income has
doubled from 4% in the early 1990s to 8% today. For workers aged 16-34, their wage share has fallen from
15% to 12% over the same period. The “lost decade” for young people has been particularly pronounced.
Finally, aggregate movements in the labour share can obscure important pay dynamics at the sectoral level.
Different sectors of the economy are affected to differing degrees by changes in the form of employment,
technology, unionisation and company concentration. As I discuss below, these sectoral data provide a
clearer picture of the significant impact of slow-moving structural factors on workers’ pay power.
(a) Job Market Flows
The quickest way to secure a pay rise is by moving job. That is quite sobering if, like me, you have never
moved job. Pay growth of UK workers moving jobs (“twisters”) has exceeded that of workers remaining in
their position (“stickers”) by 4 percentage points since 1994 (Chart 13). When workers exercise the outside
option of moving between jobs, this puts upward pressure on pay through two main channels.
The direct, or arithmetic, effect arises from moving workers securing a pay rise and boosting the batting
average for overall wage growth. More twisters means higher average pay. The indirect, or behavioural,
effect arises when companies pay higher wages to existing staff, as well as new staff, to encourage them to
stay rather than exercise their outside option. The threat of twisting can also boost pay.
This flow-based model suggests a slightly different specification of the Phillips curve. The flow of workers, as
well as the stock of unemployed, may matter for pay growth. Sure enough, if we replace the unemployment
gap with measures of voluntary job-to-job flows, this provides a (statistically and economically) significant
explanation of wage growth (Table 1). As with under-employment, in a statistical horse-race between the
two measures the unemployment gap does slightly more of the donkey work.
Flows between jobs also provide a useful framework through which to explain recent pay developments.
Chart 14 plots voluntary job-to-job flows in the UK recently. The global financial crisis raised unemployment
and job insecurity. With workers fearful of unemployment – and employers having less need to attract
workers given the large pool of unemployed – voluntary job moves plummeted, falling from around 1% of the
working population pre-crisis to a low of around 0.5% in 2009.
As the economy and jobs market has recovered, and job insecurity has fallen, more people have voluntarily
moved jobs to secure a higher wage. Job-to-job flows today are back to their pre-crisis averages. In this
tightening market, the pay growth of “twisters” has picked up markedly, from a little under 4% in the
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
aftermath of the crisis to over 7% now.11
 Through the direct (batting average) channel, this has in turn
boosted aggregate pay growth.
More surprising, however, is how little impact strong employment has had on those not moving job. Wage
growth for “stickers” has flat-lined at around 2%. Despite a tightening labour market, companies have so far
not felt compelled to pay-up to any significant degree to retain workers. The indirect, or behavioural, channel
for pay pressures has largely been missing during the recent jobs recovery.
This may be a sign that stickers’ pay is behaving differently than in the past. Chart 15 plots two empirical
Phillips curves – one for twisters, the other for stickers. The twisters’ Phillips curve is behaving relatively
similarly to the past, with pay broadly as expected given the tightness in the jobs market. But the Phillips
curve for stickers looks to be somewhat flatter, and the rise in pay somewhat weaker, than might be
expected given labour market tightness.
Why might companies have felt less need to pay-up to retain staff than in the past? Equivalently, why might
the pay power of existing workers have fallen? One possible explanation is recent change in employment
contracts. Although the cyclical degree of job insecurity has fallen as people have become less fearful of
losing their job, there may have been a compensating rise in structural job insecurity as people have become
more uncertain about their hours and income in a job.

There are various dimensions of employment contracts and different metrics of their impact on job security.
One commonly-used measure is the share of the working population self-employed, doing agency work or on
temporary or zero-hours contracts. The share of workers in each category has risen significantly recently.
For some, these shifts in employment contract are welcome as they bring increased job flexibility.
For others, they may be less welcome. The dark side of job flexibility is increased income uncertainty and
job insecurity. It is estimated that more than one in ten of those on part-time contracts would prefer full-time
work and close to half of those on zero-hours contracts would prefer to be on fixed-hours contracts. At the
same time as the quantity of work has risen, for a growing number of workers its quality may have fallen.
This pool of poorer-quality work appears to be fairly deep. For example, around a quarter of those in
temporary work say they could not find a permanent job and over 10% of part-time workers say they could
not find a full-time job.
12
 A Skills and Employment Survey published just last week found around 1.7 million
workers might be suffering from high levels of anxiety about their uncertain hours of work.13

One of the side-effects of structurally-higher job insecurity is a reduced willingness to add to that uncertainty
by moving job. If so, this would mean “twisters” are fewer in number than in the past and that stickers are at

11
 Note that data for pay growth for “stickers” and “twisters” is annual, with latest estimates to 2017.
12
 Taylor (2017).
13
 Skills and Employment Survey (2018).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
an even greater pay disadvantage to twisters as their outside option has lost value. In short, job insecurity
reduces workers’ pay power and weakens upward pressure on pay.
Empirical evidence is consistent with that. Even after controlling for different occupations and locations,
self-employment has been found to result in a wage discount for workers of around 15%.14
 Temporary
contract workers face a wage discount of around 5-6% and agency workers of around 2 ½%.
15
 Zero-hours
contract workers suffer a wage discount of around 7%.
16

A second implication of structurally lower pay power is that there is greater scope for employers to
differentiate between workers in their pay awards. If companies are less fearful of losing talent, they will no
longer feel obliged to pay a “going-rate” across their firm to maintain staff numbers. The flat-lining of stickers
pay, despite the pay recovery for twisters, is evidence of that differentiation.
Wage differentiation can also be seen in advertised salaries. Charts 16 and 17 plot the distribution of wage
rates for two sets of advertised job vacancies – those containing the words “self-employed” and “flexible
hours” and those without. These violin charts suggest that, for more flexible employment contracts, wage
distributions are materially wider. Less secure work results in a greater degree of wage differentiation.
(b) Unionisation and Collective Bargaining
A second set of structural factors potentially affecting the pay power of workers is the degree of unionisation
and collective bargaining in the labour market. Both have fallen secularly over time. Levels of unionisation
have fallen from 50% in the late 1970s to around 22% today (Chart 18). The fraction of workers covered by
collective bargaining arrangements has fallen from over 60% in 1998 to around 40% today (Chart 19).
These trends will have reduced the pressure on companies to increase wages for existing workers
(“stickers”), even when new entrants (“twisters”) are seeing pay rises. As with job insecurity, this will in turn
have tended to lower workers’ pay power and increased the degree of wage differentiation within the
workplace.
Empirical research confirms those findings. Trade union membership has been found to result in a pay
premium for workers. In the UK, this premium is typically found to lie in the range 10-15%, though it may
have fallen over time.17 There is also evidence that falls in rates of unionisation and collective bargaining
have resulted in wages becoming more dispersed and differentiated across occupations and locations.
18

14 Bradley (2016).
15
 Gardiner (2016).
16
 Gardiner (2016).
17 Bryson (2014).
18 Gregg et al (2014).
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
We can quantify these effects in our wage equations. In Table 1, we include the unionisation rate and
estimate over a longer sample period (1892 to 2015) to reflect the lower-frequency movements in
unionisation. This suggests unionisation is positive for pay growth and (statistically and economically)
significant. Over the sample, a 10 percentage point rise in the degree of unionisation raises wage growth by
around 25 basis points per year.
Over recent decades, unionisation rates have fallen by 30 percentage points. Using the long-run estimates,
that will have lowered wage growth by around 0.75 percentage points per year over the past 30 years, a
significant effect. Consistent with it, the sharpest falls in unionisation came in the 1970s and 1980s,
coinciding with falls in the UK labour share. Rolling regressions suggest the effects of unionisation on pay
were largest during this period.
If we look at the sectoral level, similar effects are seen. Sectors of the UK economy with higher levels of
unionisation have seen smaller falls in their labour shares (Chart 20). Over the past few decades, a sector
like administration and support activities with under 10% unionisation rates has seen its labour share decline,
while a sector like education with a unionisation rate close to 50% has seen its labour share rise.
(c) Automation
A third factor potentially affecting the pay power of workers is the degree of automation in the workplace.
The easier and cheaper it is to replace human with machine, the lower is likely to be the wage bargaining
power of workers relative to companies. Increased automation and the dawn of a Fourth Industrial
Revolution could, then, result in slower pay growth and workers receiving a smaller slice of the income pie.19
Recent research has confirmed that intuition. The IMF has found a significantly negative effect of automation
on labour’s share.
20
 Acemoglu and Restrepo (2018) look at the influence of multi-purpose industrial robots
on pay growth across different sectors of the US economy. They find that one extra robot per thousand
workers lowers wages by 0.25-0.5%.
At 71 robots per 10,000 employees, the UK’s degree of robot automation lies well below France (127), Spain
(150), the US (170) and Germany (301) (Chart 21).
21
 This might help explain why the UK’s labour share has
fallen less markedly than in these other countries. Using the Acemoglu and Restrepo ready-reckoner, if the
UK had the same degree of industrial robot penetration as the US spread over a decade, this would lower
wage growth by around 0.4 percentage points per year.
22


19
 For example, Acemoglu and Restrepo (2018).
20
 IMF (2017).
21 These automation measures take no account of differences in industry structure across countries.
22
 We use 0.4 as the approximate mid-point of the Acemoglu and Restrepo range.
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
What is true across countries appears also to be true across sectors. Chart 22 plots a measure of
automation across sectors against changes in the labour share. It suggests a downward-sloping
relationship, consistent with increased automation depressing pay.

(d) Monopsony
A final factor influencing the pay power of workers is the degree of concentration in the corporate sector.
There has been much recent discussion of the rise of so-called “superstar” firms, benefitting from global
network economies of scale and scope.23
 The larger a company’s share of the product (monopoly) or labour
(monopsony) market, the greater its degree of power over customers or workers.
24
Measures of industry concentration in the US (and in some other countries) have trended upwards, secularly
and significantly, over several decades. This is consistent with superstar firms gaining a greater degree of
monopolistic power over customers.25
 There is also evidence linking superstar firms to rising degrees of
concentration in the labour market, consistent with a greater degree of monopsonistic power over workers.26

These shifts in industry concentration appear to have influenced pay growth. Rising concentration across
different sectors of the US economy has been found to have had a significantly negative effect on their
labour share.27
 Monopsony power among US industries has been found to have had a negative effect on
wage growth in that sector, the more so when unionisation rates are low.28
 And larger firms have also been
found to differentiate to a greater degree in the wages they pay their workers.
29

Colleagues at the Bank, Will Abel, Silvana Tenreyro and Greg Thwaites, have recently found similar effects
in the UK.30
 Higher industry concentration is associated with lower pay, the more so the less of a sector is
covered by collective bargaining. One important difference from the US is, however, that labour market
concentration has not increased anything like as much in the UK as in the US. This, too, might help explain
the relative flatness of the UK’s labour share.

23 For example, Autor et al (2017a).
24 Concentration in the labour market is only one means by which an employer might enjoy greater bargaining power over workers. As
Krueger (2018) discusses, there are other types of “implicit contract” which might dilute workers’ bargaining power. See also Manning
(2003).
25
 Autor et al (2017b).
26
 Autor et al (2017b).
27
 Autor et al (2017b).
28
 Benmelech, Bergman and Kim (2018).
29
 Mueller, Ouimet and Simintzi (2017).
30
 Abel, Tenreyro and Thwaites (forthcoming).
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
Implications for Monetary Policy
Let me conclude with some reflections on current and prospective pay developments. A year ago, prospects
for pay were cloudy. Average weekly earnings growth – excluding the volatile bonus component – was
running at a 2% annual rate, in line with its growth over the preceding three years. Average pay settlements
were running at just over 2%. Signs of a pay pick-up had, to that point, proved to be false dawns.
A year on, I think there is more compelling evidence of a new dawn breaking for pay growth, albeit with the
light filtering through only slowly. Average weekly earnings growth (excluding bonuses) has risen to 2.9% –
a clear rise and a little stronger than the MPC had expected at the time of its Inflation Report in August. Pay
settlements have risen to 2.8% in a similar pattern (Chart 23). Measures of labour market tightness have
increased to their highest levels since before the crisis and, in some cases, ever.
Looking beneath the headline figures, evidence of an up-tick in pay is clearer still. Private sector pay growth
(again excluding bonuses) has been grinding through the gears; it recently hit the psychologically-important
3% barrier. Private sector wage settlements so far this year are running at 2.8% and in some sectors, such
as construction and IT, are running well in excess of 3%.
The public sector has, for a number of years, been held to a 1% pay cap. This had the effect of suppressing
whole-economy pay growth, both directly (given the public sector comprises around 18% of total
employment) and potentially indirectly (due to the possible spill-over effects of public pay awards on the
private sector). That public sector pay cap has now been lifted and decisively so.
Earlier this year, over 1 million NHS workers received a one-year pay settlement of at least 3%. Some
workers have received double-digit pay rises. They have been joined recently by a further 1 million workers
from the armed forces, police and teachers, who have secured pay rises of between 2-3.5%. The largest
segment of public sector workers – including those in Local Authorities – have yet to settle. But with the cap
breached, it seems possible their pay settlements could also ratchet upwards.
We can see that upward ratchet in pay in the distribution of pay settlements (Chart 24). According to the
Bank’s comprehensive wage settlements database, so far this year around 65% of deals have been struck at
rates above their level the previous year. The largest contributor to this rightwards shift in the settlements
distribution has been the public sector.
This evidence suggests the pulse of the Phillips curve has quickened as the labour market has tightened.
Unlike over much of the past decade, estimated wage equations are now broadly tracking pay (Chart 5). A
year ago, the Bank’s forecasts of average weekly earnings growth into mid-2018 was 2.5%. Wage growth in
the year to 2018 Q2 was 2.4%.
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
How are these pay developments affecting the monetary policy outlook? Inflation is currently above the
Bank’s 2% target. That is entirely due to the effects of past external price shocks, notably sterling’s
post-referendum depreciation. As these external effects fade, inflation is projected to be maintained at or
slightly above its target by a slow build in domestic cost pressures, underpinned by a rise in wage growth.
The rise in wages projected by the Bank is, to coin a phrase, limited and gradual. Private sector pay is
assumed to rise from 3% currently to around 3 ¾% three years hence, or around 25 basis points per year.
That is below the pace at which wage growth has built over the past few years. Over the same period,
unemployment is projected to fall further below its natural rate, though again at a slower pace than over
recent years.
The path of domestic costs in the economy depends on developments in firms’ productivity, as well as in
pay. Weak productivity means even low nominal pay increases can be inflationary. With growth in money
wages running at close to 3%, and with productivity growth running closer to zero, domestic cost growth in
the UK is already running at, if not slightly above, rates consistent with the inflation target, even before any
further (limited and gradual) build in wage pressures.
A limited and gradual build in domestic cost pressures is one important factor underpinning the limited and
gradual pace of further interest rates rises expected by financial markets and communicated by the MPC. At
present, financial markets expect rate rises of around 25 basis points per year over the next three years.
That is not dissimilar to the projected build in wage growth over the period.
There are two-sided risks around this forecast, reflecting both cyclical and structural factors. On the cyclical
side, there remains a significant degree of uncertainty around the amount of slack in the economy. It is
possible the NAIRU could be below 4 ¼% or that under-employment is greater than assumed. Each ½
percentage point of extra slack lowers the projected level of wages at the policy horizon by around 1%.
While wage growth was undershooting, it made sense for the Bank to lower its estimates of the NAIRU. But
with wage growth now tracking, and with wage forecasts no longer under-shooting, it is no longer clear risks
to the NAIRU, and hence wages, are skewed to the downside. With wage growth picking up for the first time
in a lost decade, the risks to domestic costs are now broadly-balanced, though still significant.
The risks to pay from structural factors are probably larger, though operate with a longer lag. Take
unionisation. Its downward trend historically has suppressed pay growth. If this trajectory were to continue,
the fraction of the workforce unionised would fall by a further 16% percentage points by 2030. According to
our estimates, that could suppress wage growth by over ¼ percentage point each year.
The same is true of trends in automation. To date, UK levels of automation have lagged international
competitors. Were the UK to catch up, this could have significant implications for wage growth. If the UK in
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
a decade was to reach levels of robot use in, say, Germany, then this could lower wage growth by around a
percentage point each year. If the dawn of the Fourth Industrial Revolution resulted in robots and
automation becoming much cheaper, easier and more ubiquitous, the impact on pay could be larger still.
Finally on company concentration, as with automation there has so far been only a modest rise in the degree
of monopsony power among UK companies compared to the US. If the rise of superstar firms were to reach
US proportions, however, the impact on pay could be larger, if not necessarily large. If the UK reached US
levels of monopsony power over a decade, this could reduce pay growth by around 5 basis points per year.
Conclusion
Pay prospects are determined in part by employment, in part by productivity and in part by power. All three
have been important in explaining the weakness of pay during its recent “lost decade”. And each is likely to
continue to be important for pay in the period ahead.
Monetary policy can support the first piece of the pay jigsaw – employment. It has done so significantly over
recent years, perhaps to the tune of creating 1 ½ million new jobs. But it can do little over the medium term
to support productivity or to reshape the structural forces influencing pay power, the second and third pieces
of the pay jigsaw.
Doing so relies on policies which shift the supply side of the economy. Longer-term, it is through improved
skills, productivity and structural reform and regeneration that pay power for workers can be restored. This
underlines the importance of the Government’s Industrial Strategy, whose objectives are to do just that.
In its White Paper on Industrial Strategy published last year, the Government proposed setting up an
Industrial Strategy Council.31
 This independent body of experts was designed to monitor, and report
publically, on progress towards meeting the objectives of the industrial strategy.
Earlier this week, I was delighted to be appointed, in a personal capacity, to Chair the Industrial Strategy
Council. I believe it can play an important role in supporting the Government’s industrial strategy, which
seeks to increase living standards across the UK, help industry to thrive and boost workers’ pay power

By common assent, economists do not agree on much. I have lost count of the number of jokes about
economists whose punchline ends “and they still couldn’t reach a conclusion”. That is why Harry S Truman,
when President of the United States, famously yearned for a one-handed economist. Whether or not this
critique is fair, the issue I will discuss tonight is one on which economists do agree: productivity matters.
At this point, it is customary to wheel out the following, now rather over-used, Paul Krugman quote:
“productivity isn’t everything, but in the long run it is almost everything.” Despite its over-use, this quote does
have one important virtue, something not to be taken lightly in this post-fact, post-truth world: it is empirically
verifiable and appears to be factually accurate. Let me illustrate that with a simple example.
Since 1850 UK living standards, as measured by GDP per head, have risen roughly 20-fold, a huge gain.
How much of that gain can be attributed to higher productivity? Well, if productivity had flat-lined over the
period, UK living standards would only have only doubled. Or, put differently, in the absence of productivity
growth, UK living standards would be an order of magnitude lower today, stuck at late-Victorian levels.
A more refined way of reaching the same conclusion is to decompose growth into the contribution from
inputs into the production process – labour and capital – and the contribution from improvements in the
efficiency with which these inputs are used – so-called Total Factor Productivity (TFP). Chart 1 does that for
the UK since the Industrial Revolution. This suggests movements in TFP have accounted for the lion’s share
of both the growth and variation in living standards since at least the mid-18th century.
Saying that productivity matters is not the same as saying we understand its determinants. The past few
years have served to underscore just how partial economists’ understanding of productivity remains. As one
illustration, Chart 2 plots successive forecasts by the Bank of England of UK productivity growth since 2008,
while Chart 3 shows IMF forecasts of productivity across advanced economies over the same period.
These charts point to a string of material and serially-correlated forecast errors over recent years.
Productivity growth has consistently underperformed relative to expectations, since at least the global
financial crisis. This tale of productivity disappointment, in forecasting and in performance, has been
extensively debated and analysed over recent years. Some have called it the “productivity puzzle”.1

With each year that passes, and as each new turning point in productivity has failed to materialise, this
mystery has deepened. This has led some to conjecture that the world may have entered a new epoch of
sub-par productivity growth, an era of secular stagnation.
2
 Various possible causes of this stagnation have
been posited, including adverse demographic trends and diminished rates of innovation.3
 The secular
stagnation hypothesis is striking in its gloomy implications for future growth in living standards.

1
 For example, Barnett et al (2014), Bryson et al (2014) and Weale (2014)
2
 For example, Gordon (2014) and Summers (2013).
3
 Gordon (2014).
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
It is striking, too, because of the starkness with which it contrasts with a second topical hypothesis. This
posits that we may be on the cusp of a Second Machine Age or Fourth Industrial Revolution, an era of
secular innovation.
4
 This might arise from the rise of the robots, artificial intelligence, Big Data, the Internet
of Things and the like. Because of its impact on future living standards, the winner of this secular struggle –
stagnation versus innovation – carries enormous societal implications.
A second issue, every bit as topical and important, concerns the distribution of gains in living standards.
Specifically, there has been mounting concern over a number of years about rising levels of within-country
inequality across a number of countries.5
 Candidate explanations for this widening dispersion in fortunes
include failings in the educational system, the impact of technology and globalisation, anti-competitive
practices and weaknesses in corporate governance.6
Both of these great debates – innovation versus stagnation, the widening dispersion of income and
opportunity across society – have a causative link to productivity. A household’s income depends largely on
wages and these wages depend, in turn, on the productivity of the company at which households work. So
falling productivity growth among firms, and widening dispersion of productivity across firms, could in theory
have contributed to stagnant household incomes and widening dispersion in these incomes.
To be clear up front, I do not have a simple explanation, much less an oven-ready solution, for these
productivity puzzles. My aims here are more modest – to draw together some of the available data on
productivity to shed light on the puzzles. Specifically, I will look at historical data for the UK, at international
data from a cross-section of countries and at sectoral and firm-specific data for UK companies.
This empirical evidence suggests a long tail of countries and companies with low, slow productivity growth.
These productivity laggards have been unable to keep-up, much less catch-up, with frontier countries and
companies.7
 At the same time, an upper tail of companies and countries has maintained high and rising
levels of productivity. These productivity leaders are pulling ever-further away from the lower tail. Or, put
differently, rates of technological diffusion from leaders to laggards have slowed, and perhaps even stalled,
recently.
This empirical pattern sheds light on the two great macro-economic debates. It helps explain why we might
see the co-existence of secular innovation (among leaders) and stagnation (among laggards). It helps
account for the fall in productivity growth rates – namely, slower rates of diffusion of new innovation to the
long lower tail of companies. And it helps explain the widening dispersion in households’ incomes, as the
mirror-image of widening productivity differences across firms.

4
 For example, Brynjolfsson and McAfee (2011, 2014).
5
 For example, Piketty (2014), Milanovic (2016).
6
 Stiglitz (2016).
7
 This has been a key theme of recent work by the OECD, for example Andrews et al (2015).
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
This empirical prognosis also helps when devising policy measures to tackle these productivity problems.
Even marginal improvements to productivity among the long tail of low-productivity companies – or,
equivalently, a speeding up of rates of technological diffusion to these companies – could make significant
inroads into the productivity puzzle. I will discuss possible policies to do so at the end.
Explaining the Productivity Puzzle
Before diving into the data, it is helpful to set out some of the candidate explanations that have been used to
explain the productivity puzzle, in the UK and internationally. These hypotheses include:
(a) Mismeasurement
There is a fairly widespread perception that the productivity puzzle may, at least in part, be no more than a
statistical mirage. It certainly seems likely that official statistics underestimate economic activity to some,
perhaps significant, degree and with it potential productivity gains.8
 For example, a recent review concluded
that productivity growth in the UK might be under-estimated by around 0.5 percentage points per year, as a
result of the failure fully to capture elements of the digital economy.9
That said, most studies have also found that mismeasurement alone is unlikely to account for the majority of
the productivity puzzle, whether in the UK or internationally.10 Many of the mismeasurement problems
already existed long before productivity started slowing. These problems would need to have increased
dramatically – and probably unrealistically – to explain fully the productivity slowdown. Consistent with that,
the slowdown in productivity appears to be largely unrelated to the penetration of information technologies
across sectors and countries.11

(b) Crisis-related scarring
There is plenty of evidence to suggest that financial crises can have a permanent, or certainly persistent,
scarring effect on output and productivity in economies.12
 This time’s crisis, the largest in at least a
generation, is unlikely to buck that historical trend. There are several channels through which financial crises
might permanently damage corporate sector productivity.
First, a collapse in credit availability is likely to constrict the financing of both new and existing companies
and hence constrain their investment plans. It may hit particularly hard young companies, without access to
alternative sources of finance, for whom productivity growth is often fastest. Empirical evidence from the

8
 For example, Feldstein (2016), Baily and Montalbano (2016).
9
 Bean (2016) discusses the potential mechanisms for the underestimation of GDP.
10 Syverson (2016), Byrne et al (2016).
11 Syverson (2016).
12 For example, Reinhart and Rogoff (2009), Oulton and Sebastiá-Barriel (2013).
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
crisis suggests these channels were potent, in the UK and internationally.13
 As credit conditions have eased
recently, however, this has become a less compelling explanation for persisting productivity problems.
A second financial channel through which the crisis may have affected firms’ investment is asset prices.
Specifically, many small firms are reliant on property collateral to back their loans. A fall in property prices
tightens their collateral constraint and credit conditions. Recent Bank of England research has suggested
this may have been a potent crisis propagation channel for smaller businesses during the crisis.14
 As asset
prices have recovered, however, this channel cannot explain the persisting productivity puzzle.
A third channel through which the crisis might have slowed productivity is by hindering resource reallocation
between firms and across sectors.15
 Flows of capital and labour between companies are one of the key
channels through which technology and ideas are diffused.16
 Since the crisis, rates of labour market churn
between companies have been low and the dispersion in rates of return across sectors has been high.17

Both are consistent with lower rates of factor reallocation having contributed to low productivity.
(c) Forbearance and monetary policy
Some have contended that productivity may have been held back by the actions of the authorities, in
particular regulatory forbearance and accommodative monetary policies. By supporting low-productivity
companies who would otherwise have failed, policy actions may have prevented the “creative destruction” of
firms.18
 Certainly, the level of company liquidations and firm exits has remained low in many countries since
the financial crisis, probably lower than would have been expected given the path of GDP.
A number of studies have considered the evidence on the impact of regulatory and monetary policies on firm
death and productivity.19
 For instance, Arrowsmith et al (2013) examine the prevalence of bank forbearance
across the small to medium-sized enterprises (SME) sector, finding the effect to be small. The same study
finds a potentially larger impact of monetary policy on company failure and productivity.20
 That said, it is
important to note that employment was also higher as a result of looser monetary policy, with potentially
smaller crisis-related scarring effects on labour markets.

13
 Chodorow-Reich (2014), Greenstone and Mas (2012) and Edgerton (2012) for the US, Amiti and Weinstein (2013) for Japan,
Paravisini et al (2015) for Peru, Bentotila et al (2013) for Spain and Franklin et al (2015) for the UK.
14
 Bahaj et al (2016).
15
 Riley et al (2015), Barnett et al (2014).
16
 For example, Newman et al (2015), Hoekman et al (2004).
17
 Barnett et al (2014), Broadbent (2012), Hseih and Klenow (2008), Calligaris et al (2016).
18
 For example, McGowan et al (2017) and Pessoa and Van Reenen (2014).
19 Cette et al (2016), Reis (2013), Gopinath et al (2015) and Gorton-Ordones (2015).
20
 See also, Broadbent (2012).
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
(d) Slowing Innovation
Some economists believe that the type of technological progress behind productivity growth over the past
two centuries may not continue at the same pace in the future.21
 One argument is that the current wave of
innovation, grounded in ICT, does not have the same potential as past innovations.22
 A second is that the
ICT revolution is already quite mature and that future progress is likely to be slower.23
 A third is that, with
world population expected to peak this century, so too might the pace of innovation.24
These arguments are contentious and have been the subject of lively debate.25
 Some have argued that the
ICT revolution has already had a greater impact on productivity than the steam engine.26
 Others have
argued that the ICT revolution is still in its infancy and has vast potential for further disruptive innovation.27

And a third contends that there are many emerging technologies with the potential to revolutionise the
economy, such as robotics, artificial intelligence, Big Data and the human genome.28
(e) Diffusion Dynamics
An alternative way of accounting for slower productivity growth is that it arises, not from slower rates of
innovation, but from slower rates of diffusion of innovation to other companies and countries.29
 For example,
recent empirical work by the OECD has highlighted the possibility of the technological diffusion engine
having slowed, or even stalled, over the most recent period, with a widening in dispersion between the
fortunes of those companies operating at the technological frontier and those operating inside this frontier.30

There are a number of possible explanations for such a phenomenon. Stifled competition in certain sectors
and for certain products may have prevented the trickle-down of innovation.31
 For example, restrictions on
patents and intellectual property (IP) might restrict new entrants and retard replication. A related hypothesis
is that, in today’s globalised markets, network economies of scale and scope are more potent, generating
natural monopolies in which single or small sets of players dominate market share.32
A third hypothesis is that the emergence of a long tail of non-frontier companies, failing to keep pace with
innovation, is the result of management failings. For example, Nicholas Bloom and John Van Reenen have
shown that weaknesses in management processes and practices go a long way towards explaining the long

21 Gordon (2012) and Cowen (2011).
22 Gordon (2012).
23 Decker et al (2016) argue there has been a fall in entrepreneurship in high tech sectors over recent decades.
24 Gordon (2014). Jones and Romer (2010).
25 See Tuuli and Batten (2015) for a summary of this debate.
26 Craft (2004).
27 Brynjolfsson and McAfee (2011).
28 Mokyr (2014a and 2014b) and Baily et al (2013).
29 Comin and Hobijn (2010).
30
 Andrews et al (2015).
31
 Andrews et al (2015).
32
 For example, Haskel and Westlake (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
tail of low productivity manufacturing companies.33
 These poor practices are most pronounced in sectors
where competition is weak and in family-owned firms where management control rests with the eldest son.
The data presented below do not, in the main, allow a direct empirical test of these competing hypotheses.
Many of these hypotheses are, in any case, complementary explanations. The data do, however, lend
greater weight to some, and less to other, of these explanations when accounting for the productivity puzzle.
Historical Productivity Trends
Let me start by placing the UK’s recent productivity performance in a broader historical context. That is
useful for determining just how unusual the recent period of sub-par productivity performance really is.
Since the Industrial Revolution, GDP per capita in the UK has risen almost without interruption, at an annual
average rate of 1.2% per year. In lockstep, there has been a near-monotonic rise in UK productivity. UK
TFP growth since 1750 has averaged 0.8% per year. Since the Industrial Revolution, GDP per capita has
doubled roughly every 65 years and productivity roughly every 85 years.
The implications for living standards have been profound. Each generation has, since the Industrial
Revolution, been around 25% better off than their parents. This is a story of secular improvements in living
standards. It is also a story of secular innovation, with the two typically going hand in hand. At least over
recent centuries, secular stagnation in living standards and innovation has simply not fit the historical facts.
This is not, however, the whole story. For a longer-term perspective, Chart 4 shows an estimate of UK TFP
back to 1 AD, alongside GDP per head. The measurement problems in GDP and productivity were probably
more acute then than now. Nonetheless, the “hockey-stick” profiles of both GDP and TFP are striking, both
in their own right and in their similarity. Over the long haul, GDP and TFP have moved in lockstep.
For many centuries prior to the Industrial Revolution, productivity and GDP growth in the UK averaged only
around 0.01% per year. Each generation was only around 0.3% better off than their parents – a scarcely
noticeable improvement. Secular stagnation in living standards and innovation persisted for many centuries.
History suggests both secular innovation and secular stagnation have, at different times, been the norm.
Although productivity growth has picked up since the Industrial Revolution, it has still been subject to
significant twists and turns. Chart 5 plots UK labour market productivity since 1760, decomposed into the
contribution from additional capital per worker (“capital deepening”) and TFP. Both have fluctuated
significantly and have made substantial contributions to UK productivity growth over time.

33
 Bloom and Van Reenen (2007).
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
Chart 6 looks more closely at the historical evolution of UK TFP growth. Trends in TFP have been far from
constant, with decadal spans when productivity has averaged little more than zero and others where it has
averaged close to 2%. The latter episodes have typically been associated with periods of rapid technological
change, as after the three industrial revolutions of the mid-18th, mid-19th and mid-20th centuries.
In each case, there was a significant lag between the arrival of new technologies and their impact on
productivity. Rates of technological diffusion were relatively slow and sedate. This is typically felt to have
reflected the time it takes for new technologies to find widespread use and become General Purpose
Technologies (GPTs).34
 After the first Industrial Revolution, it took around 70-100 years for technological
diffusion to occur, GPTs to emerge and TFP and living standards to rise.35
Subsequent industrial revolutions have also seen a lag between the emergence of new technologies and
their impact on productivity. But there is evidence this diffusion lag may have shortened. Comin and
Mestieri (2014) estimate the diffusion lag during the industrial revolutions of the late 19th and 20th centuries to
have been 20-30 years, roughly a third of that during the first industrial revolution. This may have reflected
increased flows of labour, capital, ideas and information between firms, regions, sectors and countries.
Table 1 looks at average rates of UK TFP growth over the past 2000 years. It tells a tale of long waves of
productivity stagnation and innovation. The historical data do not reject either hypothesis. Table 1 also puts
into historical perspective the UK’s recent productivity slowdown. For the past decade, average productivity
growth has been negative. This is unusual, if not unique, historically. You would have to go right back to the
18th century to see a similarly lengthy period of stagnant productivity.
Global Productivity Trends
If UK productivity patterns are unusual relative to the past, how unusual are they relative to other countries?
To assess that, I draw on a panel of around 116 countries, advanced and emerging, going back to 1950,
using data from the Penn World Tables.36
 Chart 7 plots average TFP growth across the whole sample of
countries, while Chart 8 does so separately for advanced and emerging economies. Several interesting
features emerge.
First, the slowdown of productivity growth has clearly been a global phenomenon, not a UK-specific one.
From 1950 to 1970, median global productivity growth averaged 1.9% per year. Since 1980, it has averaged
0.3% per year. Whatever is driving the productivity puzzle, it has global rather than local roots.

34
 Helpman et al (1998).
35
 Comin and Mestieri (2014).
36
 Feenstra et al (2015).
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
Second, this global productivity slowdown is clearly not a recent phenomenon. It appears to have started in
many advanced countries in the 1970s.37
 Certainly, the productivity puzzle is not something which has
emerged since the global financial crisis, though it seems the crisis has amplified pre-existing trends.
Explanations for the productivity puzzle based on crisis-related scarring are likely to be, at best, partial.
Third, the productivity slowdown has been experienced by both advanced and emerging economies. The
slowdown in median productivity growth after the 1970s among both advanced and emerging market
economies is around 1¾ percentage points (Chart 8). Indeed, looking at country-specific trends, it is striking
just how generalised the productivity slowdown has been (Chart 9).
Fourth, one way of understanding this global productivity slowdown comes from decomposing it into changes
in rates of innovation among countries operating at the productivity frontier and changes in rates of diffusion
from frontier to non-frontier countries. Growth theory would predict that, over time, technological diffusion
should lead to catch-up between frontier and non-frontier countries.38
 And the greater the distance to the
frontier, the faster these rates of catch-up are likely to be.
So what explains the 1¾ percentage point slowdown in global productivity growth since the 1970s – slower
innovation at the frontier or slower diffusion to the periphery? If the frontier country is taken to be the United
States, then slowing innovation can only account for a small fraction of the global slowing, not least because
the US only has about a 20% weight in world GDP. In other words, the lion’s share of the slowing in global
productivity is the result of slower diffusion of innovation from frontier to non-frontier countries.
To illustrate that, Chart 10 plots the distribution of levels of productivity across countries over a set of sample
periods, where productivity is measured relative to a frontier country (the United States) indexed to one.
Comparing the distributions in the 1950s and 1970s, there is a clear rightward shift. Cross-country
productivity convergence or catch-up was underway, as the Classical growth model would suggest.
In recent decades, however, that pattern has changed. Comparing the 1970s with the 1990s, there is a
small leftward shift in the probability mass. And in the period since the global financial crisis, there has been
a further leftward shift in the distribution and a widening of its range. Today, non-frontier countries are about
as far from the technological frontier as they were in the 1950s.
Another way of illustrating the same point is by looking at differences in productivity between frontier and
periphery countries over time (Chart 11). In the decades immediately following the Second World War, the
Classical growth model was operating effectively, with catch-up in productivity underway and at faster rates
among emerging market countries with furthest to travel to the frontier.

37
 Gordon (2014).
38
 Barro and Sala-i-Martin (1992) and Rodrik (2011).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
Since the 1980s, however, that convergence process appears to have stalled, perhaps even to have gone
into reverse, in both advanced and emerging economies. The gap to the frontier has failed to close and, in
particular for emerging market countries, has widened. Classical growth dynamics appear for some reason
to have stopped operating, with the gap to the frontier widening the more so the greater the distance to it.
It is possible to quantify this slowing in convergence using a similar approach to that used to measure output
convergence in the cross-country growth literature.39
 Specifically, a country’s productivity growth can be
linked by a convergence parameter to its productivity gap to the frontier country. This parameter can be
estimated econometrically and tracked over time.40

Chart 12 shows a time-series of this convergence coefficient estimated using a rolling regression. In the
1980s, this coefficient averaged 0.8. That is to say, every 10 per cent productivity gap relative to the frontier
country resulted, on average, in productivity growth being 0.8 percentage points higher than in the frontier
country. By 2000, that convergence coefficient had roughly halved. By the end of the sample, the coefficient
is statistically insignificant from zero. Convergence had not just slowed but stalled.
One of the key determinants of international technology transfer has been found to be cross-border flows of
goods and services, people and money and capital.41
 While they have waxed and waned historically, all of
these have tended to rise rapidly since the middle of the 20th century. Other things equal, that would have
been expected to increase the speed of diffusion of innovation across countries over that period. In practice,
the opposite appears to have occurred.
Taken at face value, these patterns are both striking and puzzling. Not only do they sit oddly with Classical
growth theory. They are also at odds with the evidence of history, which has been that rates of technological
diffusion have been rising rather than falling over time, and with secular trends in international flows of
factors of production. At the very time we would have expected it to be firing on all cylinders, the
technological diffusion engine globally has been misfiring. This adds to the productivity puzzle.
Sectoral Productivity Trends
Some further insight into these puzzles comes from looking at the productivity data in more granular detail.42

We start by considering sectoral patterns of productivity among UK companies.
Sectoral shifts in the economy could plausibly account for some of the fall in productivity growth. There has
been a secular shift over time away from manufacturing and towards services, with the employment share in

39 Rodrik (2011).
40 Based on the following equation: 𝑃𝑖𝑡 − 𝑃𝑓𝑡 = 𝑎𝑖 + 𝑏𝑖 ∗ 𝐺𝐴𝑃𝑖
 where 𝑃𝑖𝑡 is growth in TFP for country i in year t, Pft is the growth in TFP
for the frontier country (US) in year t, GAPi
 is the gap between the level of TFP in country i and the US as a percentage, ai
is a constant
capturing the average pace of convergence and bi captures how much of the speed of convergence depends on the TFP gap.
41 For example, Hoekman et al (2004), Stoyanov and Zubanov (2012) and Newman et al (2015).
42 See McCafferty (2014) for a discussion of productivity from a sectoral perspective.
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
manufacturing having fallen from 17% to 7% since 1990. Because productivity growth in manufacturing is
higher than in services, this shift could plausibly account for some of the fall in aggregate productivity growth.
Even if we correct for this compositional effect, however, the slowdown in UK productivity growth remains.
Chart 13 looks at the evolution of different UK sectors’ productivity, while Table 2 looks at the contributions of
various sectors to UK productivity growth in the decades either side of the global financial crisis. Pre-crisis,
the strongest rises in measured productivity were in manufacturing, information and communication and
financial services, all of which experienced annual growth of over 4%. But growth was not confined to these
sectors, with virtually all sectors making a positive contribution to productivity growth.
In the period since the crisis, those pre-crisis trends have changed dramatically. Virtually every sector has
seen productivity flat-line. The contributions of all sectors to productivity growth have fallen and most are
tightly bunched around zero (Table 2).43
 Whatever has caused UK productivity growth to fall, it has done so
on a generalised, cross-sector basis.
It is sometimes asserted that, without the collapse in financial services output associated with the financial
crisis, the UK’s productivity performance would have held up. It is certainly true that financial sector
productivity was probably over-stated in the run-up to the crisis.44
 Nonetheless, the subsequent sharp fall in
financial services productivity is plainly not the whole story. Of the 1.7 percentage point fall in the UK’s
productivity growth since 2008, less than a third can be accounted for by financial services (Table 2).
One sectoral story that could account for this generalised slowing is a reduced rate of factor reallocation
between sectors.45
 Consistent with that hypothesis, the dispersion of productivity across industries has
increased significantly over the last 40 years. And the variance in sectoral productivity gaps, relative to
pre-crisis trends, also increased sharply after the crisis (Chart 14).
Nonetheless, this pickup in the dispersion of productivity across sectors is dwarfed by the increase in
productivity dispersion within sectors (Chart 15). This suggests that any obstacles to the movement of
resources within the economy have been more important within sectors than across them. With that in mind,
it is firm-by-firm data to which we now turn.
Firm-level productivity trends
A more granular lens still comes from looking at firm-by-firm productivity data. The OECD has recently
undertaken extensive analysis using company-level data to assess global productivity patterns.46
 We draw
on two datasets for the UK that can prospectively shed light on firm-level trends.

43 See Billet and Schneider (2016) for an interactive tool for examining sectoral productivity trends.
44 Haldane et al (2010).
45 Bank of England (2016), Barnett et al (2014).
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
The first is based on the UK Office for National Statistics (ONS) Annual Business Survey and employment
surveys.47
 The ONS combines these surveys in the form of the Annual Respondent Database (ARDx). The
sample covers around 40,000 companies per year, accounting for around 80% of UK GDP. The data covers
the period 2002 to 2014. This survey is stratified to match the UK population of companies and so should be
broadly representative of aggregate UK trends.
The second is the Bureau van Dijk (BvD) FAME database, compiled from UK company accounts. Whereas
larger firms are required to submit full annual accounts, there are exemptions for smaller firms. So the
sample is smaller, at around 20,000 firms per year, and is biased towards larger firms. However, it contains
detailed data on firm-level characteristics which are useful when exploring the determinants of productivity.
These data enable us to examine the distribution of productivity across firms. Chart 16a plots this
distribution in 2014, based on the ONS data.48
 It is wide and elongated, with a long, thin upper tail of
high-productivity firms and a short, fat lower tail of low-productivity firms. This shape means that modal
productivity among UK companies is around 50% lower than mean productivity.49
One interesting question is how this distribution has evolved over time. Chart 17 looks at different
percentiles of the firm-level productivity distribution over time, both in levels terms and indexed to 2002=100.
There has been a widening dispersion in the distribution of productivity across companies over time. In
particular, there is a striking and widening divergence between frontier firms (say, the 99th percentile of firms)
and the long tail of non-frontier companies.
If we define frontier firms as the top 5% of firms by productivity performance, in line with the OECD, there is
clear and widening blue water between frontier and laggard companies (Chart 18). In arithmetic terms, it is
non-frontier companies that largely explain flat-lining productivity over recent years (Chart 19).
These dynamics cast the secular innovation versus stagnation debate in an interesting light. The distribution
of UK companies’ productivity suggests both forces have been operating, albeit at different points in the
distribution – innovation in the upper tail, stagnation in the lower one. Widening productivity dispersion
means that secular innovation and stagnation are complementary, not competing, hypotheses.
For a relatively small cohort of frontier companies, secular innovation is clearly evident, with both high and
rapidly-rising levels of productivity. For example, around 1% of UK firms have seen average productivity
growth of around 6% per year. This poses a serious challenge to the notion that stalling innovation has been

46
 For example, Andrews et al (2015).
47
 This work contains statistical data from ONS which is Crown Copyright. The use of the ONS statistical data in this work does not
imply the endorsement of the ONS in relation to the interpretation or analysis of the statistical data. This work uses research datasets
which may not exactly reproduce National Statistics aggregates.
48
 The BvD data delivers a similarly-shaped distribution.
49
 Other authors finding a long tail of low-productivity companies in the UK include Bloom and Van Reenen (2007).
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
the key driver of the productivity slowdown. At the same time, for a large cohort of non-frontier companies
secular stagnation is evident, with low and flat-lining levels of productivity. For example, around one-third of
UK companies have seen no rise in productivity throughout this century. This is a long tail.
A second implication of these results, consistent with the cross-country evidence, is that rates of
technological diffusion from frontier to non-frontier companies appear to have slowed. It is stalling diffusion,
rather than stifled innovation, that accounts for the UK’s productivity puzzle. These patterns are not unique
to the UK. They are shared by a number of other countries internationally. That has led the OECD to
conclude that the technological diffusion engine among companies appears to have broken.50

A comparison of the UK with other advanced countries suggests that both the degree of dispersion in
productivity performance is larger in the UK, and that this dispersion has widened more in the UK, than in
other countries. The dispersion of services sector productivity is more than 50% higher in the UK than in
other advanced economies (Chart 20a). It has also widened by materially more in the UK. Even in a world
of long and lengthening tails of low-productivity companies, the UK is a striking outlier.
One potential explanation for this longer tail of UK companies than elsewhere is poor management practices.
Using the database developed by Bloom and Van Reenen (2007), we can plot a normalised distribution of
management skills for a subset of UK firms.51
 This suggests (a lack of) management quality is a plausible
candidate explanation for the UK’s long tail of companies, as Bloom and Van Reenen find (Chart 21).
Looked at quantitatively, there is a statistically significant link between the quality of firms’ management
processes and practices and their productivity. And the effect is large. A one standard deviation
improvement in the quality of management raises productivity by, on average, around 10%. This suggests
potentially high returns to policies which improve the quality of management within companies.
Decomposing the Distribution
We can go further in uncovering some of the characteristics of the companies in the upper and lower tails.
Might they simply reflect sharp regional differences in productivity, with frontier companies clustered in some
regions and laggards in others? Might it simply reflect some sectors, such as manufacturing, generating
higher levels of productivity and sitting in the frontier while others, such as services, occupy the lower tail?
Charts 22a to 22f look at different cuts of the data according to: (a) the UK region in which the company is
based; (b) the sector in which the company operates; (c) whether the company exports overseas;
(d) whether the company is foreign-owned; (e) whether the company has introduced new products and
processes in the preceding three years; (f) the size of the firm; and (g) the company’s leverage.

50
 Andrews et al (2015).
51
 With thanks to John Van Reenen for making these data available to us.
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
Beginning with the regional picture, it is well-known that there are sharp differences in average productivity
across UK regions.52
 For example, the UK’s highest productivity region (London) is around 75% more
productive than its lowest (the North East). Nonetheless, if we look at the regional distribution of productivity,
the similarities are more striking than the differences (Chart 22a). Every region has the same long, thin
upper tail of frontier companies and a short, fat lower tail of laggard firms.
It is true, as a matter of arithmetic, that the upper tail of frontier companies is somewhat larger, and the lower
tail of laggards shorter, in London than in, say, the North East. But the key point is that every region has
plenty of both types of firm. Regional differences are not the main factor explaining the UK’s long tail of firms
nor why this tail is longer in the UK than elsewhere.
Second, the same is true, by and large, if we move from regions to sectors. There are some reasonably
marked differences in average levels of productivity across sector. For example, manufacturing is on
average around 35% more productive than construction. Nonetheless, these differences are dwarfed by
variations within each sector (Chart 22b). Each has a long, thin upper tail and a short, fat lower tail.
Whatever is driving slower rates of technological diffusion, sector-specific technologies are not it.

Third, if we look at the external-facing profile of firms, we begin to uncover some important differences
(Chart 22c and d). Firms which export have systematically higher levels of productivity than
domestically-oriented firms, on average by around a third. The same is true, even more dramatically, for
foreign-owned firms. Their average productivity is twice that of domestically-oriented firms.
This is not altogether surprising. Firms that export are likely to be exposed to global competition and many
are integrated into global supply chains. This increases incentives to boost efficiencies and to match
international best practices.53
 The self-same forces are likely to be at play among foreign-owned firms. The
productivity benefits these external-facing firms bring underscore the importance of openness to trade and
foreign direct investment in generating rising productivity and living standards over time.
We can quantify these benefits by looking in more detail at the relationship between firms’ productivity and
their export share (Chart 23). There is a positive and statistically-significant relationship. Every 10
percentage point increase in a company’s export share of turnover is, on average, associated with a 3%
increase in productivity. While causation could operate in either direction, this illustrates the productivity
benefits of a corporate sector which is open to trade and exposed to external competition and innovation.
Chart 22e classifies firms according to their spending on new products and processes. The differences here
are not especially surprising, with average productivity around 20% higher among firms investing in new

52
 For example, Haldane (2016).
53
 Newman et al (2015).
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
products and processes. Nonetheless it is striking, and perhaps surprising, these differences are not larger.
Clearly, they do not explain most of the productivity differences across companies.
Firm size appears to matter somewhat more to productivity (Chart 22f). For example, for small firms (with
fewer than 50 employees) the picture is a nuanced one. In general, their average levels of productivity are
materially lower than for larger firms, on average around 7% lower. There is, in particular, a larger lower tail
of small companies with low, or even negative, levels of productivity.
Nonetheless, some smaller companies do inhabit the high-productivity, upper tail. Moreover, small
companies tend to exhibit faster rates of productivity growth than larger firms, even when it is from a lower
base. Table 3 looks at average productivity growth rates among firms of different size. Smaller firms have
higher average growth rates, and greater dispersion in these growth rates, than larger ones.
Finally, when it comes to companies’ leverage, the picture is also a nuanced one.54
 High leverage is a
characteristic of firms with two very distinct productivity profiles (Chart 22g). Very low-productivity firms tend
to have high leverage because their profits, repayment capacity and investment are low. But high leverage
also characterises high-productivity firms, whose profits, borrowing and investment are high.
These patterns provide an explanation for the damaging impact of the crisis on productivity growth. The
absence of credit may have affected disproportionately high leverage, high productivity companies operating
in the upper tail. The distribution of debt is also important when gauging what impact monetary policy and
regulatory forbearance may have had on the lower tail of companies, to which I now turn.
The Role of Monetary Policy
These distributional data enable us to look closely at the lower tail of companies who have potentially been
protected from failure since the global financial crisis by regulatory forbearance and accommodative
monetary policies. Chart 16b plots this lower tail on three dates, pre-crisis (2002), mid-crisis (2008) and
post-crisis (2014). Unsurprisingly, the crisis fattened the lower tail.
Since the crisis, however, this tail has shrunk again. The tail of low-productivity companies today is, if
anything, smaller than it was pre-crisis. On the face of it, that is not easy to square with (regulatory and
monetary) policies having had a significantly damaging impact on aggregate productivity by keeping alive a
large tail of low-productivity companies.
However, these distributions cannot by themselves identify the role monetary policy has played in influencing
companies’ fortunes. To gauge that, we need to undertake a counter-factual exercise. Specifically, we

54
 Bahaj, Foulis, Gal and Pinter (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
consider a simple scenario in which Bank rate is held at 4.25% rather 0.25%. What impact would that have
had on companies’ solvency and hence on aggregate productivity?
With data on their debts, interest payments and profits, we can simulate interest cover ratios for each
company in our sample – that is to say, their profits before interest divided by interest paid (Chart 24). And
by making an assumption about the level of the interest cover ratio at which a company is forced to default,
we can simulate how a different interest rate path might have affected the solvency of each firm and hence
aggregate productivity.
For the purposes of the experiment, we assume a default threshold for the interest cover ratio of one. That is
to say, we assume a company that cannot cover its interest payments with profits fails. Different
assumptions are clearly possible. The effect of higher interest rates is to shift the distribution of companies’
interest cover ratios to the left, so that a number of them now lie below the solvency threshold. In the
simulation, an extra 10% of companies go bankrupt. This would translate into an immediate loss of around
1½ million jobs – a very significant macro-economic cost.
Nonetheless, there are also prospectively productivity benefits. The scale of these benefits depends on the
productivity characteristics of those firms going bust and specifically on their levels of debt. As Chart 25
shows, there is a U-shaped relationship between firms’ productivity and debt, with both high-productivity
(“gazelles”) and low-productivity (“zombie”) companies having high debt ratios. Higher interest rates hit both
types of company and so the net effect on productivity is an empirical question.
If we assume all firms have the same solvency threshold, more of the firms going bust are zombies than
gazelles – there is more creative destruction than destructive destruction (Chart 24c). Nonetheless, the
positive impact of higher interest rates on aggregate productivity is significantly tempered by the bankruptcy
of some high-leverage, high-productivity companies. The overall effect of higher interest rates in the
simulation is to boost the level of productivity by around 1 or 2% relative to the baseline.
This is a significant productivity gain. At the same time, it does not account for the majority of the
productivity shortfall since the crisis. Moreover, it comes at a hefty employment cost. Should monetary
policymakers have sacrificed 1 ½ million jobs for the sake of an extra 1 or 2% of productivity? Hand on
heart, I can tell you this one would not knowingly have done so.
Of course, this experiment is only illustrative and is based on a number of simplifying assumptions. For
example, it assumes there is no adverse effect on the output of firms who do not go bust but who
nonetheless find their cashflows squeezed. This would tend to raise the output and employment cost of
higher interest rates and make the terms of trade for productivity improvements less favourable still.
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
Acting in the opposite direction, gazelle firms may be able to borrow to avoid bankruptcy in the event of
higher interest rates – their solvency threshold might be higher than for zombie companies. If we assumed
instead that only the lowest-productivity firms went bust, the boost to productivity is around 3% (Chart 24d).
This is more material, but would still need to be weighed against the significant output and employment cost.
The Dispersion of Productivity and Wages
A final use to which the distributional data can be put is to analyse whether rising dispersion in productivity
among firms is mirrored in rising dispersion of wages among households. Have productivity differences
been a contributor to rising income inequalities? This has been the subject of a number of recent empirical
studies, which have tended to find productivity to have been a significant factor.55

A useful starting point is the distribution of wages across our panel of firms (Chart 26). There is some
evidence of a widening dispersion in wages across companies over time. The pattern broadly mirrors
productivity, with wages among the top 1% of companies rising, except immediately after the financial crisis.
At the same time, pay has largely stagnated among as many as three-quarters of the firms in the distribution.
By itself, this does not tell us how closely firm-level pay and productivity have been correlated and what
contribution productivity may have played in widening the dispersion of wages.56
 While there is an
established literature on the aggregate relationship between productivity and wages, the relationship
between firms’ productivity and average pay levels is less certain. Nevertheless, looking at the distributions
of pay and productivity, three points stand out (Chart 27).
First, labour productivity can explain around 60% of the variation in average pay across firms, controlling for
other firm characteristics. On average, firms with 1% higher productivity have 0.2% higher pay. And firms
with 1 percentage point faster productivity growth have 0.5 percentage points faster wage growth (Table 4).
Productivity dispersion across companies is an important, if by no means the only, driver of wage dispersion
across households.
Second, the distribution of pay is materially narrower than for productivity. The reason for this is that a
significant part of firm-specific variation in productivity is absorbed by firms in their profits or reflected in their
prices. The reason is that firms’ profits and prices absorb a significant part of firm-specific variation in
productivity. Many of the workers in the more productive tail of firms are being underpaid relative to their
average productivity, which results in frontier firms making a larger profit share. The opposite is the case at
the other end of the productivity distribution.

55 For example, Furman and Orszag (2016)
56 There is time series evidence to suggest a strong historical relationship between aggregate productivity and real wages. For example,
Castle and Hendry, Blundell et al (2013) and Disney et al (2013).
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
This analysis focusses on differences in pay across firms. But what about pay within firms?57
 We can
examine this by tracking differences between the average pay in each company and the pay of their highest
paid director. Chart 28 plots the median across firms of these series over time, while Chart 29 compares the
variance of each series as well as the gap between average pay and the highest paid director.
Since 1998, the variance in average pay and director pay has increased across firms, although it has fallen
notably since the financial crisis. The median gap between average pay and director pay has followed a
similar pattern. Although these data are far from perfect they suggest that, while within-firm pay differentials
have accounted for some of the increased dispersion in pay across households, across-firm variations have
made a larger contribution.
Tackling the Productivity Puzzle
There has been no shortage of public policy ideas over recent years for boosting productivity growth.
Reports by the IMF and OECD have suggested measures ranging from increased infrastructure spending to
improved education and training programmes.58
 Earlier this year, the UK Government issued a Green
Paper setting out various pillars to support productivity.59
 And, most recently, here at the LSE the Growth
Commission produced their second report on measures to boost UK productivity.60
In generic terms, these policy measures fall into three categories. First, there are measures which support
all companies, irrespective of sector, region or characteristic. For example, last year’s Autumn Statement
provided additional financial support for UK infrastructure spending, while in the Budget a couple of weeks
ago the UK government announced plans for increased support for technical skills.
Second, there are measures which support technological innovation – the creation and growth of frontier
firms. One potentially important micro-economic measure supporting innovation is greater recognition of the
importance of companies’ intangible assets, such as intellectual property (IP). These constitute an
increasing fraction of companies’ total assets, but are not measured or valued in as consistent and coherent
a way as tangible assets such as plants and machinery.
This can have real costs for the companies concerned. It means these assets are often not valued fully by
investors and lenders. This raises the cost of capital for these firms to sub-optimally high levels – a market
failure. Remedying that market failure, by ensuring intangible assets are consistently and coherently valued,
is important if these companies are to grow to reach their full potential.61

57 For example, Song et al (2015) find that virtually all the rise in earnings dispersion between workers is accounted for by increasing
dispersion across the average wages paid by different firms.
58 Andrews et al (2015) and Dabla-Norris et al (2015).
59 HM Government (2013).
60 Aghion et al (2017).
61
 Big Innovation Centre (2017b).
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
More broadly, it is possible that current UK corporate governance practices may act as a brake on innovative
companies. A recent report by the Big Innovation Centre makes important proposals for re-purposing UK
companies to support long-term investment and productivity.62
 The government recently initiated a Patient
Capital Review to explore how best to support the financing of innovative companies as they scale-up.63
A third category of policy measure focusses on the fortunes, not of innovative frontier companies, but the
long tail of low-productivity non-frontier firms. These companies have tended to be focussed on somewhat
less historically. Indeed, their large numbers and disparate characteristics may be one reason why this is the
case. Yet given their scale, the returns to modest improvements in these firms could be dramatic.
As a thought experiment, imagine productivity growth in the second, third and fourth quartiles of the
distribution of UK firms’ productivity could be boosted to match the productivity of the quartile above. That
sounds ambitious but achievable. Arithmetically, that would deliver a boost to aggregate UK productivity of
around 13%, taking the UK to within 90-95% of German and French levels of productivity respectively.
The policy question is how to effect those improvements. One idea which offers real potential comes from
the Productivity Commission chaired by Sir Charlie Mayfield.64
 This starts from the assumption that not only
is there a long tail of companies, but that many are unaware of that fact. For the same reason most
car-owners believe they are above-average drivers, most companies might well believe they have
above-average levels of productivity.
In fact, we know most companies have below-average levels of productivity and a large fraction of them have
seen no productivity improvement for several decades. The Mayfield Commission aims to create an app
which enables companies to measure their productivity and benchmark themselves against other companies
operating in similar sectors and regions.
By shining a light on companies’ relative performance, the aim is that this would serve as a catalyst for
remedial action by company management. Indeed, the aim is to provide firms not only with a means of
benchmarking themselves, but with tools to improve performance along identified areas. These online tools
would be a mechanism for speeding-up the process of technological diffusion to the long tail.
One practical way of doing so is by pairing up companies, frontier and non-frontier, to enable the sharing of
best practices. This is effectively a mentoring scheme for firms, the like of which is already common among
individuals. What would be in it for frontier companies? A more productive supply chain is clearly in their
interests. The public sector could also play a useful nudging role in its procurement practices.

62
 Big Innovation Centre (2017a).
63
 HM Treasury and Department for Business (2017).
64
 Mayfield (2016).
All speeches are available online at www.bankofengland.co.uk/speeches
20
20
A more ambitious idea still, which I have been considering with Philip Bond, is to develop a virtual
environment which would enable companies to simulate changes to their business processes and practices.
These platforms are already used by many frontier firms to assess the impact of new technologies and
processes on their business. These tools can be created, and tailored to companies’ circumstances, at
relatively low cost. This makes them a potentially cost-effective way of facilitating diffusion to the long tail.
Policy measures such as these are valuable in boosting living standards and narrowing the dispersion of
incomes across households. As a by-product, they also potentially offer benefits to monetary policymakers.
Structural measures to boost investment will tend to raise the interest rate in the economy which equilibrates
investment opportunities and savings plans. This is sometimes known colloquially as r*.
Policies which boost r* mean that a given interest rate is likely to impart greater stimulus. Or, put differently,
they mean the same level of demand in the economy can be achieved with a higher interest rate. And a
“pivot” in the mix of policies, with somewhat less of a contribution from monetary policy and more from
structural policies, would probably be desirable. Not least, it could help mitigate any adverse impact of
accommodative monetary policies on productivity or the distribution of income and wealth.
Conclusion
Productivity is a gift for rising living standards, perhaps the greatest gift. It is not, however, one that always
keeps on giving, as recent events attest. Whether in supporting living standards, or in shrinking their
distribution, tackling the global productivity puzzle is among the most pressing public policy issues today.
If history is any guide, there is unlikely to be any single measure which puts productivity growth back on
track. But measures which support the long tail of companies, currently operating at low levels of
productivity, have the potential to do considerable good.
As Olympic athletes have shown, marginal improvements accumulated over time can deliver world-beating
performance. Applying those marginal gains to the population of UK companies could significantly improve
UK living standards, even if those are harder to measure than gold medals.
All speeches are available online at www.bankofengland.co.uk/speeches
21
21
Annex
Chart 1: Decomposition of long run UK GDP growth
Source: Hills, Thomas and Dimsdale (2016) “Three Centuries of Data - Version 2.3”, available here.
Chart 2: UK labour productivity growth forecast revisions
Sources: ONS and Bank of England Inflation Report projections.
Notes: Forecasts for hourly labour productivity.
All speeches are available online at www.bankofengland.co.uk/speeches
22
22
Chart 3: IMF advanced economy labour productivity growth forecast revisions
Sources: IMF and Bank calculations.
Notes: Forecasts for productivity per worker.
Chart 4: Long run UK total factor productivity
Sources: Hills, Thomas and Dimsdale (2016) “Three Centuries of Data - Version 2.3”, available here.
Illustrative estimates prior to 1850 are based on data on the growth rate of technology between 1AD and 1750AD in “A farewell to Alms”
by Gregory Clark.
All speeches are available online at www.bankofengland.co.uk/speeches
23
23
Chart 5: Decomposition of long run UK labour productivity growth
Sources: Hills, Thomas and Dimsdale (2016) “Three Centuries of Data - Version 2.3”, available here.
Chart 6: Long run UK total factor productivity growth
Sources: Hills, Thomas and Dimsdale (2016) “Three Centuries of Data - Version 2.3”, available here.
Table 1: Average TFP growth in the UK
Average growth rates
Annual % change
Pre-industrial
1 to 1760 AD
0.014
Industrial Revolution
1760 to 1850
0.238
Mass Industrialisation
1850 to 1918
0.867
IT Revolution
1950 to 2008
1.704
Post-financial crisis
2008 onwards
-0.367
Sources: Hills, Thomas and Dimsdale (2016) “Three Centuries of Data - Version 2.3”, available here.
Illustrative estimates prior to 1850 are based on data on the growth rate of technology between 1AD and 1750AD in “A farewell to Alms”
by Gregory Clark.
All speeches are available online at www.bankofengland.co.uk/speeches
24
24
Chart 7: Long run global total factor productivity growth for 116 countries
Source: Penn World Tables database.
Notes: Sample of countries changes over time, with fewer countries available at the start of the period shown.
Chart 8: Long run global total factor productivity growth for advanced and emerging economies
Source: Penn World Tables database.
Notes: Percentage change in median TFP growth; five year moving average.
-6
-4
-2
0
2
4
6
1951 1961 1971 1981 1991 2001 2011
Median 1st quartile 3rd quartile
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
25
25
Chart 9: Productivity trends across individual countries
a) Advanced economies:
Emerging markets:
Source: Penn World Tables 9.0 and Bank calculations.
Notes: Percentage change in TFP growth; trend lines show HP filter.
All speeches are available online at www.bankofengland.co.uk/speeches
26
26
Chart 10: Distribution of productivity levels across 44 countries relative to the US
Source: Penn World Tables 9.0 and Bank calculations.
Notes: Median TFP level at current PPPs; US TFL level = 1; sample of 44 countries.
Chart 11: Productivity level relative to the US, by region
Source: Penn World Tables 9.0 and Bank calculations.
Notes: Median TFP level at current PPPs; USA = 1.
All speeches are available online at www.bankofengland.co.uk/speeches
27
27
Chart 12 – Convergence coefficient
Source: Penn World Tables 9.0 and Bank calculations.
Notes: The coefficient can be interpreted as follows: for every 10ppt TFP gap compared to the US, TFP growth is x percentage points
faster than in the US.
Chart 13: Productivity across sectors
Source: ONS and Bank calculations.
All speeches are available online at www.bankofengland.co.uk/speeches
28
28
Table 2: Contributions to annual labour productivity growth
Source: ONS and Bank calculations.
Chart 14: The standard deviation of productivity across industries
Source: EUKlems productivity database, ONS and Bank calculations.
Notes: The chart excludes the mining & extraction, energy and real estate industries.
All speeches are available online at www.bankofengland.co.uk/speeches
29
29
Chart 15: The variance of productivity within and across industries
Source: ONS Research Database and Bank calculations.
Notes: This work contains statistical data from the ONS which is Crown Copyright. The use of the ONS statistical data in this work does
not imply the endorsement of the ONS in relation to the interpretation or analysis of the statistical data. This work uses research
datasets which may not exactly reproduce National Statistics aggregates. This note applies to all charts and analysis using data from
the ONS Research Database.
Chart 16: The distribution of productivity across firms
(a) Distribution in 2014 (b) Lower tail of distribution in selected years
Source: ONS Research Database and Bank calculations.
Notes: Gross value added per worker.
-0.002
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
-50 0 50 100 150 200
2014
Density
GVA per worker, £000s
-0.002
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
-50 -30 -10 10
2002
2008
2014
Density
GVA per worker, £000s
All speeches are available online at www.bankofengland.co.uk/speeches
30
30
Chart 17: Firm level productivity distribution over time
(a) Levels (b) Index
Source: ONS Research Database and Bank calculations.
Notes: Productivity per worker.
Chart 18: Productivity among Frontier and Non-Frontier companies
Source: ONS Research Database and Bank calculations.
Notes: Frontier defined as top 5% of firms by GVA per worker in a given year and sector as defined by 2-digit SIC 2007 industrial
classification.
0
20
40
60
80
100
120
140
2002 2004 2006 2008 2010 2012 2014
10th percentile 25th percentile
50th percentile 75th percentile
90th percentile
GVA per person, £th
0
20
40
60
80
100
120
140
160
180
2002 2004 2006 2008 2010 2012 2014
10th percentile 25th percentile
50th percentile 75th percentile
90th percentile
GVA per person, Index 2002 =100
All speeches are available online at www.bankofengland.co.uk/speeches
31
31
Chart 19: Contribution of frontier and other firms to productivity growth
Source: ONS Research Database and Bank calculations.
Notes: Frontier as defined in Chart 18.
Chart 20: Productivity dispersion*
(a) Services (Levels) (b) Manufacturing (Levels)
(c) Services (Index) (d) Manufacturing (Index)
Source: OECD and Berlingieri, Blanchenay and Criscuolo (2017, forthcoming); ONS Research Database and Bank calculations.
Notes: Charts a and b show the log difference between the 90th and 10th percentiles; Charts c and d show the same data indexed to
zero in 2001. UK data only available from 2002, UK index base year = 2002. Data have been aggregated from a detailed industry level.
0.0
1.0
2.0
3.0
4.0
5.0
2001 2003 2005 2007 2009 2011 2013 2015
Denmark Finland
France Italy
Japan Norway
New Zealand UK
Log (90/10) ratio
0.0
1.0
2.0
3.0
2001 2003 2005 2007 2009 2011 2013 2015
Denmark Finland
France Italy
Japan Norway
New Zealand UK
Log (90/10) ratio
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
2001 2003 2005 2007 2009 2011 2013 2015
Denmark
Finland
France
Italy
Japan
Norway
New Zealand
UK
Index, 0=2001
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
2001 2003 2005 2007 2009 2011 2013 2015
Denmark
Finland
France
Italy
Japan
Norway
New Zealand
UK
Index, 0=2001
All speeches are available online at www.bankofengland.co.uk/speeches
32
32
To the extent that industry classifications in the UK and the other countries shown are slightly different this may affect the comparability
of the series. Please note that the y-axis label for Charts 20a and 20b have been amended since this speech was originally published.
Chart 21: Management practices and labour productivity
Source: BvD, World Management Survey and Bank calculations.
Notes: The productivity kernel is based on around 280,000 firm-year observations on labour productivity (in logs) spanning the 2002-
2013 period (source: BVD). The management kernel is based on around 1500 interviews with firm managers from the WMS database
(as described in Bloom and Van Reenen, 2007, 2010). Covering the 2002-2014 period. The raw data are converted to Z-scores by
removing the sample mean from each observation and dividing the demeaned values by the sample standard deviation.
Chart 22: Firm level productivity distributions
a) Region b) Industry
0
0.1
0.2
0.3
0.4
0.5
0.6
-4 -2 0 2 4
Management
score
Labour
productivity
Z score
Density
-0.01
0
0.01
0.02
0.03
-20 30 80 130 180
 North East North West
Yorkshire & Humberside East Midlands
West Midlands East of England
 London South East
 South West Wales
Scotland
Density
GVA per employee, £000's
-0.005
0
0.005
0.01
0.015
0.02
0.025
-50 0 50 100 150 200 250
Manufacturing
Construction
Wholesale and retail trade
Transportation and storage
Information and communication
Professional, scientific and technical activities
Density
GVA per worker, £000s
All speeches are available online at www.bankofengland.co.uk/speeches
33
33
c) Export status d) Foreign ownership
e) Innovation f) Size
g) Leverage
Source: ONS, ONS Research Database, BvD and Bank calculations.
Notes: These charts plot the kernel density distribution of labour productivity (real GVA per employee) across Great Britain. Chart (e)
uses unweighted firm data. Chart (g) shows data BvD rather than VML firm-level data and is not directly comparable to the other charts.
-0.002
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
-50 50 150 250
Non exporter
Exporter
GVA per worker, £000s
Density
-0.002
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
-50 50 150 250 350
Domestic
Foreign
GVA per worker, £000s
Density
-0.002
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
-50 0 50 100 150 200
Innovators
Non-innovators
Density
GVA per worker, £000s
-0.002
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
-50 0 50 100 150 200 250
Small
Medium
Large
GVA per worker, £000s
Density
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
-50 0 50 100 150
Low leverage
High leverage
GVA per worker, £000s
Density
All speeches are available online at www.bankofengland.co.uk/speeches
34
34
Chart 23 – Firm exports and productivity
Source: BvD and Bahaj, Foulis, Gal and Pinter (2016).
Notes: Results from a Pooled Regression (slope=0.34; t-value=12.2), data for 2013.
Table 3 – Median productivity growth by firm size*
Number of employees
6-10 2.8
11-20 2.5
21-50 1.8
>50 1.2
Source: BvD
Notes: Data for 1998 -2013; Three-year cumulative growth.
Chart 24: Monetary policy counterfactual experiment based on company accounts data
a) Interest Cover Ratio b) Leverage
0.00
0.05
0.10
0.15
-5 0 5 10 15
Before intervention
After intervention
Density
Interest cover ratio
ICR = 1
All speeches are available online at www.bankofengland.co.uk/speeches
35
35
c) Productivity – scenario 1 d) Productivity – scenario 2
Source: BvD and Bank calculations.
Notes: Scenario 1 assumes that half of firms with interest cover ratios of less than 1 exit the market. In scenario 2, only the lowproductivity half of firms with interest cover ratios of less than 1 exit the market.
Chart 25: The estimated relationship between leverage and labour productivity
Source: Bahaj, Foulis, Gal and Pinter (2016).
Notes: The figure plots the results from local regressions. The red line shows the results from local weighted scatter smoothing
(LOWESS), the blue line shows the results from a third degree kernel weighted local polynomial smoothing where the gray area depicts
the associated 95% bootstrapped confidence bands. Labour productivity is defined as the log of gross value-added per number of
employees. During the local regression estimations, we trim the data by deleting the top and bottom 1% of estimated productivity and
leverage values in order to mitigate problems of outliers.
All speeches are available online at www.bankofengland.co.uk/speeches
36
36
Chart 26: Distribution of average pay across companies
Source: ONS Research Database and Bank calculations.
Notes: Series show wage deciles/percentiles.
Chart 27: Distribution of firm level labour productivity, wages and profits in 2014
Sources: ONS Research Database and Bank calculations.
Table 4: Regressions of average pay on productivity
Average pay
Levels Growth
Productivity 0.20*** 0.49***
Productivity2
0.04*** 0.004
Employment 0.35*** 0.14***
Employment2
-0.03*** -0.005*
Age 0.002*** -0.002***
Exporter -0.005 -0.03*
Foreign owned 0.20*** 0.09***
Constant
Observations 241,165 215,614
R-squared 0.60 0.51
Sources: ONS Research Database and Bank calculations. Notes: Regressions are in logs.
All speeches are available online at www.bankofengland.co.uk/speeches
37
37
Chart 28: Average and director pay from company accounts data
Sources: BVD.
Chart 29: Variance of average and director pay from company accounts data
Sources: BvD.

 History of Central Bank Balance Sheets
• Recent QE
• Channels of QE
• Impact of QE
• State-dependency and Spillovers from QE
Overview
4
History of QE
Bank of England Balance Sheet – % of GDP
(1700-2014)
Source: Hills, Thomas and Dimsdale (2015).
5
0
5
10
15
20
25
30
1700 1750 1800 1850 1900 1950 2000
Bullion, coin and retained notes
Other securities including repos
Government securities including loan to APF
per cent of
nominal GDP
WWII
WWI
Financial
Crisis
South Sea
Bubble
Wall Street
Crash
War of
Spanish
Succession
Hundred
Days War
Bank of England Balance Sheet – % of Government Debt
(1700 – 2014)
Source: Hills, Thomas and Dimsdale (2015).
6
0
20
40
60
80
100
1700 1750 1800 1850 1900 1950 2000
Bullion, coin and retained notes
Other securities including repos
Government securities including loan to APF
per cent of nominal
government debt
WWII
WWI
Financial Crisis
South Sea
Bubble
Wall Street
Crash
War of
Spanish
Succession
Hundred
Days War
1970s
recession
1980s
recession
1990s recession
Long
Depression
Central Bank Balance Sheets (1900-2013)
7
Countries covered are: Australia, Canada, Finland, France, Germany, Italy, Japan, Norway, Sweden, Switzerland, the United Kingdom and the United States.
After 1999, they consider aggregated balance sheet data for the European System of Central Banks (ESCB) in lieu of the euro area countries Finland, France, Germany and Italy.
Source: Ferguson, Schaab and Schularick (2015). ‘Central bank balance sheets: expansion and reduction since 1900’.
8
Recent QE
QE Timeline
9
Central bank balance sheets
10
Source: Bank of England, Federal Reserve, Bank of Japan, European Central Bank Bloomberg, Thomson Reuters Datastream and Bank calculations.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
110%
120%
130%
2006 2008 2010 2012 2014 2016 2018
Bank of England Per cent of GDP
Federal Reserve
ECB
Bank of Japan
Forecast
Central bank balance sheets
11
Source: Bank of England, Federal Reserve, Bank of Japan, European Central Bank Bloomberg, Thomson Reuters Datastream and Bank calculations.
Central bank balance sheets
12
Source: Bank of England, OBR, IMF WEO, OECD, Global Financial Data, Federal Reserve Board, Federal Reserve Bank of St. Louis,
Bank of Japan, European Central Bank Bloomberg, Thomson Reuters Datastream and Bank calculations.
Types of Asset Purchases
13
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Central
bank
reserves
Real estate
derivatives
Early 2000s - QEJ
14
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Real estate
derivatives
BoJ
Central
bank
reserves
2008-2010 - QE 1,2,3
15
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Real estate
derivatives
Fed
Fed, BoE
Central
bank
reserves
2011-2012 - LTROs
16
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Real estate
derivatives
ECB
Fed
Fed, BoE, ECB
Central
bank
reserves
2013-2014 – QQE 1,2
17
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Real estate
derivatives
ECB
BoJ
BoJ
BoJ
Central
bank
reserves
2015 – ECB QE
18
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Real estate
derivatives
ECB
BoJ
BoJ
ECB, BoJ
Central
bank
reserves
2016 – ECB QE, BoE QE, BoJ
19
Credit
risk
Duration
risk
Equities
Government bonds
Corporate
bonds
Covered
bonds/ABS
Agency
MBS
Real estate
derivatives
ECB, BoE
ECB, BoE, BoJ
Central
bank
reserves
BoJ
BoJ
20
Channels of QE
How does QE work?
21
“The problem with QE is that it works in practice, but it
doesn’t work in theory.”
– Ben Bernanke, January 2014
What do you need to believe for QE to work?
• Information frictions
– QE signals lower future interest rates – signalling channel
– QE lowers uncertainty – uncertainty channel
– QE lowers exchange rate – exchange rate channel
• Financial frictons
– QE lowers liquidity premia – liquidity channel
– QE causes a portfolio switch into higher risk assets – portfolio balance channel
– QE encourages new borrowing/lending – lending channel
22
Transmission mechanism of QE
23
Asset
Purchases
3. Liquidity
2. Portfolio
rebalancing:
• Duration
• Local
supply
1. Policy
signalling
Money 6. Bank lending
5. Confidence 5. Risk aversion/ uncertainty
Relative
asset prices
Cost of
borrowing
Total wealth
Spending
and income
Inflation
and growth
4. Exchange
rate
Channels of QE - what do you have to believe?
24
Channel What do you have to believe for this channel to
work? (what frictions?)
State contingent?
1. Policy signalling Information frictions - need to “put money where
your mouth is”. Yes
2. Portfolio balance
• Duration
• Local supply
Preferred-habitat demand – preferences for bonds
of specific maturities. Limits to arbitrage. Some
investors do not view bonds of different maturities
as perfect substitutes.
Yes
3. Market liquidity
premia
Markets dysfunctional. Transaction costs. Yes
4. Exchange Rate Exchange rate a function of interest rate differentials
and/or risk premia Yes
5. Confidence/risk
aversion/uncertainty
QE improves the economic outlook/reduces risk of
bad outcomes (via any mechanism) Yes
6. Bank lending Increased deposits expand banks’ balance sheets.
Bank lending is not constrained. Agents cannot
perfectly substitute other forms of lending.
Yes
25
Impact of QE
Transmission mechanism of QE
26
Asset
Purchases
3. Liquidity
2. Portfolio
rebalancing:
• Duration
• Local
supply
1. Policy
signalling
Money 6. Bank lending
5. Confidence 5. Risk aversion/ uncertainty
Relative
asset prices
Cost of
borrowing
Total wealth
Spending
and income
Inflation
and growth
4. Exchange
rate
Lots of evidence for asset price impact
27
‘Unconventional Monetary Policy: a re-appraisal’, Claudio Borio and Anna Zabai
Portfolio rebalancing/liquidity channel
28
Change in long rates around selected QE announcements
Change in 10 year spot market interest rates over two day windows around QE events, against size of announcement relative to
that economy’s GDP at the time.
Note: does not control for expectations of QE announcements.
Source: Bloomberg and Bank calculations.
Policy signalling channel
29
Change in short rates around selected QE announcements
Change in 3 year spot market interest rates over two day windows around QE events, against size of announcement relative
to that economy’s GDP at the time.
Note: does not control for expectations of QE announcements.
Source: Bloomberg and Bank calculations.
Confidence/uncertainty channel
30
Change in VIX around selected QE announcements
Change in VIX over two day windows around QE events, against size of announcement relative to that economy’s GDP at the time.
Note: does not control for expectations of QE announcements.
Source: Bloomberg and Bank calculations.
Exchange rate channel
31
Change in effective exchange rates around selected QE announcements
Change in effective exchange rates over two day windows around QE events, against size of announcement relative to that economy’s GDP at the time.
Note: does not control for expectations of QE announcements.
Source: Bank of England, BIS, ECB, Federal Reserve, Bank calculations.
Portfolio balancing channel
32
Change in corporate bond yields around selected QE announcements
Change in investment grade corporate bond yields over two day windows around QE events, against size of announcement
relative to that economy’s GDP at the time.
Note: does not control for expectations of QE announcements.
Source: BoA Merrill Lynch and Bank calculations.
Portfolio balancing channel
33
Impact of QE on UK insurance companies and pension funds,
ex-ante and ex-post QE effects, £ million
Source: Joyce, Liu and Tonks (2015)
Sterling corporate bond issuance has been strong
since QE
34
Source: Dealogic and Bank calculations
(a) Issuance by UK, US and EA19 private non-financial corporations (PNFCs) or their financial vehicles. Includes investment grade and non-investment
grade bonds. Data are subject to revisions. 2003-08 is an average over the period.
Cumulative gross issuance of bonds by UK, US and EA19 PNFCs
Bank lending channel
35
Bank lending to the real economy
36
Case Study:
The Bank Of England’s August 2016
Monetary Policy Package
The Package of Monetary Policy Measures
37
• Announced by the Bank of England’s MPC on 4 August 2016
• The package comprised:
1. Rate cut:
• 25bp cut in Bank Rate to 0.25%;
2. Targeted funding:
• A new Term Funding Scheme;
3. Asset purchases:
• The purchase of up to £10bn of UK corporate bonds
• An expansion of UK government bond purchases by
£60bn to £435bn
Immediate Financial Market Reaction
38
1 day reaction
(3-4 August)
2 day reaction
(3-5 August)
UK 3-year forward
overnight index swap rate
-8bps -5bps
10-year gilt yield -17bps -15bps
£ ERI -1.3% -1.4%
FTSE All Share +1.5% +2.4%
Sterling IG
corporate bond spreads
-10bps -18bps
Sterling HY
corporate bond spreads
-8bps -22bps
fall in bond yields
depreciation of sterling
stock market rally
sharp tightening in
corporate bond spreads
Elements of Surprise
39
Market profile for Bank Rate
before and after the August MPC announcement
A B
• [A]: little reaction at the very short end of UK yield curve
• [B] at longer horizons, large falls and curve flattening
Elements of Surprise
40
Non-financial corporate investment grade spreads,
June-September2016
80
100
120
140
160
180
200
Jun Jul Aug Sep
Sterling IG Dollar IG Euro IG Basis points
2016
Referendum July MPC Aug MPC
Elements of Surprise
41
1-day change in sterling exchange rate index vs change in
UK 2-year interest rates relative to US and German interest
rates around UK monetary policy changes
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
-0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2
%
∆Exchange
Rates
%
∆ InterestRates
Rate Change
Rate Change & QE
QE
August 4
Local Supply Effects
Change in gilt yields-to-maturity and OIS curve on 4 August 2016
yields on sub 3y bonds fell less
(ineligible for asset purchases)
yields on 3y+ bonds fell more
(eligible for asset purchases)
-0.20
-0.18
-0.16
-0.14
-0.12
-0.10
-0.08
-0.06
-0.04
-0.02
0.00
2016 2026 2036 2046 2056 2066
OIS
 > 3-year
Sub 3-year
Percentage points 3yrs 7yrs 15yrs
gilt yields fell more than swap yields: swaps not eligible
for asset purchases
42
Transmission mechanism of QE
43
Asset
Purchases
3. Liquidity
2. Portfolio
rebalancing:
• Duration
• Local
supply
1. Policy
signalling
Money 6. Bank lending
5. Confidence 5. Risk aversion/ uncertainty
Relative
asset prices
Cost of
borrowing
Total wealth
Spending
and income
Inflation
and growth
4. Exchange
rate
Evidence on Second Leg More Mixed
44
Study Episode Real GDP CPI
Baumeister and Benati (2013) UK/US QE1 1.8% / 1.08% 1.5% \ 0.84%
Kapetanios, Mumtaz, Stevens and Theodoris (2012) UK QE1 2.5% 1.5%
Weale and Wieladek (2015) UK/US QE1 2.52% / 0.72% 4.2% / 0.76%
Giannone, Lenza, Pill and Reichlin (2014) ECB Liquidity policy
2008/2009
2% in IP N/A
Altavilla, Giannone and Lenza (2014) ECB OMT Impact on
Spain/Italy
2% / 1.5% 0.74% / 1.21%
Schenkelberg and Watzka (2013) Japan QE1 0.5% in IP No impact
Bank of Japan (2015) Japan QE2 1-3% 0.6-1%
Chen, Curdia and Ferrero (2012) US QE2 0.39% 0.12%
Del Negro, Eggertson, Ferrero and Kiyotaki (2015) Fed MBS + Liquidity policies 5% 3%
Gertler and Karadi (2013) QE1 – MBS Purchases 3.5% 4%
Gertler and Karadi (2013) QE1 – Sovereign Purchases 2.2% 3%
Note: Studies in yellow are empirical VAR studies, while those in white provide multipliers from structural empirical models.
Identifying QE’s Impact
45
• Most studies feed “event study” asset price responses through a macro model
• What if asset price responses are persistent?
QE1: total of £200 billion purchases
Asset
Change around QE1 announcements
(Feb 09, Mar 09, May 09,
Aug 09, Nov 09, Feb 10)
Change
4 March 2009 – 22 Jan 2010
Gilts (5-25 year average) -104 (o/w -90 gilt-OIS spread) -6 (o/w -41 gilt-OIS spread)
Corporate yields (investment-grade) -70 -387
Corporate yields (high-yield) -150 -1944
FTSE All-Share -3% +47%
Sterling ERI -4% +3%
Summary of asset price movements* around BoE QE 1
* In basis points, unless otherwise specified.
Some New Estimates
46
• Estimated VAR for UK, US and Japan QE programmes
• Four different identification schemes:
• Based on ordering, sign and variance restrictions
• Robustness check
• In line with Weale and Wieladek
Transmission of US QE
Note: Graph shows impulse responses to 1% surprise asset purchase announcement, identified with
a Choleski decomposition. The unit on the x-axis is months. Gray error bands are 68% quantiles and
the red lines show the median. Estimation period is March 2009 to February 2015.
See Haldane et al (2016). 47
(1% of nominal GDP)
Impact of US QE
(1% of nominal GDP)
Note: Graph shows impulse responses to a 1% of nominal GDP surprise asset purchase announcement, identified with a
Choleski decomposition. The unit on the x-axis is months. Estimation period is March 2009 to February 2015. The grey error
bands are 68% quantiles and the red lines show the median. See appendix of Haldane et al (2016).
48
Not all QE created equally
49
Peak impact of a central bank balance sheet
expansion of 1% of nominal GDP
Country/Programme Real GDP CPI
Canada
ECB
Japan ‐ QE1
Japan - QE2 0.13* 0.093*
Sweden
UK – QE 0.24* 0.34*
US – QE 0.63* 0.63*
UK ‐ Historical
Source: Bank calculations.
Asterisk indicates that the estimated impact is statistically significant.
Based on a structural vector autoregression (SVAR) model containing, as endogenous variables:
CPI (natural logarithm); real GDP (natural logarithm); yield on 10-year government bond; real equity prices (natural logarithm);
size of the balance sheet divided by nominal GDP, scaled by the level of nominal GDP in the first period prior of the expansion.
Average of results of four different identification schemes is shown. The individual schemes are: zero restrictions; sign restrictions;
a combination of zero and sign restrictions; sign variance decomposition restrictions.
Some QE Programmes Work Better Than Others
50
QE1: April 2001 – July 2008 QE2: August 2008 – February 2015
Note: As explained in the appendix of Haldane et at (2016), the impulse responses shown above are from a VAR model estimated on the series of actual JGB asset
purchases by the Bank of Japan, identified with a Choleski decomposition. The left hand chart suggests that QE1 in Japan did not have an impact on prices, which is
roughly in line with the survey in Ugai (2007). The multipliers in the second panel suggest, once the total size of purchases is taken in account, a similar total impact as
found in Bank of Japan (2015).
Liquidity Frictions
51
UK Market Liquidity Measure and Regime
0.00
0.05
0.10
0.15
0.20
0.25
0.0
0.2
0.4
0.6
0.8
1.0
1.2
2006 2008 2010 2012 2014
Regime (LHS)
Government Bond
Market Illiquidity
Measure (RHS)
US Market Liquidity Measure and Regime
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
0.0
0.2
0.4
0.6
0.8
1.0
1.2
2006 2008 2010 2012 2014
Regime (LHS)
Government Bond
Market Illiquidity
Measure (RHS)
State-dependence of QE
52
Peak impact of a central bank balance sheet
expansion of 1% of nominal GDP
Country/Regime Real GDP CPI
US / High financial frictions regime 0.60 0.73
US / Low financial frictions regime 0.32 0.485
UK / High financial frictions regime 0.24 0.645
UK / Low financial frictions regime 0.14 0.48
Source: Bank calculations.
Asterisk indicates that the estimated impact is statistically significant.
Based on a structural vector autoregression (SVAR) model containing, as endogenous variables:
CPI (natural logarithm); real GDP (natural logarithm); yield on 10-year government bond; real equity prices (natural logarithm);
size of the balance sheet divided by nominal GDP, scaled by the level of nominal GDP in the first period prior of the expansion.
Regime dependence is factored into the contemporaneous covariance matrix and identified using an indicator of frictions in the
government bond market.
Average of results of four different identification schemes is shown. The individual schemes are: zero restrictions; sign restrictions;
a combination of zero and sign restrictions; sign variance decomposition restrictions.
53
International spillovers from QE
Exchange rate channel
54
FX dynamics around selected QE announcements
Sources: Bank of England, ECB, Federal Reserve, Bloomberg, Bank of Japan, Bank calculations.
Evidence on spillovers to UK asset prices
Source: Rogers et al. (2014); Bank calculations; ECB refers to PSPP extension
55
Correlation of term premia estimates
Sources: Bloomberg, Bank calculations, New York Fed. Term premia estimates for the US from model by Adrian, Crump and Moench
(2013), applied to the UK and Germany following Malik and Meldrum (2014).
10 year spot nominal government bond term premia estimates
56
The Transmission of US QE
US Response to US QE** UK Response to US QE**
** 1% expansion of Fed balance sheet in terms of nominal GDP.
See Haldane et al (2016)
57
The International Transmission of US QE
UK Response to UK QE1 UK Response to US QE
2
1 1% expansion of BoE balance sheet in terms of nominal GDP 2 1% expansion of Fed balance sheet in terms of nominal GDP
See Haldane et al (2016) See Haldane et al (2016)
58
Conclusion
• Aggregate impact of QE:
– reasonably well-defined
– state-dependent
– international
• Distributional impact of QE:
– … for another day …

The theme of this conference is “Rethinking Macroeconomic Policy”. When it comes to financial stability,
that theme could hardly be more appropriate. The global financial crisis has been the prompt for a complete
rethink of financial stability and policies for achieving it. Over the course of the better part of a decade, a
deep and wide-ranging international regulatory reform effort has been underway, as great as any since the
Great Depression.
On cost grounds alone, a systematic rethink and reform of regulatory standards has been fully justified.
While the costs of the global financial crisis are still being counted, it seems likely they will be the largest
since at least the Great Depression. Two approaches are typically used to gauge these costs of crisis:
the cumulative loss of output relative to its trend and the cumulative fiscal costs of supporting the financial
system.1
 Let’s take these in turn.
Chart 1 looks at the path of output relative to a simple measure of its pre-crisis trend in the US, UK, France
and Germany after the Great Depression of 1929 and the Great Recession of 2008. In either case, it is
debatable whether estimated “pre-crisis trends” were sustainable, as they may have been artificially inflated
by credit booms. Nonetheless, it is clear that the output losses from both crises, relative to pre-crisis trends,
have been extremely large and long-lasting.
In the US, the level of output is currently around 13% below a continuation of its pre-crisis trend. Ten years
into the Great Depression, output was around 28% below its pre-crisis trend. Even if not quite on the scale
of the 1930s, the global financial crisis has imposed a huge opportunity cost on US citizens. In the UK, the
losses since the Great Recession, currently at around 16% of pre-crisis GDP, are larger than in the US and
indeed larger than those that followed the Great Depression. The crisis opportunity costs for UK and
euro-area citizens have been the highest for at least a century.2
Much the same picture emerges if we look at measures of the fiscal cost of crisis. Again, there are a number
of methods for gauging this cost. But one simple metric is to look at the pattern of government debt-to-GDP
ratios after the Great Depression and Great Recession, recognising that the larger part of the debt
sustainability cost of crisis typically arises from the denominator shrinking than from the numerator rising.
Chart 2 plots these debt-to-GDP ratios, again for the US, UK, France and Germany.
It suggests that, in the decade after the Great Depression, levels of government debt relative to GDP had
increased by around 28 percentage points in the US, 9 percentage points in Germany, but actually declined
in the UK. Since the Great Recession, levels of debt relative to GDP have increased by, on average,

1
 For example, Hoggarth, Reis and Saporta (2001).
2
 Figures in this paragraph calculate a continuation of pre-crisis GDP using the average growth rate of output in the 10 years preceding
the crisis.
All speeches are available online at www.bankofengland.co.uk/speeches
3
3
28 percentage points for the same set of countries. The fiscal cost of the Great Recession, at least on this
metric, is larger than during the Great Depression.
It is against this backdrop that policymakers internationally have engaged in a deep and wide rethink, rewrite
and reform of the global regulatory rules of the game. Wide, reflecting the multi-faceted nature of the
problems, market failures and market frictions exposed within the financial system during the crisis. Deep,
reflecting the severity of the hit to balance sheets, risk appetite and economic activity that the crisis has
inflicted and continues to inflict.
The next section reviews these regulatory reform efforts and their impact on bank balance sheets and market
metrics of banking risk. With a number of reforms yet to be fully enacted, it is too soon to be reaching
definitive conclusions. Nonetheless, some of the key questions thrown up by these regulatory reforms -
conceptual, empirical and practical - are now reasonably clear. We also have almost a decade’s worth of
additional evidence and research on which to draw when assessing these issues.
The following sections discuss some of those issues, drawing on new research and evidence: the calibration
of regulatory standards, balancing the costs and benefits of tighter regulation; the overall system of financial
regulation, balancing underlaps and overlaps, simplicity and complexity, discretion and rules; the impact of
reforms on incentives in the financial system, in particular incentives to avoid regulation; and the evolving
role of macroprudential regulation in safeguarding stability of the financial system.
The financial system is dynamic and adaptive. So any financial regulatory regime will itself need to be
adaptive if it is to contain risk within this system. In the terms used by Greenwood et al (2017), resilience
needs to be “dynamic”. As past evidence has shown, too rigid a regulatory system will soon become otiose.
And there are already calls, in some quarters and in some countries, for a rethink and rewrite of regulatory
rules on which the ink is barely dry.3
 This poses both opportunities and threats.
The opportunities arise from the need to keep the regulatory framework fresh and agile. With the best will in
the world, no one could say with certainty how the far-reaching and interwoven reforms to regulation over the
past decade will precisely land. Judging how the financial system might adapt to future trends in financial
technology is even harder to predict. As new evidence, incentives and innovation arise in the financial
system, the opportunity is created for regulators to learn and adapt the regulatory framework
(Carney, 2017a).
Equally, there are also threats to financial stability from any process of change. History is replete with
examples of regulatory standards being diluted or dismantled in the name of enhancing the dynamism of the
financial system and the economy. To follow this course unthinkingly would risk repeating regulatory

3
 For example, Calomiris (2017), Greenwood et al (2017), Duffie (2017).
All speeches are available online at www.bankofengland.co.uk/speeches
4
4
mistakes from the past, recent and distant. Only ten years on from the biggest crisis in several generations,
there are already some eerie echoes of those siren voices. With that in mind, we conclude with some
thoughts on issues which might be fruitful for future research on regulatory policy.
International Regulatory Reform
There are already a number of detailed accounts of the regulatory reforms undertaken by international
policymakers over the past decade (Carney (2017b), Duffie (2017), FSB (2017a), Greenwood et al (2017),
Sarin and Summers (2017), Yellen (2017)). The following is a summarised and simplified account of the
state of play. It partitions reform efforts into their microprudential and macroprudential components,
recognising that the two often overlap and are usually mutually reinforcing in their impact.
We focus here squarely on international banking regulation. We do not cover insurance regulation or
international accounting standards. We do not discuss regulatory reforms undertaken nationally, such as the
“Volcker Rule” in the US (Financial Stability Oversight Council (2011)) and the “Vickers proposals” in the UK
(Independent Commission on Banking (2011)). Nor do we discuss international reforms of market
infrastructure – for example, clearing – and financial market instruments (FSB (2017b)). Finally, we do not
cover changes to banks’ large exposures regime and a range of pay and governance reforms.
Microprudential Reform
Under the umbrella of Basel III, international reform of microprudential regulation has focused on four key
areas: capital, leverage, liquidity and resolution. Taking these in turn:
Reform of risk-based capital standards has focused on increasing the quantity and quality of capital held by
banks against their asset exposures. Minimum regulatory requirements for banks’ “core” (common equity)
capital have been raised from 2% under Basel II to 4.5% under Basel III, even for the smallest banks. And
allowable deductions to core capital have been reduced. On quality of capital, the types of financial
instrument eligible as loss-absorbing capital (including for Tier 1) have been tightened considerably. For
example, certain hybrid capital instruments are no longer eligible, as they were shown during the crisis to be
incapable of absorbing loss in situations of stress (Tucker (2013), Moody’s (2010)).
These reforms to capital standards have, encouragingly, been implemented in full by nearly all countries
internationally (FSB (2017a)). Comparing regulatory capital, pre- and post-reform, is not straightforward.
But taking together changes in both the quantity and quality of capital, it has been estimated that Basel III
raised risk-based capital standards for globally systemic banks by a factor of around ten (Cecchetti (2015)).
One of the new elements of the Basel III package was to supplement risk-weighted capital standards with a
risk-unweighted leverage ratio. Because this measure does not require banks or regulators to form a
All speeches are available online at www.bankofengland.co.uk/speeches
5
5
judgement on the riskiness of banks’ assets, it is in principle simpler, more transparent and less subject to
risk-weight arbitrage (Haldane and Madouros (2012)). Indeed, those were among the reasons a number of
countries, including the US and Canada, had a leverage ratio regime ahead of the crisis.4
 The Basel III
leverage ratio, set at a minimum level of 3% Tier 1, is due to be implemented internationally by 2018.
A second new element of the Basel III package was to augment solvency with liquidity-based standards.
Banks’ liquidity has long been a pre-occupation of the Basel Committee (Goodhart (2011)). But it took
wholesale liquidity runs on the world’s largest banks during the crisis to provide the impetus for
internationally-agreed liquidity standards. Under Basel III, these take the form of a liquidity coverage ratio
(LCR), designed to ensure banks have sufficient high-quality liquid assets to meet their 30-day liquidity
needs; and a net stable funding ratio (NSFR), designed to ensure banks’ funding profiles are sustainable.
The LCR has been implemented in full in most countries; the NFSR is due for implementation by 2018.5

During the crisis, a crucial missing ingredient from the financial regulatory architecture was found to be the
ability to wind up financial institutions in an orderly fashion – that is, while minimising disruption to financial
markets and the economy and without exposing tax-payers to risk (FSB (2014)). A number of measures
have been taken or are in progress to fill this gap, including the introduction of more effective national
resolution regimes for financial firms and greater cross-border co-operation and co-ordination when dealing
with international banks in situations of stress (FSB (2017c)).
Another element is to ensure banks have sufficient loss-absorbing liabilities which can be “bailed-in” in the
event of failure, to prevent losses being shouldered by tax-payers. The Financial Stability Board has agreed
standards for such Total Loss-Absorbing Capacity (TLAC) for global systemically-important banks (G-SIBs).
These standards are to be phased-in over coming years, to reach a minimum level of 16% from 2019, and
18% from 2022, of the resolution group’s risk-weighted assets, as well as 6% and 6.75% on a leverage
exposure basis, respectively.
Macroprudential Reform
These new or augmented microprudential standards have been supplemented with a set of new
macroprudential measures. These focus on safeguarding the stability of the financial system as a whole
(Tucker (2009), Bank of England (2009, 2011)). The most significant of these reforms have focused on three
areas: macroprudential capital buffers; stress-testing; and shadow banks.
Historically, capital standards have been static requirements. As part of Basel III, a new time-varying
component of banks’ capital was added – the counter-cyclical capital buffer (CCyB). This recognises that

4
 And a number of countries, including the UK, introduced leverage ratio capital requirements in the aftermath of the crisis.
5
 For LCR, see BCBS (2013a): http://www.bis.org/publ/bcbs238.pdf; for NSFR, see BCBS (2014b):
http://www.bis.org/bcbs/publ/d295.htm
All speeches are available online at www.bankofengland.co.uk/speeches
6
6
risks to the financial system vary over the credit cycle, typically being highest at its peak and lowest at its
trough. The CCyB aims to counteract somewhat that time-varying risk profile, with additional capital required
during the upswing which can be released during the downswing. There is international reciprocity in the
setting of the CCyB to reduce incentives for cross-border arbitrage (BCBS (2010a)). The framework has
been implemented in most jurisdictions.
Similarly, one of the key lessons of the crisis was that some institutions impose greater degrees of risk on the
system because of their size, complexity or interconnectedness (FSB (2010)). Basel III recognises the need
for these systemically-important firms to carry a structurally higher capital requirement, currently of up to
3.5%, to help mitigate the additional risk they bring to the system. These capital add-ons apply to the 30
designated global systemically-important banks (G-SIBs) and the roughly 160 domestic systemicallyimportant banks (D-SIBs), to be phased-in between 2016 and 2019.
Stress tests were used by regulators before the crisis to assess whether banks had sufficient capital to
withstand an adverse tail event. But these tests tended to be neither comprehensive nor transparent. In
2009, the US authorities undertook a comprehensive stress test of the major US banks and published the
results. For banks failing the test, regulatory restrictions on their behaviour were imposed. For some people,
this marked the turning point for the US financial system. A comprehensive annual stress-testing exercise is
now undertaken in the US.6
 More recently, the US has been joined by the UK and the EU, among others.7
Finally, one of the striking features of the pre-crisis financial system was the emergence of the so-called
“shadow” banking system. In the US, on some definitions, this grew to exceed in size the conventional
banking system (Pozsar et al (2010)). Since the crisis, reform efforts have focused on two areas. First,
specific reforms have been enacted to sectors which, during the crisis, were found to contain fault-lines - for
example, Money Market Mutual Funds (IOSCO (2012)). Second, a framework has been put in place by the
FSB to define and measure shadow banking entities, to publish data on them to enhance market discipline
and to help authorities identify, and develop policy tools for mitigating, the risks they might pose (FSB
(2013a)). The FSB have recently put forward a package of recommendations to address structural
vulnerabilities from the asset management sector (FSB (2017d)).
Supporting this package of regulatory reforms, micro- and macroprudential, have been initiatives to boost the
quantity and quality of reporting by financial institutions. These should help in pricing institution-specific risk
by financial markets and ratings agencies. Notable initiatives have included: enhanced Pillar 3 disclosures
by banks, covering all aspects of the regulatory reform agenda; and the work of the Enhanced Disclosure
Task Force (EDTF), a private sector group established by the FSB. Over time, this has led to increased
compliance with the EDTF disclosure template (Chart 3).

6
 The Comprehensive Capital Analysis and Review or CCAR.
7
 Dent and Westwood (2016) includes a comparison of international concurrent stress-testing practices.
All speeches are available online at www.bankofengland.co.uk/speeches
7
7
Balance Sheet Impact
So what has been the impact of these regulatory reform measures on banks’ overall resilience? One simple
set of resilience metrics focusses on bank balance sheet measures of solvency and liquidity. Comparisons
of international banks’ balance sheets are made difficult by changes over time in both the definitions of
variables and the sample of banks. We consider a panel of international banks, designated as either global
systemically-important (G-SIB) by the FSB in 2016, or domestic systemically-important (D-SIB). This gives a
panel of 30 G-SIBs and about 160 D-SIBs.8
 For each bank, we consider two solvency-based metrics
(leverage and risk-weighted capital) and two liquidity-based metrics (a simple liquid asset ratio and the ratio
of loans to deposits). These measures do not map precisely to Basel definitions.9
Chart 4 looks at a measure of banks’ Tier 1 risk-weighted capital ratios. For both G-SIBs and D-SIBs in our
sample, these have risen significantly over the past decade, almost doubling from around 7-8% to around
13-14%. A very similar picture emerges for leverage ratios (Chart 5). These have also roughly doubled over
the past decade, from around 3% to around 6%. On these metrics, there has been a material strengthening
in solvency-based standards among systemically-important banks over the past decade. This is also the
case for measures of TLAC (see Chart 6 for a sample of UK banks).
Liquidity metrics show a similar pattern of improvement. For example, liquid asset ratios - high-quality liquid
assets as a fraction of the total balance sheet – have risen from around 6% in 2008 to more than 8% (Chart
7), though the increase is more muted for D-SIBs. Meanwhile, the ratio of loans to deposits (LTD) has also
improved, with lending backed by a larger share of stable sources of funding than before the crisis (Chart 8).
Market-Based Metrics
A second set of metrics of bank solvency and liquidity focus on financial market perceptions of bank risk.
There are a wide variety of potential such metrics, each with their own imperfections, including measures of
default such as CDS spreads, bond yields and ratings; measures of volatility, such as option-implied
volatilities; and measures of profitability, such as price-earnings ratios. These are summarised and
evaluated in Sarin and Summers (2016).

8
 These banks have been identified based on publicly available lists of systemically-important firms:
• The Financial Stability Board’s list of G-SIBs as of 21 November 2016.
• O-SIIs notified to the European Banking Authority as of 25 April 2016.
• US bank holding companies (BHCs) subject to the Federal Reserve’s annual Comprehensive Capital Analysis and Review
(CCAR) as of March 2014.
• Banks designated as systemically-important financial groups by the Swiss National Bank.
• The four major banks in Australia.
• The five largest banks in Canada.
These include bank holding companies as well as their primary operating companies where applicable, as well as foreign subsidiaries
that are explicitly designated as systemically-important for a particular country.
9
 The data are from the Standard and Poor’s Capital IQ database.
All speeches are available online at www.bankofengland.co.uk/speeches
8
8
Chart 9 plots a measure of default – CDS spreads – for a panel of G-SIBs. It shows a familiar pattern of
pre-crisis under-pricing of risk; a rapid re-pricing of default risk during the crisis; and a subsequent partial
unwind. CDS spreads today sit roughly midway between their pre-crisis and mid-crisis averages.
Bank bond spreads and ratings tell a similar story. Assuming pre-crisis banking risk was materially
under-priced, this evidence is consistent with regulatory reform having boosted the resilience of the global
banking system.
At the same time, measures of bank volatility and profitability have seen fewer signs of recovery. Chart 10
plots a measure of the price-to-book ratio of G-SIBs and D-SIBs. This currently lies well below its historic
average and little different than unity. Put differently, if we used a measure of banks’ capital ratios using the
market rather than the book value of their equity, this would suggest a far smaller degree of improvement in
measured bank solvency and resilience (Chart 11), though the effect is less pronounced for D-SIBs.
Sarin and Summers (2016) reconcile these market movements by appealing to the shifts in the franchise
value of banks. Improved solvency standards have decreased the perceived default risk of banks. But
coincident with lower risk are lower returns to banks’ activities, due to the combined effects of stricter
regulation, misconduct fines, low levels of interest rates and increased competition. This leaves banks a
riskier proposition for equity investors than before the crisis, as the residual claimant on profits. But, by and
large, improved solvency standards have reduced risk among bond-holders and depositors in banks.
“By and large” because, accompanying these changes in banks’ capital standards, has been a move
towards putting losses from default onto bond-holders. This can be seen in the evolution of the implied
“support ratings” given to banks by rating agencies. In 2010, holders of the major UK banks’ debt enjoyed
around 4 notches of implied ratings uplift owing to expectations of government support (Chart 12). By 2016,
that had fallen to less than one notch of support. A similar pattern is evident among other global banks.
Calibrating Regulatory Standards
Is the calibration of these new regulatory standards too tough, too lax or just right? That has been among
the most animated of the regulatory debates over the past decade. One standard for comparison is historical
experience. There has been a significant evolution in the levels of both capital and liquidity ratios of the
major banks over the past century. Chart 13 plots a measure of the leverage ratio for the UK and US
banking systems over a long historical sweep,10 while Chart 14 plots a simple measure of the liquidity ratio
for UK banks over the past half-century (see also Jordà et al (2017)).
Both solvency and liquidity ratios have exhibited a long, downwards drift. Between the end of the

10
 Chart 13 is not directly comparable with Chart 5 because it is based on a different definition of leverage and sample of banks.
All speeches are available online at www.bankofengland.co.uk/speeches
9
9
19th century and the troughs prior to the financial crisis, leverage ratios fell by around three quarters in the
US and the UK. Liquid asset ratios among UK banks underwent an even larger fall in less than half the
time. Given the scale of these falls, even with the regulatory reforms of the past decade, levels of capital and
liquidity in the banking system are at levels significantly below those 100 and 50 years ago, respectively.
On the face of it, this gives grounds for questioning whether even these revamped regulatory standards are
sufficient to withstand likely future shocks. We should, however, probably be cautious about jumping too
quickly to that conclusion. Over the past century, there has been significant change in the structure of the
financial system, including in the structure, scale and scope of financial regulation and the safety net. Those
changes could mean that simple, historical comparisons of regulatory standards are misleading.
Admati and Hellwig (2013) provide a comprehensive and lucid account of the case for higher capital
standards. Their argument centres on the fact that the impact of higher capital standards on banks’ overall
cost of capital needs to take account of the lower risk that arises from this shift - the Modigliani-Miller offset
(Modigliani and Miller (1958)). It needs also to distinguish between any private costs to banks from tighter
regulation and the social benefits this confers, with the latter the key public policy yardstick.
When it came to re-calibrating regulatory standards for capital and liquidity after the crisis, international
regulators engaged in a detailed, quantitative exercise which sought to weigh these social costs and benefits
of tighter regulation, drawing on existing empirical evidence. The Long-Term Economic Impact (LEI) study,
published by the Basel Committee in 2010, is a useful starting point for discussion of the appropriate
calibration of regulatory standards (BCBS (2010b)).
The main conclusion from this work was that, under conservative assumptions about likely economic costs,
there were positive economic benefits to society from a sizeable increase in the capital banks were required
to maintain. The study did not settle on an optimal level of bank capital. But the results presented were
consistent with societal benefits peaking at a Tier 1 risk-weighted capital ratio of between 16-19%.11
 This is
north of most global banks’ current capital ratios.
The range of published estimates in the LEI study reflected different assumptions about the persistence of
the effects of crises on GDP, an area of particular empirical uncertainty in the academic literature. A
contemporaneous study by Miles et al (2013) concluded that optimal capital requirements were likely to be
higher – perhaps around 20% - if account was taken of the offsetting risk and cost of capital effects of higher
solvency standards (the Modigliani-Miller offset).
It is useful to revisit the calibration in the LEI study in the light of subsequent research. A little notation may
be useful to organise this evidence. Suppose the aim of policy is to keep output in the economy, y, as close

11 These figures are expressed in terms of current definitions of capital and risk-weighted assets. The mapping from the estimates
reported in the LEI report and those above are due to Brooke et al (2015).
All speeches are available online at www.bankofengland.co.uk/speeches
10
10
as possible to its trend growth path, y̅. The objective for the authorities is then to minimise a loss function,
which can be written as:
L = (yt − y̅t
)
2
Let’s simplify further and assume two factors can cause output to deviate from its trend: first, higher capital
requirements, k, which act to reduce output each period by δ; and second, the occurrence of a financial
crisis which, with probability γ, leads to a discrete drop in output of ∆. That is:
yt = y̅t − δk− γ(k)Δ(k)
This captures the view that higher bank capital could reduce credit supply, and hence economic activity, in
the near term. But by making the financial system more resilient to future shocks, it may also reduce the tail
risk of bad macroeconomic outcomes.
Both probability and severity of crises are influenced negatively by the level of bank capital, with the
relationship likely to be convex (γ
′
(k) < 0, γ
′′(k) > 0, ∆
′
(k) < 0, ∆
′′(k) > 0) – that is to say, one would expect
a one percentage point increase in the capital ratio to have a larger dampening impact on the probability and
severity of crisis when banks are close to their regulatory minima than when capital buffers are plentiful.
In this stylised set-up, the marginal condition that defines optimal bank capital is:
δ = −Δ
∂γ
∂k
− γ
∂Δ
∂k
Optimal capital is higher the lower is δ, the economic cost of a marginal increase in capital requirements; the
greater are γ and Δ, the likelihood and severity of crises; and the greater are ∂γ
∂k
and ∂Δ
∂k
, the marginal effects
of capital on the likelihood and severity of crises. So what have we learned over the past decade about the
likely magnitude of these parameters?
The Benefits of Higher Capital Requirements
The assumptions underpinning the marginal benefits of higher capital in the LEI study were as follows:
banking crises occur, on average, once every 20-25 years; the median estimate of the cumulative
discounted costs of a crisis is around 60% of annual pre-crisis GDP; each percentage point increase in the
capital ratio reduces the probability of a banking crisis by a smaller amount, ranging from 1.4% to 1% (for a
capital increase from 10% to 11%) to 0.4% to 0.3% (for a capital increase from 14% to 15%); and, finally,
the level of bank capital has no impact on the severity of crisis.
All speeches are available online at www.bankofengland.co.uk/speeches
11
11
Since the LEI report, a rich seam of the literature has emerged on the determinants of crises and their
severity (∆). Some of the most illuminating pieces of this research have drawn on evidence from a long
historical time-series and across multiple countries (for example, Jordà et al (2013), Taylor (2015)). The key
findings are as follows.
First, credit booms are probably the single most important determinant both of the likelihood of crises and of
economic performance in the recovery after them (Schularick and Taylor (2012), Jordà et al (2013)). A
sustained 1 percentage point increase in the credit-to-GDP ratio raises the probability of crisis from 4% to
around 4.3% per year. It also raises the severity of a crisis, with real GDP per capita almost 1% lower after
five years.
12
 Colleagues at the Bank of England have considered whether it is the level of credit, or its
growth, prior to a crisis that matters most for subsequent economic performance (Bridges et al (2017)). They
find that credit growth has historically been a significant predictor of crisis severity, whereas the level of
indebtedness appears less important.
Second, not all forms of credit are equal. In the post-WWII era, mortgage credit growth has been the
dominant driver of financial crisis risk. And growth in mortgages, rather than in other forms of credit, is the
key determinant of the drag in the recovery phase from crisis (Jordà et al (2017)). Third, asset prices are
also important with ‘leveraged bubbles’ – synchronised house price and mortgage credit booms - particularly
dangerous (Jordà et al (2015)).
Taken together, this evidence is consistent with the probability (γ) and output costs of credit crises (∆) being
at least as large as assumed in the original LEI study, perhaps larger, given the still-high levels of the
credit-to-GDP ratio in most countries, and the monetary and fiscal space available to the authorities at
present relative to the average of the past – a recent paper by Romer and Romer (2017) presents evidence
that this factor is a significant determinant of crisis severity. The still-accumulating output losses during the
recovery phase from this time’s crisis would also point in this direction (Chart 1).

What role does higher bank capital play in reducing the likelihood of financial crises (∂γ
∂k
) or their severity (∂∆
∂k
)?
At least for the likelihood of crisis, subsequent evidence has tended to be rather ambiguous. Historical
evidence, using aggregate economy-wide covariates, has reached the perhaps surprising conclusion that
bank capital ratios have virtually no predictive power for the occurrence of financial crises in major advanced
economies (Jordà et al (2017)). That is, ∂γ
∂k
is indistinguishable from zero. This result holds both in the full
sample (1870-2013) and in the post-WWII period.
Micro-econometric studies on the link between bank failure and bank capital have found a more tangible
relationship, however. For example, Vazquez and Federico (2015) find that US and EU banks with stronger

12
 This echoes and extends findings from earlier research by Borio and Lowe (2002), Borio and Lowe (2004) and Drehmann, Borio and
Tsatsaronis (2011), which found credit gap measures to be key determinants of crisis risk.
All speeches are available online at www.bankofengland.co.uk/speeches
12
12
pre-crisis capital and structural liquidity positions were less likely to fail. Berger and Bouwman (2013) report
a similar finding using a longer-run data set of US banks. And a recent study by IMF economists finds that
risk-based capital ratios in the range 15-23% would have been sufficient to absorb losses in the vast majority
of past advanced economy banking crises (Dagher et al (2016)).13

At the time of the Basel Committee’s study, there was little evidence on the impact of bank capital on the
severity of crises (∂∆
∂k
), which is why this channel was ignored in the quantitative calibration. That has since
changed. Jordà et al (2017) find that, while bank capital does not prevent a crisis from occurring, it matters
for the pain suffered in its aftermath. They find that real GDP per head is 5 per cent higher 5 years after the
onset of a crisis-related recession if bank capital is above its historical average when the crisis hits.
The benefits of capital in reducing the severity of crisis are also borne out by experience since the crisis.
Chart 15 plots international banks’ capital ratios prior to the crisis against their subsequent lending growth.
The relationship has a statistically significant upward slope. Banks that entered the crisis with higher capital
have, on average, been better able to continue their lending. On average, each extra 1 percentage point of
pre-crisis capital boosted banks’ cumulative lending over the subsequent decade by over 20%.
This finding is corroborated by micro-econometric evidence. Carlson et al (2013) find that US banks with
higher pre-crisis capital ratios had stronger loan growth in its aftermath, with the effect particularly
pronounced at lower capital ratios. Cornett et al (2011) and Kapan and Minoiu (2013) report that banks
relying more heavily on stable sources of funding, such as core deposits and equity capital, continued to lend
relative to other banks during the crisis. And Jimenez et al (2014) find that, in periods of economic
weakness, loan applications were less likely to be rejected by Spanish banks that were well-capitalised.
A recent paper by Bank of England colleagues identifies a distinct channel through which bank capital affects
crisis severity (Tracey, Schnittker and Sowerbutts (2017)). They use banks’ misconduct fines as a novel
instrument to identify exogenous negative bank capital shocks. They find that banks respond to such shocks
by relaxing their lending standards, as measured by the loan-to-value and loan-to-income ratios on new
mortgages. This is likely to increase their vulnerability to future shocks, increasing crisis severity.
This evidence suggests that some of the benefits of higher capital requirements may have been understated
in the original LEI study, with implications for the range of optimal capital requirements. For example, if we
assumed that every percentage point of extra capital increased the level of real GDP each period in the

13
 Relatedly, Demirguc-Kunt et al (2010) and Beltratti and Stulz (2012) find that poorly capitalised banks had lower stock returns during
the financial crisis. And Boyson et al (2014) find that banks that entered the recent financial crisis with higher capital were less likely to
see their funding dry up during the crisis.
All speeches are available online at www.bankofengland.co.uk/speeches
13
13
aftermath of a crisis by 0.1% – broadly consistent with the evidence here - this would raise optimal capital
requirement by around 2 percentage points, other things equal.14
Working in the opposite direction, however, have been developments in resolution arrangements and new
standards for TLAC. No account was taken of these in the LEI study. But if TLAC can be credibly bailed-in,
including for systemically-important institutions, this would tend to reduce both the likelihood and severity of
future crises.15
 It may also discipline banks’ management, avoiding them taking excessive risks in the first
place. Some studies suggest this market discipline effect could be material, reducing the likelihood of a
financial crisis by as much as 30% (Afonso et al (2015), Brandao-Marques et al (2013)).
Colleagues at the Bank of England (Brooke et al (2015)) have estimated that, if these measures of the
beneficial incentive effects of TLAC and credible resolution regimes are correct, and if increased resolvability
in addition reduces the cost of crises by around 60%,16 then optimal capital ratios for the UK banking system
could be up to 5 percentage points lower than would otherwise be the case.
A recent study by economists at the Federal Reserve Board (Firestone et al (2017)) also considers the
impact of improved resolution arrangements. They use estimates from Homar and van Wijnbergen (2016) to
model a reduction in the expected duration of crises from such arrangements. Overall, they find that optimal
capital levels for the US banking system can range from 13% to 25%.
The Costs of Higher Capital Requirements
The costs of higher bank capital requirements arise from potentially tighter credit supply conditions. Banks
may adjust to the need to fund themselves with more equity by tightening lending rates and restricting loan
volumes. The LEI study assumed that each percentage point increase in the capital ratio would raise loan
spreads by around 13 basis points. That translated into a fall in GDP of around 0.1% relative to trend.17

What have we learned about these costs since the LEI study? Cecchetti (2014) documents how banks have
adjusted their balance sheets and credit provision since the introduction of Basel III. He finds that banks
increased their capital ratios significantly, by over 4 percentage points on average, across his sample. Net
interest margins and profitability fell. But with the exception of European banks, banks’ assets increased,
their lending spreads narrowed, lending standards eased, and the ratio of bank credit-to-GDP went up.

14 This calculation is based on the marginal condition for optimal capital reported earlier. We parameterise the crisis probability and
severity functions as follows: 𝛾 = exp(𝛽0 + 𝛽1𝑘)⁄(1 + exp(𝛽0 + 𝛽1𝑘)); ∆= 𝜃0 + 𝜃1𝑘. The model is calibrated to deliver an optimal capital
ratio of around 18% when 𝜃1 = 0, i.e. the LEI case. We achieve this by setting 𝛿 = 0.1, 𝛽0 = 0.5, 𝛽1 = −0.2, and 𝜃0 = 10, that is to say, a
crisis reduces the level of GDP by 10% relative to baseline. If instead we set 𝜃1 = −0.1, such that each percentage point increase in
capital reduced the GDP hit in a crisis by 0.1%, the optimal capital ratio increases to over 20%.
15 See Cunliffe (2017) and Bank of England (2017) for discussion of resolution.
16 This estimate is based on the difference in the estimated cost of crises across their sample depending on whether they occurred
under more or less credible resolution regimes.
17 Admati and Hellwig (2013) have forcefully questioned the basis for assuming such costs, given that standard finance theory would
predict that the cost of debt and equity funding for a bank will decline in response to an increase in its capital position.
All speeches are available online at www.bankofengland.co.uk/speeches
14
14
A recent paper from the BIS (Gambacorta and Shin (2016)) reaches a similar conclusion. It finds that banks
with higher unweighted capital ratios have tended to have higher loan growth, with each one percentage
point increase being associated with higher subsequent lending growth of 0.6 percentage points per year.
This evidence is consistent with the macroeconomic costs of higher bank capital being lower than assumed
in the Basel LEI study. Indeed, taken at face value, it would suggest there have been virtually no costs of
achieving higher levels of capital across the global banking system, at least among most global banks.
While credit conditions have clearly improved since the crisis, it is possible that the recovery in lending might
have been stronger still had capital requirements risen by less. To begin to analyse that question, Chart 16
compares the change in bank capital since Basel III was introduced with subsequent lending growth among
a panel of large international banks. On average, lending growth has been positive over this period,
consistent with Cecchetti (2014).
But credit growth has also tended to be statistically significantly lower among banks that have seen the
largest increase in their capital ratios. On average, banks that have increased their capital ratios by an extra
one percentage point have provided 4% less in cumulative credit since Basel III was introduced (3.5% less if
we exclude European banks). This is very similar to the estimates used by the FSB’s Macroeconomic
Assessment Group (2010), which reported a range of estimates from -0.7% to -3.6%.
There are of course different possible interpretations of this negative relationship. Banks facing weak
macroeconomic conditions may simply have seen a reduction in loan demand and responded by maintaining
higher capital buffers on a voluntary basis. To parse these conflicting interpretations, we turn to recent
econometric evidence on the impact of higher capital requirements.
Aiyar et al (2014, 2016) find that shifts in required capital had large negative effects on UK banks’ lending
decisions. De-Ramon et al (2016) report a similar finding, noting that this has, if anything, increased since
the crisis. Bahaj et al (2016) find that, in times of credit expansion, higher required capital has only a minimal
effect on lending. But when credit growth is weak, higher required capital can result in a large reduction in
lending. This echoes previous research which has found that banks reduce lending in response to negative
capital shocks (Peek and Rosengren (1995)).
Lower lending was one cost of higher equity considered in the LEI study. A second potential cost, not
considered by the LEI study, was the potential for falls in market liquidity in core financial markets - for
example, securities financing markets such as repo. This could potentially raise the cost of capital for users
of these markets. Market commentary in recent years has often laid the blame at the leverage ratio. This, it
is argued, has led some dealer-banks to reduce their inventory holdings and market-making capacity,
thereby reducing secondary market liquidity in some markets.
All speeches are available online at www.bankofengland.co.uk/speeches
15
15
There are of course a variety of other reasons why banks’ willingness to make markets, and why market
liquidity more generally, might have been affected by the crisis – for example, reduced risk appetite and
increased counterparty risk. Moreover, it was plausibly the case that pre-crisis liquidity may have been too
plentiful and too cheap in some financial markets, so some correction in the quantity and pricing of liquidity
was to be expected, and indeed was potentially desirable, from a welfare perspective.
Research at the Bank of England has sought to identify the impact of leverage ratio requirements on the
functioning of UK government bond (‘gilt’) and gilt repo markets, using transaction-level data (Bicu et al
(forthcoming)).18 It does find some causal impact of the leverage requirement on various metrics of liquidity,
a worsening that is particularly acute at quarter-ends. Significantly, the banks most constrained by the
leverage ratio reduced their activity in financial markets most.
At the same time, however, dealers unaffected by the leverage ratio requirement also reduced their liquidity
provision and, if anything, by more. This suggests factors other than the leverage ratio may have been at
work in curtailing liquidity in these markets. It also leaves open the question of whether the correction in
liquidity, even if privately costly, came at any social cost. Baranova, Liu and Shakir (2017) assess the costs
that could arise from regulation which affects market liquidity at different levels of stress. They find higher
costs in benign conditions, but substantial benefits in situations of stress as dealers make markets for longer.
Overall Implications for Optimal Capital
How do these research findings tilt the optimal bank capital calculus relative to the LEI study? Table 1
summarises the evidence. They are a mixed bag. On the benefits side, there is now stronger evidence on
the costs of credit booms and the role of capital in constraining the severity of the downturn in the aftermath
of these booms. It also suggests that the costs of raising extra capital are no larger, and may well be
smaller, than originally anticipated. This strengthens the hand of macroprudential authorities when tightening
capital requirements during a credit boom. Other things equal, it would also increase quantitative estimates
of banks’ optimal capital ratio.
On the other side of the ledger, the LEI study did not anticipate two factors. First, the role of TLAC in
augmenting banks’ capital base in situations of stress, potentially reducing the probability and severity of
crises. Second, higher capital requirements could impose liquidity-related costs on the financial system,
though their scale (and whether they are a social cost) remains open for debate. These arguments, in
particular around resolution, have been used by policymakers in some countries, including the UK, when
coming the view that capital requirements should be lower than in the original LEI study. For example,
having assessed all the factors and evidence within Table 1, the Bank of England’s Financial Policy
Committee judged that the appropriate structural level of Tier 1 equity in the system would be 13 ½% of

18 See the Financial Policy Committee’s June 2016 Financial Stability Report (pp 27-33) for an assessment of market liquidity in UK
markets more broadly. The Securities and Exchange Commission’s Report to Congress contains a detailed assessment of the impact
of Basel III and the Volcker Rule on liquidity in US Treasury and corporate debt markets (SEC (2017)).
All speeches are available online at www.bankofengland.co.uk/speeches
16
16
risk-weighted assets (Bank of England (2015c)).
Table 1: Overall Implications of Research Findings for Optimal Capital
Impact on optimal capital:
Benefits:
Likelihood and severity of crises
Impact of capital on probability of crises
Impact of capital on severity of crises
Impact of TLAC and resolution regimes on prob. and severity of crises
Costs:
Impact of capital on credit conditions and growth
Impact of capital on market liquidity (leverage ratio) in normal
conditions
The System of Financial Regulation
Regulatory reform has tended to progress crisis by crisis, market failure by market failure, regulatory
standard by regulatory standard. This is not especially surprising, given the nature of the policy design
process. Nonetheless, if we put together the various pieces of recent regulatory reform, we find a
fundamentally different regulatory jigsaw, or system of financial regulation, than in the past.
One important dimension of that new architecture is the significantly larger number of regulatory rules or
constraints that now operate. On top of risk-based capital standards have been added regulatory rules for
liquidity, leverage and loss-absorbing capital. In other words, we have moved from a system of largely
uni-polar regulation to multi-polar regulation (Haldane (2015)). Some individual parts of the regulatory
rulebook - such as the use of internal ratings-based risk weights - also remain complex.
The new regulatory architecture has also introduced measures which are likely to make for a greater degree
of regulatory discretion. The authorities in the US, UK and euro area have moved to annual stress-testing
exercises in which the stress scenario, modelling framework, success criteria and regulatory response are
each subject to significant degrees of regulatory discretion. Regulators internationally are also now setting a
CCyB requirement, which is also set in a largely discretionary fashion.
All speeches are available online at www.bankofengland.co.uk/speeches
17
17
In short, the new regulatory framework involves a larger number of regulatory constraints, many of which are
individually complex, operating with a greater degree of regulatory discretion than in the past. Some have
questioned whether this system may be too complex (for example, Admati and Hellwig (2011)).
And some of the recent debate on regulatory reform in the US also raises those same concerns (US
Department of the Treasury (2017)).
There are several different dimensions to regulatory complexity. Much has already been written on the
complexity of individual rules or regulatory constraints and the associated potential for regulatory arbitrage
(Haldane and Madouros (2012), Aikman et al (2014), Behn, Haselmann, and Vig (2016)). The Basel
Committee’s Task Force on Simplicity and Transparency are looking into these questions at a practical level.
We do not explore those issues further here.
Instead, we focus on two other dimensions of the system of financial regulation: (i) the number of regulatory
constraints; and (ii) the extent of discretion around each individual regulatory rule.
(i) The Number of Regulatory Constraints
Although the post-crisis architecture places many regulatory constraints on banks, the key going-concern
constraints are risk-weighted capital requirements (RWCR), the leverage ratio (LR), the liquidity coverage
ratio (LCR) and the net stable funding ratio (NSFR). We assess these four constraints, recognising that
other aspects of the regulatory system might also impose binding constraints on banks. For example, stress
testing can be interpreted as holding banks to a different RWCR standard and a potentially different overall
capital calibration (Greenwood et al (2017)).
Some have recently contended that this multi-constraint system of financial regulation might be
over-identified, with potentially distortionary implications for banks’ business models and behaviour. For
example, Greenwood et al (2017) argue that it may be distortionary and unnecessary to have multiple,
independent constraints on banks’ behaviour. And Cecchetti and Kashyap (2016) suggest that the LCR and
NSFR are strongly overlapping in their impact, so that both may not be needed.
These are well-reasoned critiques of the new regulatory framework whose messages should be analysed
carefully when evaluating the new framework. They are just the sort of academic challenge to regulatory
orthodoxy which was so missing in the pre-crisis period. Nonetheless, it is also worth reminding ourselves
why and how such a multiple-constraint framework was arrived at in the first place. At a conceptual level,
three arguments could be used to justify such a multi-pronged approach.
First, banks are subject to multiple sources of risk or balance sheet fault-line. Historical experience suggests
they fail for a variety of different reasons. To misquote Tolstoy, while sound banks tend all to be alike,
unsound banks tend to be unsound in their own way. At least in principle, this could point to the need for
All speeches are available online at www.bankofengland.co.uk/speeches
18
18
different types of regulatory constraint to counter different balance sheet fault-lines: one instrument for each
market failure. This is, if you like, the Tinbergen Rule as it applies to financial regulation (Tinbergen (1952)).
Second, uncertainty as well as risk is pervasive in the financial system. These Knightian (1921) uncertainties
have multiple sources - measurement of the risks banks face, how contagion propagates across the financial
system and how regulatory actions affects behaviour, to name but three. A portfolio of regulatory tools can
be seen as a means of offering insurance against these uncertainties. This is, if you like, the Brainard Rule
as it applies to financial regulation (Brainard (1967)).
Third, any individual regulatory constraint creates incentives for banks to respond in ways which may seek to
avoid or arbitrage the rules. In the next section, we discuss how having multiple regulatory constraints might
mitigate this risk. In this section, we discuss the conceptual case for multiple regulatory constraints before
presenting some new empirical evidence. Table 2 summarises some of the key arguments.
Table 2: Assessment of the relative suitability of Basel III standards to address selected forms of risk
Risk First Best Mitigant Second Best Mitigant Less effective Mitigants
Microprudential
solvency risk –
‘true’ asset risk
RWCR: Requires loss
absorbing capital to
cover solvency risks. If
risk can be measured
and risk weights can be
chosen appropriately,
this allows for the
greatest level of
granularity.
LR: Provides loss
absorbing capacity but
does not include any
risk granularity by
design.
LCR & NSFR: Neither
ratio attempts to mitigate
the risk of losses.
Microprudential
solvency risk –
‘unknown’ asset
risk under
Knightian
uncertainty
LR: Effective when risks
are unknowable and
cannot pinpoint
particular asset classes
of concern, especially in
the face of limited
historical data or fattailed loss distributions.
RWCR: Provides loss
absorbing capacity but
may perform less well
out-of-sample and
vulnerable to model risk
(IRB approach) or
miscalibration of risk
weights (standardised
approach).
LCR & NSFR: Neither
ratio attempts to mitigate
the risk of losses.
Vulnerability to
risk shifting
arbitrage
RWCR: High degree of
granularity reduces the
scope for risk-shifting
LCR & NSFR:
standardised
assumptions mitigate
some scope to shift risk
but also allow some
scope for distortion if
weights are
miscalibrated.
LR: Greatest scope for
distortion through risk
shifting because of lack of
risk sensitivity.
Vulnerability to
gaming
LR: Lack of granularity
and degrees of freedom
minimises gaming
opportunities.
LCR and NSFR: Small
number of modelled
assumptions offer some
safeguard against
gaming.
RWCR: High degree of
freedom offered to banks
increases incentives for
gaming, especially under
IRB approach.
All speeches are available online at www.bankofengland.co.uk/speeches
19
19
Risk First Best Mitigant Second Best Mitigant Less effective Mitigants
Rapid and
unsustainable
balance sheet
expansion
LR: Requires banks to
raise capital to support
credit creation,
regardless of asset
composition.
NSFR: Limits reliance
on short and mediumterm wholesale funding
to support balance
sheet expansion.
RWCR: Susceptible to
expansion into assets with
low measured risk. Places
no constraint on debt
funding.
LCR: 30 day time horizon
only limits the expansion
funded by very short term
liabilities.
Sudden
withdrawal of
funding due to
firm-specific or
short-lived
market-wide loss
of credibility
LCR: Assures available
buffer of liquid assets to
meet immediate outflows
enabling survival of first
stages of run/preparation
for resolution if
appropriate.
NSFR: Reduces
runnable fraction of
liabilities, thus
decreases ex-ante risk
of being exposed to a
run, but does not
directly ensure bank has
a buffer of usable shortterm liquidity.
RWCR & LR: Higher
capital should in principle
help banks retain funding,
but does not provide a
cushion if a run occurs.
Sustained loss of
funding due to
market-wide
liquidity stress
leading to slowburn insolvency
NSFR: Matches liquidity
of assets against stability
of liabilities to ensure
bank is broadly resilient
to a medium-term
funding run.
LCR: Assures available
buffer of liquid assets to
meet immediate
outflows, but not that
maturity transformation
is sustainable beyond
30 day horizon.
RWCR & LR: Require
small fraction of liabilities
to be non-runnable equity
but a small amount
relative to illiquid assets.
Crystallisation of
systemic liquidity
risk leading to a
fire sales,
liquidity hoarding,
and/or a
contraction in
lending
NSFR: Reduces banks’
vulnerability to mediumterm liquidity risks and
hence the probability of
them being required to
deleverage rapidly in
periods of stress to
shore up their liquidity
position.
LCR: Reduces reliance
on the most unstable
short-dated liabilities.
Risk that banks
liquidating buffers to
meet outflows in a
stress could exacerbate
a fire sale.
RWCR & LR: Do not
directly mitigate the
likelihood of deleveraging
due to liquidity problems.
Capital and Leverage
The objective of the capital framework is to ensure banks have sufficient capital to absorb unexpected losses
and continue lending in situations of stress. RWCRs oblige banks to assign granular risk weights to their
assets. If true risk of an asset can be estimated accurately – it is a “known known” - then the RWCR is
typically better suited than the LR to guarding against solvency risk (Gordy (2003)). Greenwood et al (2017)
conclude “the social optimum can be implemented with a single requirement that each bank maintain a
sufficient ratio of equity to risk-weighted assets, provided the risk weights are chosen appropriately”.
The last part of this sentence is, however, an important proviso. One key question is whether risks in the
financial system are likely to be known with sufficient certainty that they can be estimated meaningfully and
accurately. Based on historical experience, that assumption cannot be taken for granted when it comes to
estimating financial risks. As discussed by Aikman et al (2014), there are at least three reasons for this.
All speeches are available online at www.bankofengland.co.uk/speeches
20
20
First, assigning probabilities is particularly difficult with rare, high-impact events, such as financial crises or
the failure of a large financial institution. Degrees of freedom are small in number, historical precedents
rarely exact and causal mechanisms imperfectly understood. This means estimated default probabilities,
and losses given default, are often highly imprecise. Indeed, that is (one reason) why model-based
estimates of the same underlying risks can differ so significantly across banks (BCBS (2014a)).
Second, the behaviour of complex, interconnected financial systems can be very sensitive to small changes
in initial conditions and shocks. That might be because these systems exhibit multiple equilibria, with
path-dependency or hysteresis. Or it may reflect network feedback effects propagating financial contagion.
Complex systems exhibit tipping points, with small changes in parameter values capable of moving the
system from stability to collapse (Anderson and May (1992), Gai and Kapadia (2010), Gai, Haldane and
Kapadia (2011)). In complex webs, the failure of two identical-looking banks can have very different
implications for financial system stability. The radical uncertainty in such complex webs generates emergent
behaviour which can be near-impossible to predict, model and estimate (Haldane (2016)).
Third, because they contain human actors whose beliefs about the future shape their behaviour today,
financial systems are particularly prone to instabilities and sunspots. If financial market participants are
driven by crowd psychology, emotion and narratives, as much as by economic fundamentals and rational
calculation, then risks are unlikely to be well captured by standard models (Tuckett and Taffler (2008),
Tuckett (2011), Shiller (2017), Bailey et al (2016)). These risks are likely to be highly non-linear, heavily
state and time-dependent and thus significantly fat-tailed.
In a world of such Knightian uncertainty, it may be difficult to estimate risk weights on individual assets with
any degree of precision. Indeed, attempts to do so may result in “over-fitting”, increasing the potential
fragility of these model estimates out-of-sample. In uncertain settings, simpler weighting schemes have
been found, in a variety of different environments, to offer a better defence against “unknown unknowns”
(Gigerenzer (2014)). For example, a 1/N or unweighted asset allocation heuristic (Benartzi and Thaler
(2001)), which allocates an equal amount of wealth to each of the assets in one’s portfolio, has been found
to outperform more complex strategies such as Markowitz’s (1952) mean-variance optimisation in
out-of-sample tests, unless the sample size is very large.19
 That logic is one rationale for the use - and, in
some settings, predictive superiority – of the LR in capturing solvency risks. It is a variation of the Brainard
(1967) portfolio argument.
In this vein, Aikman et al (2014) conduct simulations which demonstrate how simple methods, akin to a
leverage ratio, can sometimes dominate complex, risk-weighted approaches to calculating banks’ capital
requirements when guarding against solvency problems out of sample. This is more likely when the
underlying risks are themselves fat-tailed. While complex approaches can appear to perform better

19
 For example, DeMiguel et al (2007) find that, for a sample threshold of N = 25, complex rules outperform simple ones only for sample
sizes of in excess of 3000 months (250 years) of data.
All speeches are available online at www.bankofengland.co.uk/speeches
21
21
in-sample, simpler approaches may be more robust to out-of-sample structural shifts and fat tails, the like of
which we have seen in past financial crises, from railways in the 19th century to subprime mortgages in the
21st. This problem is not unique to banks. Stress tests can reduce reliance on banks’ own models. But they
then still rely on regulators’ risk models, which may be vulnerable to similar issues, especially if they are
formulated in an excessively granular manner (Hale et al (2015)).

Liquidity and Funding
Historically, most banks failures are precipitated by insufficient liquidity. Due to maturity transformation,
banks are vulnerable to a sudden withdrawal of funding due to bank-specific or market-wide losses of
credibility. In such circumstances, banks will be more robust if they have a buffer of high-quality liquid assets
allowing them to meet outflows, survive the first stages of a bank run and, if necessary, giving the authorities
time to prepare for resolution. The Basel III LCR was designed and calibrated with these considerations in
mind.
But excessive maturity transformation can also create risks over longer horizons. This highlights the
importance of funding metrics which consider the overall extent to which illiquid assets are supported by
unstable sources of funding. If banks are running a high degree of structural maturity transformation, this
increases the risk of failure. While one could envisage a range of possible structural funding metrics,
including a loan-to-deposit ratio, the Basel III NSFR is designed with these risks in mind.
The NSFR speaks directly to a market failure that arises from a market-wide loss of wholesale funding, the
like of which was exhibited during the crisis. In this way, it may potentially reduce the probability of
damaging asset fire-sales, liquidity hoarding and contractions in lending which may otherwise result. The
NSFR is also likely to complement the leverage ratio in acting as a brake on too-rapid balance sheet
expansion. The leverage ratio ensures that any such expansion is supported by higher capital. The NSFR
ensures any such expansion is supported by more stable funding sources.
None of this is to suggest that these risks could not be met with a different, and perhaps smaller, set of
regulatory constraints. Cecchetti and Kashyap (2016) have recently argued that the LCR or NSFR may be
redundant as one of the constraints is always slack in their simplified bank balance sheet model. Put
differently, the existing system of financial regulation may be over-identified. On the other hand, the horizon
for assessing banks’ liquidity risk may matter. In particular, the LCR and NSFR may complement each other
to the extent there are differences in asset liquidity and funding stability at different maturities and that these
differences evolve over the cycle.
All speeches are available online at www.bankofengland.co.uk/speeches
22
22
An Empirical Assessment of Regulatory Metrics
There are conceptual reasons why a portfolio approach to regulatory design, with a small, complementary
set of constraints, may have merit in a robust control sense: addressing the different risks facing financial
institutions and providing insurance against various uncertainties. Ultimately, however, the extent of
over-identification, and any costs it might impose, will depend on the empirical distribution of shocks to banks
and the state of their balance sheets at the time. To that we now turn, based on crisis experience.
Any counter-factual empirical exercise is subject to huge caveats. Nonetheless, it is revealing to consider
experience during the crisis to see what this revealed about the risks banks faced and how different
regulatory constraints might have handled them. For example, recent research has found the leverage ratio
and structural funding metrics performed well in predicting bank failure during the crisis (Huang and
Ratnovski (2009), Demirguc-Kunt et al (2010), Bologna (2011), Arjani and Paulin (2013), Vazquez and
Federico (2015)). And Lallour and Mio (2016) find that the NSFR had significant discriminatory power in
identifying failing banks during the crisis, after controlling for banks’ solvency ratios.
This line of research typically deploys regression approaches which weight together the information across
different indicators. Here we adopt a somewhat different approach. Specifically, we consider how effective
various combinations of regulatory constraints would have had been in identifying banks which subsequently
failed during the crisis (the “hit rate”), while at the same time avoiding incorrectly signalling stress among
banks which survived (the “false alarm rate”).
To do this, we exploit a dataset on the pre-crisis balance sheet characteristics of global banks developed by
Aikman et al (2014). The dataset includes almost all global banks which had more than $100 billion in
assets at end-2006 – 116 banks in total across 25 countries. A range of balance sheet metrics are proxied
at consolidated (group) level for each of these banks at end-2006. Restricting attention to those banks for
which data are available to compute all of risk-weighted capital ratios, leverage ratios and NSFRs reduces
the sample to 76 banks. If we focus on risk-weighted capital ratios, leverage ratios and loan-to-deposit (LTD)
ratios (as a simplified proxy for the NSFR which captures the ratio of retail loans to retail deposits) the
sample size is 96 banks.20

These banks can be divided into those that ‘survived’ and those that ‘failed’ between 2007 and the end of
2009. The definition and classification of failure follows Laeven and Valencia (2010), supplemented by
additional judgement in a few instances.21 To fix ideas, suppose that banks are subject to a single regulatory

20
 The dataset also includes a liquid asset ratio but this is a relatively poor proxy for the liquidity coverage ratio (LCR), so we exclude
consideration of the LCR from this analysis.
21 Because very few banks technically defaulted during the crisis, but many would have without significant government intervention, the
definition of failure is necessarily somewhat judgemental. Beyond clear-cut cases of default or nationalisation, Laeven and Valencia
(2010) define banks to have failed if at least three of the following six conditions were present: (i) extensive liquidity support (5 percent
of deposits and liabilities to non-residents); (ii) bank restructuring costs (at least 3 percent of GDP); (iii) partial bank nationalisation (eg
government recapitalisation); (iv) significant guarantees put in place; (v) significant asset purchases (at least 5 percent of GDP); (vi)
All speeches are available online at www.bankofengland.co.uk/speeches
23
23
metric – the leverage ratio. And using this metric, suppose that we set a cut-off threshold consistent with a
particular calibration of that regulatory standard – a leverage ratio of 3%.
One can identify banks which operated below that standard at a point in time. We define the ‘hit rate’ as the
number of banks which had a leverage ratio below 3% at the end of 2006 and subsequently failed during the
crisis, relative to the total number of banks that failed. And we define the ‘false alarm rate’ as the number of
banks with a leverage ratio below 3% which survived, relative to the total number of banks that survived. If a
3% leverage ratio could perfectly discriminate, its hit rate would be 100% and false alarm rate 0%.
Now suppose that we have flexibility over the cut-off threshold necessary to achieve a particular hit rate, x.
At the same time, we wish to minimise false alarms. As the leverage ratio cut-off increases, the hit rate and
false alarm rate must both go up. The key question is by how much each goes up – the relative balance of
marginal benefits and marginal costs of hits and false alarms – as the leverage ratio cut-off increases.
Charts 17 and 18 plot this for the 76-bank sample. Chart 17 plots the settings of the leverage ratio needed
to achieve particular target hit rates. Chart 18, meanwhile, plots what is referred to as the ‘receiver operating
characteristic’ (ROC) curve. Using the sequence of cut-off thresholds for the leverage ratio from Chart 17,
this plots the sequence of associated hit rates and corresponding false alarm rates at different settings of the
leverage ratio, alongside the 45 degree line which corresponds to the performance of a completely
uninformative metric.
Two points are clear from these charts. First, it is possible to achieve relatively high hit rates of up to 70% at
relatively modest calibrations of the leverage ratio of under 4% and with relatively low false alarm rates of
around 30%. This suggests that, with the benefit of hindsight and abstracting from definitional changes
which affect the interpretation of specific numbers, a leverage ratio of around 4% before the crisis would not
have been met by around 70% of banks which subsequently ended up failing. It served as a decent signal of
subsequent failure.
Clearly, such banks might still have failed during the crisis even with a leverage ratio of above 4%. But the
low false alarm rate at that calibration, corresponding to the observation that most banks which survived the
crisis had a leverage ratio of above 4% going into it, indicates that such a constraint may have helped to
curtail their risk-taking, as measured by their leverage ratio, and reduced their likelihood of failure.
Second, to achieve high hit rates of above 80%, both the calibration of the leverage ratio and the false alarm
rate increase sharply. Achieving a 90% hit rate with a leverage ratio alone requires its calibration to be
boosted to around 5.7%. Even then, it comes at the cost of a high false alarm rate of over 80%. In other
words, the balance of marginal benefits to costs becomes notably less positive when using a singular

deposit freezes and bank holidays. Aikman et al (2014) discuss where the classification of failure departs from Laeven and Valencia
(2010).
All speeches are available online at www.bankofengland.co.uk/speeches
24
24
instrument if policymakers have a low tolerance for failure. That matters if, for example, the costs of higher
capital requirements increase non-linearly (Greenwood et al (2017)).
These points are also evident if we assess individually the performance of the RWCR and NSFR. Chart 18
and Table 3 below show that hit rates of 80 or 90% can only be achieved with high false alarm rates and
stringent calibrations of these metrics. Overall, each metric individually does somewhat worse than the
leverage ratio in balancing hit and false alarm rates. And similar results hold when the loan to deposit (LTD)
ratio is considered instead of the NSFR in the wider sample (Chart 19).
Table 3: Target hit rate, calibration of individual regulatory tools and resulting false alarm rate
Target Hit
Rate (%)
LR
Calibration
False Alarm
Rate for LR
Calibration (%)
RWCR
Calibration
False Alarm Rate
for RWCR
Calibration (%)
NSFR
Calibration
False Alarm Rate
for NSFR
Calibration (%)
70 3.82 29.3 8.61 58.5 0.99 70.7
75 4.14 39.0 8.66 58.5 1.05 82.9
80 4.15 39.0 8.71 61.0 1.06 82.9
85 5.00 75.6 9.04 68.3 1.12 87.8
90 5.66 82.9 9.83 73.2 1.17 87.8
Now suppose that the regulator can draw on more than one regulatory metric – for example, a LR, RWCR
and NFSR. This now requires the setting of three cut-off thresholds and so gives more degrees of freedom.
But the objective otherwise remains the same, namely achieving a particular hit rate for signalling bank
failures while minimising false alarms. Chart 18 and Table 4 show the results from these multi-constraint
simulations.
Table 4: Target hit rate, calibration of individual and combined regulatory tools and resulting false
alarm rate
Target Hit
Rate (%)
LR Calibration
RWCR
Calibration
NSFR
Calibration
False Alarm Rate for
Combined Regulation (%)
70 3.82 5.52 0.63 29.3
75 3.80 5.52 0.72 36.6
80 4.15 5.52 0.63 39.0
85 3.71 5.52 0.83 51.2
90 4.07 5.53 0.83 53.7
All speeches are available online at www.bankofengland.co.uk/speeches
25
25
At a target hit rate of 70%, a portfolio of regulatory measures does little better than the leverage ratio on its
own in signalling bank stress. At targeted hit rates of over 80%, however, that picture changes. The ROC
curve for the regulatory portfolio lies to the left of all those corresponding to individual metrics. In other
words, it is possible to achieve lower false alarm rates, for the same hit rate, when multiple regulatory metrics
are used. The calibration of each metric in the portfolio is also less stringent than the calibration for each
metric individually.
These results hold when comparing any pair of regulatory metrics with individual metrics – a higher hit rate
per false alarm rate, with less stringent calibrations. It also holds even more strongly in the wider 96-bank
sample when the LTD ratio is considered instead of the NSFR (Chart 19). This suggests that, at least in this
sample, imposing a small number of regulatory constraints can achieve the same hit rate as any singular
constraint, but at a materially lower societal and regulatory cost, as measured by levels of capital and
liquidity and/or regulatory false alarms.
Intuitively, these results broadly accord with the conceptual discussion. A regulatory portfolio can help when
insuring against multiple sources of risk and myriad sources of uncertainty. It also accords with what we
know from various individual case studies of bank failure during the global financial crisis: some banks failed
because they were over-leveraged, others because their assets were excessively risky, others still because
they undertook too much maturity transformation.
Because these fault-lines were, for some banks, reasonably well-correlated – their risk management was
poor across all dimensions – individual regulatory metrics performed fairly well in identifying these banks
prior to them failing. But some banks’ risk-management failings were singular, not plural. Their risk
blind-spots were idiosyncratic and uncorrelated. By using a portfolio of regulatory stress metrics, it is
possible to isolate those banks which were risk-management outliers in one, but not all, dimensions.
Consider two very different banks which failed during the global financial crisis. American bank Countrywide
had a leverage ratio of 7.7% and a risk-weighted capital ratio of 11.6% at the end of 2006. Even if capital
regulation had been much tougher in 2006, it may not have been required to raise capital. But its NSFR was
just 0.76, indicative of the structural liquidity risk it was undertaking. By including the NSFR in the suite of
regulatory metrics, it would have been possible to capture the risks that Countrywide was undertaking
without resorting to materially more stringent capital regulation.
By contrast, Belgian bank KBC Group had an NSFR of 1.12, above the current indicative regulatory
standard. For that time, it also had a reasonable risk-weighted capital ratio of 8.7%, well above the median
capital ratio in the sample. But its leverage ratio was 3.5%. A system of regulation excluding the leverage
ratio would have been unable to capture risks of the type KBC Group was undertaking prior to the crisis.
All speeches are available online at www.bankofengland.co.uk/speeches
26
26
The message from this counterfactual exercise, for all its obvious imperfections, is that multiple regulatory
metrics may have helped historically in capturing the multiple dimensions of risk and uncertainty exhibited by
banks pre-crisis. With the benefit of hindsight, multiple metrics would have helped identify most failing
banks, without either high false alarm rates or potentially punitive calibrations of regulatory standards. While
the exercise is based solely on survival and failure of banks during the global financial crisis, it highlights how
a small regulatory portfolio beats, counterfactually, a single stock in (systemic) risk and (capital) return terms.
As Greenwood et al (2017) argue, multiple regulatory constraints come at a cost by curtailing business
models and thus reducing diversity in the financial system. Excessive homogeneity of the financial system
can create systemic risks (Haldane (2009a), Wagner (2010)). How much it does so is, however, a matter of
degree. If regulatory constraints act as control bounds on structurally defective business models, that
strengthens the financial system, even if (indeed, precisely because) it constrains diversity. Empirical
evidence suggests this latter effect dominated during the recent crisis.
(ii) Discretion versus Rules
The new architecture has introduced measures which are likely to make for a greater degree of supervisory
or policymaker discretion in the setting of regulatory standards. This arises, most obviously, in the
application of supervisory judgement to certain risks that banks face, to stress-testing and to macroprudential
policy. Multiple regulatory rules have been augmented with considerable supervisory discretion. Viewed in
the round, this new regulatory regime could reasonably be described as “constrained discretion”. Regulatory
rules provide the constraint within which policymakers exercise discretion.
In its broad contours, this new regulatory framework has some similarities with the prevailing monetary policy
framework in a number of countries (Bernanke and Mishkin (1997)). These regimes have been found to be
an effective way of balancing the pre-commitment necessary to avoid policy time-consistency problems with
the flexibility necessary to respond to unforeseen circumstances (Arestis and Mihailov (2009), Borio (2010)).
Equally, as in the monetary policy sphere, there is a question about whether this new regulatory regime
strikes the right balance between regulatory rules and the degree of discretion with which they are operated.
The time-inconsistency problem that pervades the debate over the balance between rules and discretion in
monetary policy (Kydland and Prescott (1977), Barro and Gordon (1983)) is arguably even more acute for
prudential policy. This is partly because adverse crisis outcomes are highly non-linear and costly, making it
more difficult to pre-commit to avoiding forbearance and bail-out. The low probability of crises may also
mean that policymakers are insufficiently tough in tackling financial sector risks when times are good and
memories of previous crises distant (Reinhart and Rogoff (2009), Malmendier and Nagel (2011), Gennaioli et
al (2015)). This can create political pressures to relax regulations to support shorter-term goals.
Public choice theory (Olson (1965)) would also suggest that lobbying pressure is likely to be more acute for
regulatory policy than for monetary policy. The private costs of regulation are borne strongly by narrow, but
All speeches are available online at www.bankofengland.co.uk/speeches
27
27
powerful, interest groups in the financial industry. And while higher than target inflation is quickly observable,
it may be very difficult to judge in real time that regulation is insufficiently stringent given the difficulties of
quantifying the probability of future financial crises.
These arguments point to the need for strong institutional frameworks, supported by clear mandates,
objectives and instruments, to deliver financial stability policy. Indeed, on conceptual grounds, the need for
such a framework appears to be at least as strong, if not stronger, than for monetary policy. They also
support the case for clarity in the application of these regulatory policies.
Not least given its newness, there may be further to go clarifying the motivation behind macroprudential
interventions and the circumstances which might justify different macroprudential instruments - in short, in
defining and refining the macroprudential policy reaction function. The UK’s Financial Policy Committee has
made some progress in this area, most notably in setting out its strategy for using the countercyclical capital
buffer (Bank of England (2016a)). A discussion of the principles underlying the UK’s approach to
macroprudential policy can be found in Brazier (2017a).
The benefits of pursuing this path are clear from monetary policy experience (Brazier (2015)). Increasing the
predictability of policy can enhance the ex-ante signalling and expectations channels of regulatory policy, as
has been achieved in relation to monetary policy (Bernanke and Mishkin (1997)). It enhances ex-post
accountability to stakeholders, political and societal. And it reduces the potential behavioural biases
otherwise associated with discretionary decision-making and which have been found in the past to affect
discretionary regulatory policy, including regulatory capture (Dal Bó (2006)) and defensive decision-making
(Gigerenzer (2014)).
At the same time, there is clearly a balance to be struck. As Greenwood et al (2017) argue, strict
rules-based systems are likely to be arbitraged and exploited by banks. For example, recent theoretical work
and experimental evidence suggests remuneration contracts can be restructured to recreate the excessive
risk-taking incentives that new rules seek to reduce (Thanassoulis and Tanaka (2017), Harris et al
(forthcoming)).
These arguments make it difficult to specify strict regulatory rules for all seasons. They point to the need for
a forward-looking, horizon scanning approach with scope for supervisory judgement and macroprudential
discretion (BCBS (2017)). This does not, however, obviate the potential benefits from seeking, over time, to
specify clearer mandates and regulatory reaction functions, especially on the macroprudential front.
Incentives and Arbitrage
The empirical exercise in the previous section looked at how a set of regulatory standards, applied
counterfactually, might have done in spotting stress among a set of banks. Plainly, any such counterfactual
All speeches are available online at www.bankofengland.co.uk/speeches
28
28
exercise is subject to significant caveats. The most important of these is that it cannot take account of how
changes in the regulatory regime might themselves have reshaped risk-taking incentives at the time. This
Lucas Critique plainly looms large in the field of financial regulation.

Financial regulation, like any tax, is very likely to change the behaviour of the party subject to it. This is
neither surprising nor, necessarily, undesirable. Indeed, sometimes it is the precise purpose of the
regulation in the first place. For example, average risk weights on assets held in the trading book have
increased by 45% across a sample of major UK banks between 2006 and 2013. Partly in response, trading
books have shrunk by, on average, 24% across these banks and by 45% across the world’s G-SIBs. This
was an intended, and probably desirable, behavioural response to a necessary recalibration of regulatory
standards.
That is not to say, however, that all behavioural adjustments are either exactly as intended or desirable.
This is particularly the case when these responses seek to avoid regulation entirely – so-called regulatory
arbitrage. Risk, like energy, does not disappear into the ether. It is typically conserved, at least to some
degree. In response to tighter financial regulation, risk is likely to change shape or location, often both.

This is not just a conceptual point. The history of financial regulation can be seen as an on-going,
evolutionary race to adjust regulatory rules to limit avoidance incentives (Haldane (2013)). The first Basel
Accord (“Basel I”) was a direct response to cross-border regulatory arbitrage. Basel II came largely as the
result of standardised approaches to risk measurement being arbitraged. And Basel III came in part as a
response to both risk models, and the Basel framework itself, being arbitraged. This race has been
characterised as “bloodhounds in pursuit of greyhounds” (Eichengreen (2009)). Regulators need both to
learn from past experience and to anticipate future opportunities for avoidance (Woods (2017)).
The arbitrage problems faced by the bloodhounds were well-exemplified in the run-up to the global financial
crisis. These included the migration of activity and risk to unregulated “shadow banks” (Adrian and Ashcraft
(2012)); the hard-wiring of rating agency risk assessments into the regulatory engine (Edmonds (2016)); the
payment of bank CEOs in common equity encouraging “gambling for resurrection” (IMF (2014)); and the
implicit subsidies conferred on “too-big-to-fail” institutions, encouraging them to become larger and more
complex and connected still (FSB (2013b)).
Another example of these incentive effects came in the area of capital regulation. Whichever risk-weighting
scheme is in place, it is likely to give rise to incentives to adjust asset positions to maximise profits. For
example, if the regulatory constraint takes the form of a leverage ratio there are incentives to alter the
composition of assets towards those with higher risk weights – though the evidence on such “risk-shifting” is
mixed (Sheldon (1996), Furlong (1988)). Contrarily, if assets are risk-weighted and determined by banks’
internal models, there are incentives to lower modelled risk weights over time (Mariathasan and Merrouche
(2014)). In short, when setting capital standards for banks, there is a two-sided incentive problem.
All speeches are available online at www.bankofengland.co.uk/speeches
29
29
Pre-crisis, both incentives were at play, albeit to differing degrees in different parts of the global financial
system. In the US, where a leverage ratio was in operation and often the binding constraint, there were
incentives for banks to seek higher-risk assets rather than expand balance sheets (Chart 20). In Europe,
without a leverage ratio but with risk-based capital standards, there were incentives for banks to expand
balance sheets and shade downwards risk weights. Canadian banks’ incentives sat somewhere in between.
Some recent studies have looked at these behavioural shifts in greater detail. During the euro-area crisis,
banks increased their exposure toward higher risk government bonds, which carried no capital requirement
(Acharya and Steffen (2015)). And following the Lehman crisis, German banks reduced their corporate
lending less when the capital requirement was set under the standardised approach (Behn, Haselmann,
Wachtel (2016)). In the UK, higher risk mortgages shifted towards lenders whose capital requirements were
less risk-sensitive after the introduction of Basel II (Benetton et al (2017)).
Recent research has considered how the leverage ratio announcement affected behaviour among a panel of
over 650 European banks (Acosta-Smith et al (2017)). It finds a significant increase in risk-taking among
those banks for whom the new regime was a binding constraint. This risk-taking was greater, the further
these banks were from meeting the new 3% threshold: banks with leverage ratios of 1.5%, 2% and 2.5%
were found to increase their risk-taking by 3.4, 2.3 and 1.1 percentage points of risk-weighted assets
respectively. This is clear empirical evidence of the risk-shifting channel at work.
This is only, however, one side of the risk equation. There were two mitigating factors on the other side.
First, a rise in the leverage ratio also boosts these banks’ capital. Once translated into default probabilities,
Acosta-Smith et al (2017) find that the second effect swamps the first: a one percentage point rise in the
leverage ratio raises the odds ratio (on banks being in distress versus safe) through risk-shifting by 1-3.5%.
But the reduction in the odds ratio from lower leverage is close to 40-50%.
Second, the leverage regime is not a replacement for the risk-weighted capital regime but an addition to it.
The capital regime places an automatic upper-bound on the extent to which banks can increase their
risk-weighted assets. In other words, the capital ratio regime places constraints on incentives to risk-shift.
Conversely, the leverage ratio can serve as an effective constraint on incentives to game or shade risk
weights. Risk-taking incentives are, in effect, book-ended by the leverage and capital constraints.
From an incentives perspective, if regulatory arbitrage incentives are two-sided, so too should be the
constraints needed to straightjacket that behaviour. That is another way of rationalising the “multi-polar”
regulatory regime operating internationally. When it comes to calibration, this means leverage and capital
ratios need to be jointly determined to prevent incentives skewing in one or other direction. This is the
approach in the UK when setting capital and leverage constraints (Bank of England (2015a)).
All speeches are available online at www.bankofengland.co.uk/speeches
30
30
There are other means of constraining adverse incentives. Incentives to game risk weights can be
constrained by imposing floors and/or by using standardised approaches for certain categories of assets.
22

Some countries already make use of such approaches, including the US, the UK, Germany, France and
Spain. The stress-testing regimes operating in a number of countries are also a way of cross-checking, and
backstopping, the models used by banks. In a world of uncertainty as well as risk, having this portfolio of
approaches for dealing with avoidance incentives - some discretionary, others rule-like - makes sense.
Evidence on the incentive effects of financial regulation is almost always drawn from looking at banks’
experience either side of a change in policy. By then, however, any unintended or undesirable
consequences of this change will already have been felt. Regulatory policy is in a perpetual catch-up loop,
the bloodhounds in pursuit of the greyhounds.
In an ideal world, it would be possible to gauge in advance how a regulatory change may reshape incentives,
in particular risk-taking or regulatory-avoiding incentives. One approach to doing so is to use experimental
methods. Experiments have previously been used to examine how different pay structures affect loan
officers’ risk assessment and lending decisions (Cole, Kanz and Klapper (2015)). They have also been used
to examine the effect of specific interventions on behaviour in other areas of public policy (Halpern (2015)).
Recently, the Bank of England has conducted a lab experiment to assess how the design of pay regulation
may affect risk-taking behaviour and project search effort (Harris et al (forthcoming)). Specifically, the
experiment was designed to examine how caps on bonuses and “malus” (bonuses that are not paid out if, for
example, performance falters in subsequent years) might affect individuals’ risk choices and efforts to seek
out the best projects. The experiment showed evidence that, while both schemes tend to reduce risk-taking,
they could be arbitraged relatively easily by introducing absolute or relative performance targets. There was
also some evidence that a bonus cap might reduce incentives to search for good projects.
The Bank of England is considering extending this experimental approach to a wider range of policy design
questions and a wider range of financial market participants - risk-takers and risk-managers. In principle, this
approach might provide some early indications of how new regulation might reshape risk incentives,
including arbitrage incentives, which could help in recrafting regulation before it is introduced.
Macroprudential Policy
One of the greatest intellectual errors made in the run-up to the crisis was a classic “fallacy of composition”:
it was assumed that the resilience of individual financial institutions was both a necessary and sufficient
condition to ensure the resilience of the financial system as a whole. Events during the crisis, and

22
 See BCBS (2016): http://www.bis.org/bcbs/publ/d362.pdf and BCBS (2013b): http://www.bis.org/publ/bcbs258.pdf
All speeches are available online at www.bankofengland.co.uk/speeches
31
31
subsequent theoretical and empirical work, has shown that the resilience of individual firms is neither
necessary nor sufficient for the mitigation of systemic risk (Masera (2014), Crockett (1996)).
Out of this intellectual vacuum, a new framework for regulation has been born – macroprudential regulation.
This explicitly recognises the links that might tie together individual nodes in the financial system. These
might arise from correlated asset exposures, the type of which has historically emerged during the upswings
and downswings of the credit cycle (Aikman et al (2015)). Or they might emerge from financial exposures,
on- or off-balance sheet, between intermediaries operating in the global financial web (Haldane and May
(2011), Arinaminpathy, Kapadia and May (2012)).
Over the past decade, a lot has been written in the area of macroprudential policy (Galati and Moessner
(2013), Aikman et al (2013), Freixas et al (2015)). A sizeable number of countries are now undertaking
macroprudential policy in some shape or form. Table 5 in the Annex provides a rough summary of current
international macroprudential practices (for a more comprehensive overview, see Cerutti et al (2017)). By
any historical metric, however, macroprudential policy remains a fledgling framework. Understandably, many
of its key facets remain contentious. And the macroprudential policies put in place internationally so far are
more notable for their differences than their similarities. This is providing a diverse body of case law.
We discuss two key aspects of this framework: its appropriate objectives; and the choice of instruments.
Objectives of Macroprudential Policy
The Bank of England entered the debate on the potential role of macroprudential policy in a discussion paper
published in 2009 (Bank of England (2009)). That paper cast the debate on the potential objectives of such
a regime as a choice between ‘protecting banks from the cycle’ and ‘protecting the economy from the banks’.
Macroprudential policy could focus narrowly on building resilience in the financial system in a dynamic way,
so that it was better able to absorb large adverse shocks. Or it could pursue the broader and bolder
objective of smoothing the swings in debt and asset prices associated with the financial cycle.
The subsequent academic literature has usefully refined how we think about these goals of a
macroprudential regime. One fruitful strand has focused on the pecuniary externalities generated by
fire-sales in asset markets (Lorenzoni (2008), Jeanne and Korinek (2010), Bianchi and Mendoza (2010),
Benigno et al (2013)). Collateralised borrowing leads to externalities because individual borrowers do not
internalise the fact that increasing debt in good times raises the likelihood they will be forced to sell assets
following adverse shocks, pushing prices lower, tightening collateral constraints and exacerbating downturns.
These feedback and amplification loops can mean that private borrowing in good times is greater than a
social planner would choose, facing the same constraints. That is the theory. Experience during the crisis
tends to support the importance of these transmission channels. When highly-levered banks were forced to
All speeches are available online at www.bankofengland.co.uk/speeches
32
32
sell illiquid assets at highly discounted prices, this lowered valuations further and tightened constraints for
other banks (Brunnermeier (2009)). This contributed to the depth and duration of the economic downturn.
Another strand of the literature has examined whether unlevered investors may also be prone to this fire-sale
mechanism. This is a question of rising importance given the rapid growth in open-ended investment funds
since the crisis. In Feroli et al (2014), asset managers are motivated by their relative ranking. This
generates a feedback loop in which falling asset prices incentivise further selling for fear of diverging from
the pack (Morris and Shin (2016), Vayanos and Woolley (2013)). This channel is amplified if investors
perceive there to be a first-mover advantage in withdrawing funds, generating ‘run-like’ behaviour (Goldstein,
Jiang and Ng (2017), Chen, Goldstein and Jiang (2010), Morris, Shim and Shin (2017)).
A recent paper by Bank of England colleagues (Baranova et al (2017)) provides a framework for quantifying
the risks posed by investment funds in corporate bond markets (see also Ceterolli et al (2016)). In their
model, investors act pro-cyclically, withdrawing funds when corporate bond prices fall and causing
investment funds to make a first-round of asset sales. Dealers provide liquidity, but require a further fall in
price as compensation. This leads to further redemptions and further asset sales, amplifying the fall in price.
Chart 21 shows that this amplification effect can be quantitatively significant: weekly redemptions from bond
funds of 1% of total net assets, similar to the level observed at the peak of the crisis, increase the liquidity
premium in bond spreads by 40 basis points. Moreover, redemptions of as little as 1.3% of total assets
exhaust the dealer’s capacity to intermediate trades, leading to a market ‘freeze’.
In a similar spirit, Braun-Munzinger et al (2016) develop an agent-based model of the corporate bond market
comprising a market maker, fund traders and fund investors. They find that funds pursuing similar trading
strategies can exacerbate price movements and contribute to the pro-cyclicality of financial markets.
Additionally, the growth of passive investments may have both positive and negative effects on volatility:
they decrease yield volatility on average, but can increase the likelihood of large dislocations after shocks.
While this fire-sale mechanism applies most directly to financial intermediaries with marked-to-market
balance sheets, and funds that can be redeemed at short notice, a similar dynamic can operate if there are
forced sales by owners of, or investors in, real estate who are credit-constrained borrowers.
This, too, can drive prices lower in a feedback loop. Some of these mechanisms were in play recently
among UK real estate investment vehicles following the EU referendum result (Bank of England (2016b)).
A related, but distinct, mechanism through which financial frictions can affect the wider economy is through
aggregate demand externalities. In Korinek and Simsek (2016), credit-constrained households de-lever
sharply when an adverse shock hits. If the shock is large enough, the resulting fall in aggregate demand can
push the economy into a liquidity trap with interest rates constrained at the effective lower bound. In this
environment, macroprudential policies that slow the build-up of household leverage ex-ante can be
All speeches are available online at www.bankofengland.co.uk/speeches
33
33
welfare-improving in avoiding this outcome. Farhi and Werning (2016) also offer a model of macroprudential
policy in the face of aggregate demand externalities.
Another strand of the literature emphasises behavioural sources of pro-cyclicality. For example, powerful
narratives, such as a collective belief in a ‘new paradigm’, might manifest themselves in over-exuberance in
the financial system (Tuckett (2011), Shiller (2017)). Myopia about risk might also drive excessive
risk-taking, especially as memories of past financial crises fade (Guttentag and Herring (1986), Herring
(1998), Haldane (2009b), Gennaioli, Schleifer and Vishny (2012)). And such behaviour might be amplified
by contracts that reward short-term performance excessively and by herding in financial markets (Avery and
Zemsky (1998), Lakonishok et al (1992), Bikhchandani and Sharma (2001)). Aikman, Nelson and Tanaka
(2015) show how reputational concerns and peer benchmarking can drive credit cycles.
The case for macroprudential interventions to address build-ups in leverage also has empirical support.
Mian and Sufi (2010) argue that the persistence of the decline in US GDP after the crisis was caused by
excessive household leverage. Jordà et al (2013) report that credit booms not only increase the likelihood
and severity of financial crises, but also make normal recessions more painful. A one standard deviation
increase in ‘excess credit’ results in real GDP per capita being 1.5% lower five years after a normal
recession.23
 Bunn and Rostom (2015) find that more highly indebted groups of households made larger cuts
in spending following the financial crisis. And Bailey et al (forthcoming) exploit Facebook data to identify how
social interactions can drive contagious risk-taking in the US housing market.
In summary, the market failures associated with fire-sale externalities and behavioural tendencies which can
drive short-termism provide a strong case for a macroprudential regulator with an objective of preserving the
dynamic resilience of the financial system, both among banks and, prospectively, among non-banks. No
less compelling, however, is the evidence, both micro and macro, linking credit booms to aggregate demand
externalities. That, in turn, provides a rationale for pre-emptive macroprudential interventions to avoid
excessive inflation of credit and asset prices in the first place.
Instruments of Macroprudential Policy
The Bank of England’s second public foray into the macroprudential policy debate was a 2011 discussion
paper on elements of the macroprudential toolkit (Bank of England (2011)). That paper described 12 distinct
macroprudential tools (see also CGFS (2010, 2012), Hanson et al (2011) and ESRB (2015) for discussions
of macroprudential instruments). The majority of these targeted different aspects of banks’ balance sheets
including: the countercyclical capital buffer (CCyB); sectoral capital requirements (SCRs); leverage ratio
buffers; dynamic provisions; and liquidity buffers.

23
 Defined as the rate of change of aggregate bank credit (domestic bank loans to the nonfinancial sector) relative to GDP, relative to its
mean, from previous trough to peak.
All speeches are available online at www.bankofengland.co.uk/speeches
34
34
Other potential macroprudential instruments included those aimed at influencing lending standards - for
example, through setting loan to value limits (LTV), loan to income limits (LTI) or margining requirements on
collateralised borrowing in financial markets. A third set of potential instruments focused on making market
infrastructures more resilient and improving financial market practices – for example, by mandating central
clearing, through trading venue design and through enhanced disclosure requirements.
So how has thinking on macroprudential instruments moved forward in the period since? Chart 22
documents the use of different types of macroprudential tool over the past decade in a panel of advanced
economies.24
 Perhaps contrary to expectations, the majority of interventions have fallen into the “lending
standards” category, a group that includes LTI and LTV requirements. By numbers, such actions account for
around two-thirds of overall macroprudential actions over the period since 2010.
At the other end of the spectrum is the CCyB. That has only been used by 6 countries to date. This is
surprising given that it is the only tool that has a well-defined operating framework internationally and which
includes jurisdictional reciprocity. A potential explanation is that the Basel guidelines suggest the buffer
should be activated when excess credit growth threatens an increase in system-wide risk. The majority of
advanced countries have not come close to experiencing aggregate credit booms in the post-crisis period.
When it comes to assessing the efficacy of these tools, it has largely been a case of ‘learning by doing’.
Cerutti et al (2017), Kuttner and Shim (2013), Crowe et al (2013) and Boar et al (2017) provide evidence on
the impact of tools using cross-country panel studies. Aikman et al (forthcoming) and Banerjee and Mio
(2017) study the UK’s experience with the CCyB and with macroprudential liquidity actions respectively. And
He (2013) considers the impact of the HKMA’s use of LTV limits. Overall, however, the evidence base on
the transmission of macroprudential tools remains fairly slim.
There is also relatively little guidance from the literature on tool strategy, selection and interaction. One
exception is work assessing the role of monetary policy in leaning against financial cycles. A paper by the
Federal Reserve Board (Ajello et al (2016)) analyses the costs and benefits of using interest rates to lean
against vulnerabilities in the financial system. In the baseline calibration of their model, the costs of using
monetary policy in this way are large relative to the benefits: the optimal adjustment in interest rates in the
face of financial stability risks is in the order of 3 basis points.
While the adjustment to interest rates can be larger – up to 75 basis points - if alternative assumptions are
made about cost of crises and the sensitivity of crisis risk to monetary policy, the calibrations required to
deliver these outcomes are extreme. Svensson (2017) argues that the costs of using interest rates to lean
against financial crisis risk are likely to be greater still if one takes into account that doing so will make the
economy weaker at the point a crisis strikes and hence might actually worsen its severity.

24 We would like to thank Karam Shergill and Rhiannon Sowerbutts for collecting these data.
All speeches are available online at www.bankofengland.co.uk/speeches
35
35
Aikman et al (2016) analyse empirically the joint non-linear dynamics of credit, financial conditions and
monetary policy in the United States. They find that the transmission mechanism of monetary policy to
long-term yields is blunted in high credit states. This suggests that attempts to lean against the wind with
monetary policy when credit is already elevated may be futile. Filardo and Rungcharoenkitkul (2016), by
contrast, find benefits from using monetary policy to ‘lean’ when the evolution of financial imbalances is
extremely persistent. But the general consensus is against deploying monetary policy, at least in an activist
way, for financial stability purposes given its limited efficacy and potential real-economy costs.
One recent paper that addresses issues of CCyB strategy is Aikman, Giese, Kapadia and McLeay
(forthcoming). Developing Ajello et al’s (2016) approach, the authors study an economy in which
policymakers face a trade-off between stabilising inflation and output today versus keeping a lid on financial
stability risks which threaten a crisis tomorrow. The optimal strategy is to adjust the CCyB in line with
forward-looking indicators of crisis risk – credit growth in their model – but to relax (tighten) the CCyB relative
to this plan if output is below (above) its target level. This strategy dramatically improves the inter-temporal
trade-off facing a policymaker relative to the case where monetary policy is the only tool (Chart 23). Indeed,
this is consistent with the Bank of England MPC’s guidance that monetary policy should be the ‘last line of
defence’ in the presence of financial stability risks (Bank of England (2013)).
The dashed lines show the steep trade-off facing a policymaker with a monetary policy tool only. Attempts to
reduce the crisis probability with higher interest rates entail significant costs for current output and inflation.
The solid lines show how this trade-off improves when the CCyB is added to the instrument set, even under
conservative assumptions about the impact of the CCyB on the economy’s productive capacity. The
variation in the CCyB required to deliver these benefits can be large. Given the historical distribution of
shocks, the standard deviation of the CCyB is around 2 percentage points.

A final strand of the literature has analysed whether the presence of macroprudential policy gives rise to
coordination problems with monetary policy. For example, De Paoli and Paustian (2017) study a
non-cooperative game between monetary and macroprudential authorities and find coordination problems to
be significant following cost-push shocks in cases where objectives do not overlap. But they also find that a
leadership structure in which macroprudential policy moves first – or is varied at a lower frequency than
monetary policy given that financial vulnerabilities build slowly – mitigates these coordination problems.
Future Research and Policy
Financial regulation has undergone a fundamental rethink and reform since the global financial crisis. By
most accounts and on most evidence, that has resulted in a financial system which is more resilient than in
the past, better equipped to head-off market frictions and failures of various kinds, better attuned to various
adverse incentive effects, and better able to safeguard risks which imperil the financial system as a whole. It
All speeches are available online at www.bankofengland.co.uk/speeches
36
36
is a regime of “constrained discretion”, comprising a portfolio of regulatory measures calibrated, albeit
roughly, to equate societal costs and benefits. That’s the easy bit.
The hard bit is what happens next. Not least given the scale of regulatory change over the past decade, this
new regulatory framework will plainly need to adapt in the period ahead in the light of the new evidence,
experience and incentives associated with operating it. This paper has discussed some of those issues.
From a potentially very long list, we conclude by highlighting some of the areas where we think further
research and practical exploration might be useful in the future debate on regulatory reform.25
(a) Optimal Levels of Capital: One of the most animated, on-going areas of regulatory debate is whether
capital standards have been appropriately calibrated. Relative to the pre-crisis LEI study, current levels of
capital requirements in most countries are below that calibration. The single most important reason for that
is because the LEI study did not take into account the potential impact of non-equity sources of capital,
specifically TLAC, in reducing the impact and probability of crisis. The key question, then, is whether these
instruments prove to be as loss-absorbing in future situations of stress where bail-in becomes necessary.
This issue is particularly relevant when it involves systemically-important institutions or sets of institution,
when the costs of bailing-in (and bailing-out) are large and lumpy. At this early stage, the jury must still be
out. On the one hand, historical evidence on bailing-in different types of notionally loss-absorbing bank
liabilities in situations of systemic stress has not been encouraging, reflecting the acuteness of the
time-consistency problem facing the authorities in these cases. On the other, new statutory resolution
arrangements are much stronger than ever previously, and statutory TLAC requirements are now prescribed
in advance. This means next time could plausibly be different. Given its importance to the overall capital
calibration, this issue deserves further empirical and theoretical consideration.
(b) Multi-Polar Regulation: The new regulatory framework is a different beast than its predecessors in terms
of the number, complexity and discretionary nature of the constraints it imposes. There are good conceptual
and empirical grounds for such a portfolio approach in insuring against future risks and, in particular,
uncertainties. And from a risk- and uncertainty-averse social welfare perspective, even a marginally
over-identified system might, in general, be preferable to a marginally under-identified one, if recent crisis
experience is any guide. Indeed, that is the essence of robust control. Ultimately, however, the past is
another country. There are legitimate questions to answer about whether multiple regulatory constraints
could lead to excessive homogeneity and inefficiency in the financial system. And arbitrage is an
ever-present threat, even with multiple regulatory metrics. This is an area where further research and
practical experience with operating the new regime will be essential in gauging whether there is scope for
streamlining, provided the resulting regulatory regime remains robust to the radical uncertainty that
necessarily affects any complex, adaptive system such as finance.26


25
 See also, Calomiris (2017), Duffie (2017), Greenwood et al (2017).
26
 FSB (2017e), for example, describes a Policy Evaluation Framework to achieve efficient resilience.
All speeches are available online at www.bankofengland.co.uk/speeches
37
37
(c) Models of Financial Stability: In a world of monetary, macroprudential and microprudential policy, all
having an impact on the economy and on the financial system, there is an increased onus on developing
quantitative frameworks which enable us to understand their impact, individually and collectively, and their
interaction (Bank of England (2015b)). That calls for models able to capture quantitatively monetary,
financial and regulatory channels of transmission and the feedback mechanisms between them. Progress
has been made, in particular since the crisis, in developing macro-models with an explicit financial sector
which can capture rich, two-way feedbacks between the economy and financial system (for example,
Brunnermeier et al (2012), Brunnermeier and Sannikov (2014)). There has been progress, too, in
developing models of systemic risk which assign macroeconomic factors, and within-system feedbacks, a
prominent role (Greenwood et al (2014), Cont and Schaanning (2017)). Yet we are still probably in the
foothills when developing a unified framework for bringing these factors together in one place, a framework
that could capture the rich feedback and amplification mechanisms that operate in practice and a model
which could then serve as a test-bed for each of the three arms of policy. Indeed, it could be that a single,
Holy Grail, framework is infeasible or indeed undesirable.
(d) Future of Stress-Testing: Bank stress testing has evolved considerably since the financial crisis and is
now a cornerstone of the regulatory regime in many jurisdictions. The direction of travel necessary to enrich
these tests, and to make them truly macroprudential, is to incorporate feedback effects that can amplify the
actions of individual institutions at the system-wide level (Demekas (2015), Brazier (2015), Tarullo (2016)) –
feedbacks, for instance, that result from fire-selling assets, hoarding liquidity and counterparty risk.27
 A
natural consequence is that we might need to extend the field of vision for such simulations to include nonbank parts of the financial system. Non-bank sources of systemic risk proved to be potent during the crisis,
in particular among shadow banks. As regulation has squeezed the banking system, there has been further
migration of financial activity into the shadows, particularly within Europe. What was once credit and funding
risk on the balance sheets of banking firms is metamorphosing into market and liquidity risk on the balance
sheets of funds and investment vehicles of various types (Stein (2013)). Understanding these risks calls for
new and enhanced surveillance tools. Systematic, market-wide stress simulations might be needed to
capture new market and liquidity risks and their propagation across different financial institutions and
markets (Brazier (2017b)). The same considerations apply to key pieces of the financial infrastructure, in
particular central counterparties (Cœuré (2017), Duffie (2017)). As a potentially new “too big to fail” entity,
they too need to be stress-tested and their resolution plans agreed and implemented. This is a whole new
risk-management agenda, where work has only just begun in earnest.
(e) Market-Based Finance: The emergence of a large and diverse shadow banking system, both prior to the
crisis in the US and subsequent to it elsewhere around the world, plainly poses both considerable
opportunities and potential threats to financial stability. So-called market-based finance provides the
financial system with a second, non-bank, engine on which to fly which could be beneficial in a diversity

27 For example, the results of the Bank of England’s 2014 stress test found that risk-weight procyclicality was a significant contributor to
the change in capital ratios in the stress test scenario (Bank of England (2014)).
All speeches are available online at www.bankofengland.co.uk/speeches
38
38
sense. Nonetheless, it also gives rise to potentially new sources of systemic risk and contagion, as risks
change shape and location. The FSB has made significant progress in progressing the regulatory debate on
such matters (FSB (2017e)). Certainly, these trends carry implications for both the conduct of regulation and
for central bank procedures. A world of greater market and liquidity risk may call for different sets of
regulatory instrument than the bank-based solvency and liquidity metrics of Basel III. Market-based
instruments, such as margin requirements, may have a greater role to play (see, for example, ESRB (2017)).
It may also call for different types of market intervention by central banks – different markets, different
instruments, different counterparties. The crisis has already seen a mini-revolution in the design of liquidity
facilities by central banks. As the financial system changes shape, it seems plausible to think that further
change could be necessary. If so, that change would benefit from further research on the costs and benefits
of the extended regulatory and central bank safety net.
(f) The Macroprudential Policy Framework: As a still-fledgling framework, there are a wide range of
questions still surrounding the macroprudential framework. There is no settled, practical approach to defining
the breadth of objectives of a macroprudential regime. Should the potential for aggregate demand
externalities associated with a debt overhang in the household sector, for instance, fall under the purview of
a macroprudential authority? Nor, in the main, is there any settled approach to defining the appropriate set
of macroprudential instruments, whether for banks or especially for non-banks, or the optimal strategy for
their use to address emerging vulnerabilities. If household debt externalities are within scope, is it better to
deal with this risk by restricting mortgage lending directly via loan to income or loan to value limits, or by
adding a macroprudential overlay to risk weights on mortgages (Turner (2017))? This lack of a settled
approach has some benefits, in making for a diverse range of cross-country experiences. This is giving rise
to a period of “learning by doing” among regulators. It does, however, come at some cost. A regime without
especially well-defined objectives is likely to suffer greater problems of time-inconsistency. It may also
increase uncertainty among outside participants about the likely regulatory policy reaction function.
The direction of travel, over time, probably needs to be towards somewhat clearer constraints, and
somewhat more circumscribed discretion, if macroprudential regimes are to be effective, robust and
transparent.
(g) Political Economy of Financial Regulation: The scope and range of regulatory responsibilities assigned to
central banks and regulators have expanded materially during the course of the crisis. Accompanying that,
some of the new regulatory requirements and practices put in place are quite discretionary in nature,
including stress-testing and some other macroprudential measures. A number involve regulators making
overtly distributional choices, for example around access to credit. This takes central banks and reguIators
more explicitly into the political-economy realm than at any time in their recent (and perhaps distant) history.
It has probably also contributed to some people questioning the appropriate scope of central banking, its
degree of independence from the political process and from wider society and appropriate accountability
mechanisms. There is a debate to be had, an analytical debate, about the appropriate degree of discretion
to confer on regulators, to ensure they retain the flexibility they need to respond to events while ensuring
All speeches are available online at www.bankofengland.co.uk/speeches
39
39
their decisions are clear, transparent and unpolluted by behavioural biases and time-inconsistency problems.
There are also interesting issues to explore about how regulators explain and account for their decisions to
wider society, particularly when they have strongly distributional consequences. This is clearly unfinished
business.
(h) The Contribution of the Financial System to the Economy and to Society: One of the striking features of
the past several decades has been the rising share of financial services in measures of economy-wide
value-added and, in tandem, rising financial sector balance sheets as a fraction of GDP in a number of
economies. Sometimes this goes by the name “financialisation”. There are good reasons to think increasing
financial depth is a natural feature of economies as they grow and develop. Indeed, there is a fairly
well-established literature quantifying the boost to growth and productivity which arises from financial depth,
especially for developing countries (Levine et al (2000)). Latterly, however, the question has been asked
whether it is possible to have too much of a good thing. Some have asked why the cost of financial
intermediation continues to rise and what this might signal about the efficiency of financial services as an
industry (Friedman (2009), Philippon (2015)). Others have pointed to a possible U-shaped relationship
between measures of financial depth and productivity and growth (Cecchetti and Kharroubi (2012), Heil
(2017)). These questions have an important bearing on the contribution the financial system makes to the
economy and to society. They are also meta-questions for regulatory policy. They warrant further research.
(i) Financial Stability Implications of FinTech: Technologically-enabled innovation in financial services, or
FinTech, has grown rapidly in recent years. The FSB’s recent report contains a useful taxonomy of such
innovations (FSB (2017f)). With this development comes the promise of greater consumer choice, improved
access to credit for some borrowers, and greater efficiency and productivity in the traditional intermediary
sector. There are also potential resilience benefits from increasing diversity in the provision of financial
services (Carney (2017a)). While the sector is probably too small at present to pose a threat to financial
stability, there is ample historical experience of risks emerging rapidly in fast-growing sectors if left
unchecked. Such future risks might include: conventional vulnerabilities associated with excessive use of
leverage and maturity, liquidity and credit transformation; the emergence of new highly-interconnected
entities; and cyber and other operational risks. There is also the potential for these developments to make
traditional universal banks less resilient, if they are forced to rely on less stable funding sources, for example.
A challenge for policymakers is to ensure that the regulatory regime, and the wider policy framework –
including the scope of central banks’ liquidity facilities – adapts to keep pace with these developments
(Lagarde (2017)).
All speeches are available online at www.bankofengland.co.uk/speeches
40
40
Annex
Chart 1: Level of real GDP in the Great Recession and Great Depression
US UK
Germany France
Sources: ONS, Bank of England ‘Millennium of Data’ (2017), IMF WEO, Maddison Historical GDP Data and Bank calculations.
Notes: ‘Continuation of pre-crisis’ trend is a simple extrapolation of GDP beyond 2007 using the average GDP growth from 1998 to 2007
for each country.
The strong rebound in GDP growth for Germany following the Great Depression will partly reflect the armament period in the run-up to
World War II.
60
70
80
90
100
110
120
130
140
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Continuation of pre-crisis trend (1998-07)
Years from start of crisis
Index, Year 0 = 100
90
100
110
120
130
140
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Continuation of pre-crisis trend (1998-07)
Years from start of crisis
Index, Year 0 = 100
60
70
80
90
100
110
120
130
140
150
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Continuation of pre-crisis trend (1998-07)
Years from start of crisis
Index, Year 0 = 100
60
70
80
90
100
110
120
130
140
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Continuation of pre-crisis trend (1998-07)
Years from start of crisis
Index, Year 0 = 100
All speeches are available online at www.bankofengland.co.uk/speeches
41
41
Chart 2: Government debt to GDP ratio in the Great Recession and Great Depression
US UK
Germany France
Sources: Bank of England ‘Millennium of Data’ (2017), IMF Historical Public Debt Database and Bank calculations.
Notes: Data for French government debt between 1934 and 1948 is not available in the IMF dataset.
0
5
10
15
20
25
30
35
40
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Change in government debt to GDP ratio
from start of crisis (percentage points)
-20
-10
0
10
20
30
40
50
60
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Change in government debt to GDP ratio
from start of crisis (percentage points)
Years from start of crisis
-4
-2
0
2
4
6
8
10
12
14
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Change in government debt to GDP ratio
from start of crisis (percentage points)
Years from start of crisis
0
5
10
15
20
25
30
35
40
0 1 2 3 4 5 6 7 8 9 10
Great Depression (Year 0 = 1929)
Great Recession (Year 0 = 2007)
Change in government debt to GDP ratio
from start of crisis (percentage points)
Years from start of crisis
Years from start of crisis
All speeches are available online at www.bankofengland.co.uk/speeches
42
42
Chart 3: Compliance with EDTF disclosure
Sources: Progress reports of the Enhanced Disclosure Task Force from 2013, 2014, and 2015; Bank calculations
Chart 4: G-SIB and D-SIB Tier 1 capital ratios
Sources: S&P Capital IQ and Bank calculations
Notes: Weighted average based on a sample of 189 banks which are systematically important as of 2016. Yearly is defined as the
fiscal year of reporting of the individual banks. Tier 1 Capital Ratio = Tier 1 Capital/Risk Weighted Assets
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Pre-EDTF 2012 2013 2014
Report
Aggregate implementation UK implementation
0
2
4
6
8
10
12
14
16
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
Per cent
Total GSIBs DSIBs
All speeches are available online at www.bankofengland.co.uk/speeches
43
43
Chart 5: G-SIB and D-SIB leverage ratios
Notes: Weighted average based on a sample of 189 banks which are systematically important as of 2016. Yearly is defined as the
fiscal year of reporting of the individual banks.
Chart 6: Major UK banks’ regulatory capital, MREL resources and requirements, % RWA (2011-2022)
Sources: FSA regulatory returns, MREL+ returns, Bank calculations
0
1
2
3
4
5
6
7
8
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
Per cent
Total GSIBs DSIBs
0%
5%
10%
15%
20%
25%
30%
35%
2011 2012 2013 2016 2020 2022
T1 + other total capital
resources (~T2)
MREL
resources
End-state
requirement
Interim
requirement
2014: UK bail-in
power introduced
All speeches are available online at www.bankofengland.co.uk/speeches
44
44
Chart 7: G-SIB and D-SIB high-quality liquid assets (liquid asset ratio)
Sources: S&P Capital IQ and Bank calculations
Notes: Weighted average based on a sample of 189 banks which are systematically important as of 2016. Yearly is defined as the
fiscal year of reporting of the individual banks. Liquid Asset Ratio = (Cash equivalents + government securities)/Total assets
Chart 8: G-SIB and D-SIB loan-to-deposit ratio
Sources: S&P Capital IQ and Bank calculations
Notes: Weighted average based on a sample of 189 banks which are systematically important as of 2016. Yearly is defined as the
fiscal year of reporting of the individual banks. Loan to Deposit Ratio = Loans granted by a bank/Stable deposits received. Stable
deposits are assumed to be a constant share of total deposits.
0
2
4
6
8
10
12
14
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
Per cent
Total GSIBs DSIBs
0
0.5
1
1.5
2
2.5
3
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
Total GSIBs DSIBs
Per cent
All speeches are available online at www.bankofengland.co.uk/speeches
45
45
Chart 9: CDS spreads for G-SIBs
Sources: Bloomberg; Bank calculations
Notes: Weighted average of 22 banks designated as G-SIBs as of 2016
Chart 10: G-SIB and D-SIB price-to-book ratio
Sources: Bloomberg; Bank calculations
Notes: Sample of 103 G-SIBs and D-SIBs designated as G-SIBs/D-SIBs as of 2016
0
50
100
150
200
250
2001 2003 2005 2007 2009 2011 2013 2015
Basis points
0
0.5
1
1.5
2
2.5
3
3.5
4
1994 1998 2002 2006 2010 2014
All Average GSIB DSIB
All speeches are available online at www.bankofengland.co.uk/speeches
46
46
Chart 11: Capital ratio using market value of equity
Sources: S&P Capital IQ, Bloomberg, and Bank calculations.
Notes: Weighted average based on a sample of 189 banks which are systematically important as of 2016. Yearly is defined as the fiscal
year of reporting of the individual banks. Market-based Capital Ratio = Market Capitalisation/Risk Weighted Assets
Chart 12: Estimates of implicit subsidy
Sources: Moody’s, Bank of America and Bank calculations.
Notes: Includes Barclays, HSBC, LBG and RBS. Calculated by multiplying the spread between indicative bonds at standalone and
supported credit ratings by the volume of ratings-sensitive liabilities; end-year data. Published in:
http://data.parliament.uk/writtenevidence/committeeevidence.svc/evidencedocument/treasury-committee/capital-andresolution/written/69208.pdf
0
5
10
15
20
25
2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
Per cent
GSIBs All DSIBs
0
1
2
3
4
2010 2011 2012 2013 2014 2015 2016
0
5
10
15
20
25
30
35
40
45
50
Notches of
ratings uplift
Value of
implicit
subsidy, £bn
Quantity of implicit subsidy (RHS) Average ratings uplift (LHS)
All speeches are available online at www.bankofengland.co.uk/speeches
47
47
Chart 13: Long-run UK and US leverage ratios
Sources: United States: Berger, A, Herring, R and Szegö, G (1995), 'The role of capital in financial institutions', Journal of Banking and
Finance, Vol 19(3-4), pages 393-430; SNL and Bank calculations. United Kingdom: Sheppard, D (1971), The growth and role of UK
financial institutions 1880-1962, Methuen, London; Billings, M and Capie, F (2007), 'Capital in British banking', 1920-1970, Business
History, Vol 49(2), pages 139-162; BBA, published accounts; FPC core indicators and Bank calculations
Notes: (a) US data show equity as a percentage of assets (ratio of aggregate dollar value of bank book equity to aggregate dollar value
of bank book assets). SNL data from 1996 on, based on accounting definition of equity.
(b) UK data on the capital ratio show equity and reserves over total assets on a time-varying sample of banks, representing the majority
of the UK banking system, in terms of assets. Prior to 1970 published accounts understated the true level of banks' capital because
they did not include hidden reserves. The solid line adjusts for this. 2009 observation is from H1. Data from 2010 onwards is based on
the simple leverage ratio published by the FPC (as of September 2017) as one of its core indicators for the countercyclical capital buffer.
(c) Change in UK accounting standards.
(d) International Financial Reporting Standards (IFRS) were adopted for the end-2005 accounts. The end-2004 accounts were also
restated on an IFRS basis. The switch from UK GAAP to IFRS reduced the capital ratio of the UK banks in the sample by
approximately 1 percentage point in 2004.
0
5
10
15
20
25
1880 1900 1920 1940 1960 1980 2000
Per cent
United
Kingdom(b)
United States(a)
(c) (d)
All speeches are available online at www.bankofengland.co.uk/speeches
48
48
Chart 14: Long-run UK liquidity ratio (sterling liquid assets relative to total holdings of UK banking
sector)
Sources: Bank of England and Bank calculations.
Notes: Data for building societies are included from 2010 onwards. Prior to this, data are for UK banks only. Data are end-year except
for 2017 where end-July data are used. Broad ratio defined as Cash + Bank of England balances + money at call + eligible bills + UK
gilts. Reserve ratio proxied as Bank of England balances + money at call + eligible bills. Narrow ratio defined as Cash + Bank of
England balances + eligible bills.
Chart 15: Individual banks’ 2006 capital positions and 2006-2016 lending growth
Sources: S&P Capital IQ and Bank calculations.
0
5
10
15
20
25
30
35
1968 72 76 80 84 88 92 96 00 04 08 12 16
Broad ratio
Reserve ratio
Narrow ratio
Percentage of total assets
(all currencies)
-200
0
200
400
600
5 10 15
Growth in net
loans (%), 2006
to 2016
Tier 1 capital ratio in 2006 (%)
All speeches are available online at www.bankofengland.co.uk/speeches
49
49
Chart 16: Change in bank capital and lending growth, 2010-2016
Sources: S&P Capital IQ and Bank calculations.
Chart 17: Leverage ratio as a function of the target hit rate
Sources: Buckmann, M., Kapadia, S., Kothiyal, A. and Bank calculations.
-100
0
100
200
300
-10 0 10 20 30
Growth in net
loans (%), 2010
to 2016
Tier 1 capital ratio, change from 2010 to 2016 (percentage points)
0
10
20
30
40
50
60
70
80
90
100
2 3 4 5 6
Target hit rate (%)
Leverage ratio (%)
All speeches are available online at www.bankofengland.co.uk/speeches
50
50
Chart 18: Receiver operating characteristic (ROC) curves for individual and combined regulatory
tools (76 bank sample with NSFR)
Sources: Buckmann, M., Kapadia, S., Kothiyal, A. and Bank calculations.
Chart 19: ROC curves for individual and combined regulatory tools (96 bank sample with LTD)
Sources: Buckmann, M., Kapadia, S., Kothiyal, A. and Bank calculations.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
LR RWCR NFSR LR + RWCR + NFSR
False alarm rate
Hit rate
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
LR RWCR LTD LR + RWCR + LTD
Hit rate
False alarm rate
All speeches are available online at www.bankofengland.co.uk/speeches
51
51
Table 5: Current international macroprudential tools (taken from Grace, Hallissey and Woods (2015))
Country Instrument used
Poland
Series of measures to limit FX-lending incl. higher DSTI ratios for FX-loans, higher risk weights for FX-loans.
Borrowers can only borrow same currency as income (Jan 14).
Canada
80% LTV cap with mandatory mortgage insurance, capped 25 year loan term, maximum total debt service ratio
of 44%. Time varying. Canada has taken four macroprudential policy measures since 2008 to tighten to these
requirements.
New Zealand Proportionate LTV cap at 80% (2013), temporary restriction. Core funding ratio of 75% in 2010.
Sweden
85% LTV cap (2010). Higher capital for mortgages (2013). 1% CCB on domestic exposures (2015). 3% SRB
and 2% Pillar 2 for 4 systemic banks (2015)
UK
Proportionate LTI cap at 4.5 times. 0.5% counter-cyclical capital buffer from June 2017, with expectation of
increasing to 1%.
Australia Increase in risk weights for self-verified mortgages and non-prime home loans in 2004.
Norway
LTV cap of 90% (Mar 2010), reduced to 85% (Dec 2011). Countercyclical capital buffer of 1% from June 2015.
2% capital buffer for systemically-important institutions (July 16) and systemic risk buffer of 3% (June 14).
Korea
LTV cap (2002), differentiated by property type, adjusted counter-cyclically. DTI cap (2005), differentiated by
property type, adjusted counter-cyclically. Macroprudential Stability Levy (MSL) (2011), price-based tax on
banks’ non-core foreign currency liabilities.
Croatia
A wide range of instruments including changes to reserve requirements, higher risk weights and liquidity
requirements on foreign currency exposures.
Hong Kong
LTV cap (1990s), differentiated by property and borrower type, adjusted counter-cyclically, in conjunction with
mortgage insurance. DTI cap.
Singapore
LTV cap for 1st (80%), 2nd (50%), subsequent (40%), and non-individual (20%) mortgages and mortgages with
a long term (from 2013). Mortgage servicing requirement of 30% (2013).
Switzerland Counter-Cyclical Capital buffer on real estate exposures (Feb 2013)
Finland 90% LTV cap; 95% for FTBs, based on fair value of all collateral.
Ireland Higher RW for higher LTV residential mortgages and on CRE lending (2006/7).
Netherlands
LTV cap of 100% by 2018 (Aug 2011).
Systemic risk buffer for large banking groups
Spain Rules-based dynamic provisioning introduced in 2000.
All speeches are available online at www.bankofengland.co.uk/speeches
52
52
Chart 20: Average 2007 risk weights, leverage and capital ratios of major EU, US and Canadian
banks(a)(b)
Sources: Bloomberg, FDIC, annual reports.
(a) Data as of end-2007. Sample includes Bank of America, Barclays, BMO, BNP Paribas, BONY, CIBC, Citigroup, Crédit Agricole,
Credit Suisse, Deutsche Bank, HBOS, HSBC, JPM, Lloyds, NBC, RBC, RBS, Santander, Scotiabank, Société Générale, State Street,
TD, UBS, UniCredit, Wachovia, Wells Fargo.
(b) Canadian and US banks’ balance sheet size is adjusted for IFRS.
Chart 21: Estimated increase in the liquidity component of corporate bond spreads under different
levels of initial redemptions (taken from Baranova et al (2017))
0
10
20
30
40
50
60
70
0% 20% 40% 60% 80% 100%
European Union
US
Canada
Tier 1 /
Assets
RWA / Assets
All speeches are available online at www.bankofengland.co.uk/speeches
53
53
Chart 22: Usage of key macroprudential tools since 2010
Sources: National central bank websites, IMF, European Commission Shim et al 2013, ESRB and MDB.
Note: This chart documents the use of the CCyB, sectoral capital requirements (SCR), liquidity-based tools, and lending standards
(including loan to income and loan to value ratio limits) over the period 2007-2017. Countries included: Australia; Brazil; Canada; China;
France; Germany; India; Italy; Japan; Korea; Netherlands; Norway; Singapore; Spain; Sweden; Switzerland; United Kingdom; United
States.
Chart 23: Intertemporal trade-off with monetary policy and the CCyB (taken from Aikman, Giese,
Kapadia, & McLeay (forthcoming))
0
5
10
15
20
25
30
35
40
2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
SCR CCyB Liquidity Lending standards
Instances of use
All speeches are available online at www.bankofengland.co.uk/speeches
54
54
References
Acharya, V. V., & Steffen, S. (2015). The “greatest” carry trade ever? Understanding eurozone bank risks. Journal of Financial
Economics, 115(2), 215-236.
Acosta-Smith, J., Grill, M., & Lang, J. H. (2017). The leverage ratio, risk-taking and bank stability. ECB Working Paper Series, No. 2079.
Admati, A. R., & Hellwig, M. F. (2011). Good Banking Regulation Needs Clear Focus, Sensible Tools, and Political Will. International
Centre for Financial Regulation Research Paper.
Admati, A., & Hellwig, M. (2013). The bankers' new clothes: What's wrong with banking and what to do about it. Princeton University
Press.
Adrian, T., & Ashcraft, A. B. (2012). Shadow banking regulation. Annu. Rev. Financ. Econ., 4(1), 99-140.
Afonso, G., Santos, J. A., & Traina, J. (2015). Do “too-big-to-fail” banks take on more risk? Journal of Financial Perspectives, 3(2), 129-
143.
Aikman, D., Bahaj, S., Bridges, J., Ikeda, D. & Siegert, C. (forthcoming). What impact did the FPC's July 2016 CCyB rate cut have on
the supply of bank credit? mimeo.
Aikman, D., Galesic, M., Gigerenzer, G., Kapadia, S., Katsikopoulos, K., Kothiyal, A., Murphy, E., & Neumann, T. (2014). Taking
uncertainty seriously: simplicity versus complexity in financial regulation. Bank of England Financial Stability Paper No. 28,
May 2014.
Aikman, D., Giese, J., Kapadia, S., & McLeay, M. (forthcoming). Targeting Financial Stability: Macroprudential or Monetary Policy?
Aikman, D., Haldane, A. G., & Nelson, B. D. (2015). Curbing the credit cycle. The Economic Journal, 125(585), 1072-1109.
Aikman, D., Haldane, A. G., & Kapadia, S. (2013). Operationalising a macroprudential regime: goals, tools and open issues. Banco de
España Financial Stability Review, 24, 9-30.
Aikman, D., Lehnert, A., Liang, N. & Modugno , M. (2016). Financial Vulnerabilities, Macroeconomic Dynamics, and Monetary Policy.
Finance and Economics Discussion Series 2016-055. Washington: Board of Governors of the Federal Reserve System.
Aikman, D., Nelson, B., & Tanaka, M. (2015). Reputation, risk-taking, and macroprudential policy. Journal of Banking & Finance, 50,
428-439.
Aiyar, S., Calomiris, C. W., & Wieladek, T. (2014). Does Macro‐Prudential Regulation Leak? Evidence from a UK Policy Experiment.
Journal of Money, Credit and Banking, 46(s1), 181-214.
Aiyar, S., Calomiris, C. W., & Wieladek, T. (2016). How does credit supply respond to monetary policy and bank minimum capital
requirements? European Economic Review, 82, 142-165.
Ajello, A., Laubach, T., Lopez-Salido, J. D., & Nakata, T. (2016). Financial stability and optimal interest-rate policy. Federal Reserve
Board Working Paper.
Anderson, R. M. & May, R. M. (1992). Infectious diseases of humans: dynamics and control (Vol. 28). Oxford: Oxford University Press.
Arestis, P., & Mihailov, A. (2009). Flexible Rules cum Constrained Discretion: A New Consensus in Monetary Policy. Economic Issues,
14(2).
Arinaminpathy, N., Kapadia, S., & May, R. M. (2012). Size and complexity in model financial systems. Proceedings of the National
Academy of Sciences, 109(45), 18338-18343.
Arjani, N., & Paulin, G. (2013). Lessons from the financial crisis: bank performance and regulatory reform. Bank of Canada Discussion
Paper, No. 2013-4.
Avery, C., & Zemsky, P. (1998). Multidimensional uncertainty and herd behavior in financial markets. American Economic Review, 724-
748.
Bahaj, S., Bridges, J., Malherbe, F., & O'Neill, C. (2016). What determines how banks respond to changes in capital requirements?
Bank of England Staff Working Paper, No. 593.
Bailey, M., Cao, R., Kuchler, T. and Stroebel, J., (forthcoming). Social networks and housing markets. Journal of Political Economy.
Banerjee, R. N., & Mio, H. (2017). The impact of liquidity regulation on banks. Journal of Financial Intermediation.
Bank of England (2009). The role of macroprudential policy. A Discussion Paper. November 2009.
Bank of England (2011). Instruments of macroprudential policy. A Discussion Paper. December 2011.
Bank of England (2013). Monetary policy trade-offs and forward guidance. August 2013.
Bank of England (2014). Stress testing the UK banking system: 2014 results. December 2014.
Bank of England (2015a). The Financial Policy Committee’s powers over leverage ratio tools. Policy Statement. July 2015.
All speeches are available online at www.bankofengland.co.uk/speeches
55
55
Bank of England (2015b). One Bank Research Agenda. Discussion Paper.
Bank of England (2015c). Supplement to the December 2015 Financial Stability Report: The framework of capital requirements for UK
banks.
Bank of England (2016a). The Financial Policy Committee’s approach to setting the countercyclical capital buffer: A policy statement.
April.
Bank of England (2016b). Financial Stability Report. July 2016 | Issue No. 39.
Bank of England (2017). The Bank of England’s approach to resolution.
Baranova, Y., Coen, J., Lowe, P., Noss, J., & Silvestri, L. (2017). Simulating stress across the financial system: the resilience of
corporate bond markets and the role of investment funds. Bank of England Financial Stability Paper No. 42, July 2017.
Baranova, Y., Liu, Z., & Shakir, T. (2017). Dealer intermediation, market liquidity and the impact of regulatory reform. Bank of England
Staff Working Paper No. 665.
Barro, R. J., & Gordon, D. B. (1983). Rules, discretion and reputation in a model of monetary policy. Journal of Monetary Economics,
12(1), 101-121.
BCBS (2010a). Guidance for national authorities operating the countercyclical capital buffer. December 2010.
BCBS (2010b). An assessment of the long-term economic impact of stronger capital and liquidity requirements. August 2010.
BCBS (2013a). Basel III: The Liquidity Coverage Ratio and liquidity risk monitoring tools.
BCBS (2013b). The regulatory framework: balancing risk sensitivity, simplicity and comparability. Discussion paper.
BCBS (2014a). Analysis of the trading book hypothetical portfolio exercise. September 2014.
BCBS (2014b). Basel III: the net stable funding ratio.
BCBS (2016). Reducing variation in credit risk-weighted assets – constraints on the use of internal model approaches. Consultative
Document.
BCBS (2017). The Basel Committee's work programme. Updated 25 April 2017. Retrieved at www.bis.org/bcbs/bcbs_work.htm.
Behn, M., Haselmann, R. F., & Vig, V. (2016). The limits of model-based regulation.
Behn, M., Haselmann, R., & Wachtel, P. (2016). Procyclical capital regulation and lending. The Journal of Finance, 71(2), 919-956.
Beltratti, A. & Stulz, R. (2012). The credit crisis around the globe: Why did some banks perform better? Journal of Financial Economics,
Vol. 105(1), 1-17.
Benartzi, S. & Thaler, R. (2001). Naïve Diversification Strategies in Defined Contribution Saving Plans. American Economic Review, Vol
98(1), 79-98.
Benigno, G., Chen, H., Otrok, C., Rebucci, A., & Young, E. R. (2013). Financial crises and macro-prudential policies. Journal of
International Economics, 89(2), 453-470.
Benetton, M., Eckley, P., Garbarino, N., Kirwin, L., & Latsi, G. (2017). Specialisation in mortgage risk under Basel II. Bank of England
Staff Working Paper No. 639.
Berger, A. N., & Bouwman, C. H. (2013). How does capital affect bank performance during financial crises? Journal of Financial
Economics, 109(1), 146-176.
Berger, A., Herring, R. and Szegö, G. (1995). The role of capital in financial institutions. Journal of Banking and Finance, Vol 19(3-4),
393-430.
Bernanke, B. S., & Mishkin, F. S. (1997). Inflation Targeting: A New Framework for Monetary Policy? The Journal of Economic
Perspectives, 11(2), 97-116.
Bianchi, J., & Mendoza, E. G. (2010). Overborrowing, financial crises and 'macro-prudential' taxes. National Bureau of Economic
Research Working Paper No. w16091.
Bicu, A., Chen, L., & Elliott, D. (forthcoming). The leverage ratio and liquidity in the gilt and repo markets. Bank of England Staff Working
Paper.
Bikhchandani, S., & Sharma, S. (2001). Herd Behavior in Financial Markets. IMF Staff Papers, 47(3).
Billings, M. and Capie, F. (2007). Capital in British banking. 1920-1970, Business History, Vol 49(2), 139-162.
Boar, C., Gambacorta, L., Lombardo, G., and Pereira da Silva, L. (2017). Finance and macroeconomic performance. Is there a role for
macroprudential policies? Mimeo, BIS.
Bologna, P. (2011). Is There a Role for Funding in Explaining Recent U.S. Banks’ Failures? IMF Working Paper WP/11/180.
International Monetary Fund.
All speeches are available online at www.bankofengland.co.uk/speeches
56
56
Borio, C. (2010). Implementing a macroprudential framework: Blending boldness and realism. Speech at the BIS-HKMA research
conference on Financial Stability: Towards a Macroprudential Approach, Hong Kong SAR, 5-6 July.
Borio, C. E., & Lowe, P. W. (2002). Asset prices, financial and monetary stability: exploring the nexus.
Borio, C. E., & Lowe, P. W. (2004). Securing sustainable price stability: should credit come back from the wilderness?
Boyson, N., Helwege, J., & Jindra, J. (2014). Crises, liquidity shocks, and fire sales at commercial banks. Financial Management, 43(4),
857-884.
Brainard, W. C. (1967). Uncertainty and the Effectiveness of Policy. The American Economic Review, 57(2), 411-425.
Brandao-Marques, L., Correa, R., & Sapriza, H. (2013). International evidence on government support and risk taking in the banking
sector. IMF Working Paper WP/13/94. International Monetary Fund.
Braun-Munzinger, K., Liu, Z., & Turrell, A. (2016). An agent-based model of dynamics in corporate bond trading. Bank of England Staff
Working Paper No. 592.
Brazier, A. (2015). The Bank of England’s approach to stress testing the UK banking system. Speech given at London School of
Economics Systemic Risk Centre, 30 October.
Brazier, A. (2017a). How to MACROPRU. 5 principles for macroprudential policy. Speech given at the London School of Economics
Financial Regulation Seminar.
Brazier, A. (2017b). Foreword. In Baranova, Y., Coen, J., Lowe, P., Noss, J., & Silvestri, L. Simulating stress across the financial
system: the resilience of corporate bond markets and the role of investment funds. Bank of England Financial Stability Paper
No. 42, July 2017, 3-6.
Bridges, J., Jackson, C., & McGregor, D. (2017). Down in the slumps: the role of credit in five decades of recessions. Bank of England
Staff Working Paper No. 659.
Brooke, M., Bush, O., Edwards, R., Ellis, J., Francis, B., Harimohan, R., Neiss, K. &

During the summer months, I have my hair cut slightly shorter than during the winter months. This helps 
keep my head a little cooler in the summer heat and my ears a little warmer in the winter cold. Anyone who 
has seen them will know this is a smart strategy for my ears, to say nothing of my head. This strategy is 
explicitly counter-cyclical. When the temperature goes up, my hair-length comes down and vice-versa. I am 
not alone. National statistics show that expenditure on haircuts increases as temperatures rise. 
The financial system also makes choices about haircuts. The haircuts in question are the amount of 
collateral a borrower places with the lender over and above the face value of borrowing. But collateral 
haircuts tend to behave rather differently to personal haircuts. They fall when the financial temperature is 
increasing and rise when the chill sets in. This strategy is explicitly pro-cyclical. It will tend to result in 
financial markets being hot-headed in the summer and frozen-eared in the winter. 
Table 1 demonstrates this pattern. It compares haircuts on a range of financial instruments used to back 
borrowing – so-called securities financing. They are shown on two dates, before (June 2007) and after 
(June 2009) the financial crisis. Haircuts rose by up to 90 percentage points in the space of these two years, 
as the scorching pre-crisis summer gave way to a frozen crisis winter. In other words, haircuts exhibited a 
rather dramatic pro-cyclicality over the course of the crisis. 
Table 1: Typical haircut on term securities financing transactions (per cent) 
 June 20071 June 20091
Medium-term G7 government 
bonds 
0 1 
Medium-term US agencies 1 2 
AAA-rated prime MBS 4 10 
Asset-backed securities 10 25 
AAA-rated structured products 10 100 
AAA- and AA-rated investment 
grade bonds 
1 8 
High-yield bonds 8 15 
G7 countries equity 10 15 
Source: Committee on the Global Financial System (2010). 
1 Prime counterparty. 
This haircut cycle played an important causal role in the crisis. Secured financing became an increasingly 
important source of credit in both bank and non-bank markets over the past decade. In the US, the repo 
market financed roughly half of the growth in investment banks’ balance sheets between 2002 and 2007. In 
the UK, the securitisation market trebled in size over the same period. Those were the heady days of 
summer. Since then the US repo market has shrunk by 40%, while the UK securitisation market remains 
frozen. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
3
3
Pro-cyclicality in the haircuts applied to secured financing transactions in turn amplified the cycle in credit. 
Thin haircuts made it cheaper for banks to mobilise collateral to finance borrowing when the credit cycle was 
in the upswing, adding momentum to the upward pendulum of asset prices and credit. And fat haircuts 
immobilised collateral when the credit cycle reversed, exaggerating the downward pendulum swing. 
A further factor amplifying these swings came from the fact that much of the secured financing took place 
within the financial sector. In the run-up to crisis, banks and near-banks entered into secured financing 
transactions with one another, inflating both counterparties’ balance sheet. In the UK, fully two-thirds of the 
trebling in balance sheets between 2002 and 2007 can be explained by expanding claims on other parts of 
the financial system. 
These cross-system dependencies in financing have “externality-like” effects. During a generalised boom, 
individual banks lend freely to one another and all of the moving parts of the financial system are lubricated. 
The system co-ordinates itself on a high liquidity equilibrium. Liquidity has positive spillover effects or 
externalities; liquidity is a public good. And with rising asset prices and liquidity and falling haircuts, the 
credit multiplier is high and self-reinforcing. 
But that cycle can just as quickly reverse. In a situation of stress, individual banks hoard rather than lend 
liquidity. The system then co-ordinates itself on a low liquidity equilibrium. Liquidity then has negative 
spillovers, imposing negative externalities; illiquidity is a public bad. And with asset prices and liquidity 
falling and haircuts rising, the credit multiplier becomes self-reinforcing downwards. 
Given these pro-cyclicalities, there have been recent proposals by both policymakers and academics to 
regulate collateral requirements.1
 This is one possible arm of so-called macroprudential policy. Within the 
UK, haircuts on secured financing or OTC derivative transactions have been identified as one possible tool 
for executing macroprudential policy.2
 The UK’s new interim Financial Policy Committee, housed in the Bank 
of England, will provide advice to government on possible macroprudential tools over the next year or so. 
The debate on haircuts as a policy tool is live internationally too. In a recent speech in Atlanta, US Treasury 
Secretary Tim Geithner proposed the introduction of international minimum standards for margins on 
derivatives transactions.3
 This would be akin to the international minimum standards for capital adequacy 
introduced through the Basel agreements. Since 1934, the US authorities have had regulatory powers to 
impose minimum margin requirements on lending against equity and some other assets – so-called 
Regulation T. But these policies have not been used actively, with minimum margins unchanged since 1974. 
 
1
 For example, Committee on the Global Financial System (2010), “The role of margin requirements and haircuts in procyclicality”, 
CGFS Publications No. 36, Geanakoplos, J (2010), “Solving the present crisis and managing the leverage cycle”, Federal Reserve Bank 
of New York Economic Policy Review, August, 16 (1) and Kashyap, A K, Berner, R and Goodhart, C, “The Macroprudential Toolkit”, 
forthcoming in the IMF Economic Review. 2
 http://www.hm-treasury.gov.uk/d/consult_newfinancial_regulation170211.pdf. 
3
 http://www.treasury.gov/press-center/press-releases/Pages/tg1202.aspx. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
4
4
To date, analysis of the macroprudential role of haircuts has been largely descriptive. A recent paper, 
co-authored with Prasanna Gai and Sujit Kapadia, attempts to fill that gap.4
 We develop a model of a 
banking network, inter-connected through unsecured interbank lending and secured funding markets. This 
financial web exhibits classical tipping point properties. It is even-tempered most of the time. Indeed, having 
a tight-knit circle of financial friends helps keep the financial system strong and stable. Risks are diffused 
across the system. A problem shared is a problem halved. The system is in a high-liquidity equilibrium. 
But, on occasion, the system can be pushed beyond its tipping point. Connectivity then generates 
contagion. A problem shared is a problem multiplied. The best of financial friends become the worst of 
enemies. In the model, one of the key channels for contagion is the secured financing market as banks 
hoard rather than lend liquidity when haircuts rise. The liquidity feast then turns to famine as secured and 
unsecured financing markets dry up. The system switches to a low-liquidity equilibrium. These liquidity 
droughts were perhaps the defining feature of the financial crisis during 2007 and 2008. 
In the model, the likelihood of such systemic liquidity crises depends critically on two key structural 
characteristics of the financial system – the two c’s: concentration and complexity. The greater the 
concentration within the financial system, the greater the potential for systemic collapse as larger banks 
spread a disproportionate amount of financial pain around a densely networked financial system. A greater 
degree of system complexity has a similar effect, creating more channels for contagion and heightening 
banks’ incentives to hoard liquidity when the weather worsens. The two c’s are very much features of 
today’s financial system. On the face of it, that bodes ill for future systemic crises. 
So what role might policy play in avoiding those crises? The model provides a test-bed to consider a range 
of policy options, including for haircuts policy. An illustrative policy experiment, based on a simulation of the 
model in which haircuts spike sharply, is shown in Chart 1. Up the vertical axis is a measure of the 
probability of a systemic liquidity crisis. Along the horizontal axis is the initial size of the haircut on secured 
financing transactions. 
The three lines trace three policy options. Consider first the “baseline” path without policy intervention. 
Provided haircuts remain high come rain or shine – in the example, above around 20% - the probability of a 
liquidity crisis remains very low. But if haircuts are lowered even modestly, resilience is eroded. In the 
example, lowering haircuts from 20% to 5% raises the crisis probability by an order of magnitude, from less 
than 0.1 to almost 1. In other words, a hands-off haircuts policy runs a significant risk of systemic collapse if 
haircuts are pro-cyclically trimmed during the upswing. 
To prevent this, policy makers could seek to control haircuts directly. In the example, significantly greater 
resilience could be achieved by imposing a haircut limit of 20%. Alternatively, policy makers could seek to 
 
4
 “Complexity, Concentration and Contagion”, which is forthcoming in the Journal of Monetary Economics, Vol 58 (5). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
5
5
lean against the pro-cyclical tendencies of the financial system as systemic risk increases. The other two 
lines in Chart 1 show different sets of activist policy intervention. Both require banks to build up larger liquid 
asset buffer stocks in the event of haircuts being trimmed. This lessens the need for banks to hoard liquidity, 
so reducing system-wide liquidity stress. 
The two policies differ in how aggressively they respond to such excesses – “weak” and “tough”. Either way, 
the effects are striking. Even the weak policy shrinks the probability of collapse by more than half, whatever 
the initial level of haircuts. And under the tough policy, the financial system is effectively inoculated against 
haircut-induced pro-cyclicality. 
It may be over-optimistic to think policy could eliminate the adverse effects of haircut-induced pro-cyclicality. 
For example, we have little theoretical understanding of how haircut-based policies might affect banks’ 
behaviour. And we have little empirical case law on the implementation of these policies. For example, 
haircuts policy might be circumvented by banks substituting towards unsecured finance. So any quantitative 
calibration of the effects of a haircut-based policy rule is necessarily tentative. Nonetheless, the model 
simulations are suggestive – and, at least qualitatively, encouraging. 
Chart 1: Aggregate haircuts and probability of a systemic liquidity crisis 
Source: Gai, Haldane and Kapadia (2011). 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
25 20 15 10 5 0
Probability of a systemic liquidity crisis
Initial aggregate haircut (h) 
Baseline
'Weak' haircut-dependent liquidity requirement
'Tough' haircut-dependent liquidity requirement
Baseline
Weak
Tough
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
6
6
This financial network set-up can be used to assess a number of other topical policy issues. These include: 
 Central clearing: Central clearing of financial transactions tackles the complexity of, and 
concentration within, the financial system at source. The dense knitting of the financial web is 
unravelled and replaced with a simple hub-and-spokes topology. This will tend to reduce the liquidity 
hoarding incentives of investors, so making for more resilient funding markets in the event problems 
strike. Provided the central clearing house is beyond reproach, this topology also reduces the 
contagious consequences of individual bank stress: liquidity cascades are headed-off at the clearing 
house. The proviso is important, though. Robustness of the central clearing basket becomes more 
important as more eggs are placed in it. This calls for a sea-change improvement in risk 
management practices among the clearing houses, as more financial transactions come to be 
centrally cleared to meet G20 commitments. Otherwise the too-important-to-fail problem simply reemerges in a different guise. 
 Liquidity policy: One implication of the model is the importance of banks maintaining buffers of the 
highest-quality liquid assets. These have a doubly beneficial effect. First, they reduce banks’ need 
to hoard liquidity in situations of stress. Second, high-quality assets reduce the amplitude of the 
haircut cycle in the first place, as Table 1 demonstrates. The first reduces the impact of a liquidity 
event, the second its probability. The upshot is a fall in the chances of a systemic liquidity crisis. 
The new Basel III liquidity regulation uses a strict definition of what counts as liquid assets. The 
model underlines the importance of sticking to that definition. The model suggests that financial 
system robustness would be further enhanced by targeting liquidity requirements on the most 
connected banks in the system, as this would lean against the destabilising effects of system-wide 
concentration. 
Hot heads make for bad decisions, frozen ears for uncomfortable ones. Financial markets have felt the 
effects of this change in the weather more dramatically than most. Suitably designed, macroprudential policy 
can help moderate those swings in temperature, thereby improving the health of the financial system.

Stock prices can go down as well as up. Never in financial history has this adage been more apt than on 
6 May 2010. Then, the so-called “Flash Crash” sent shocks waves through global equity markets. The 
Dow Jones experienced its largest ever intraday point fall, losing $1 trillion of market value in the space of 
half an hour. History is full of such fat-tailed falls in stocks. Was this just another to add to the list, perhaps 
compressed into a smaller time window? 
No. This one was different. For a time, equity prices of some of the world’s biggest companies were in 
freefall. They appeared to be in a race to zero. Peak to trough, Accenture shares fell by over 99%, from $40 
to $0.01. At precisely the same time, shares in Sotheby’s rose three thousand-fold, from $34 to $99,999.99. 
These tails were not just fatter and faster. They wagged up as well as down. 
The Flash Crash left market participants, regulators and academics agog. More than one year on, they 
remain agog. There has been no shortage of potential explanations. These are as varied as they are many: 
from fat fingers to fat tails; from block trades to blocked lines; from high-speed traders to low-level abuse. 
From this mixed bag, only one clear explanation emerges: that there is no clear explanation. To a first 
approximation, we remain unsure quite what caused the Flash Crash or whether it could recur.1
That conclusion sits uneasily on the shoulders. Asset markets rely on accurate pricing of risk. And financial 
regulation relies on an accurate reading of markets. Whether trading assets or regulating exchanges, 
ignorance is rarely bliss. It is this uncertainty, rather than the Flash Crash itself, which makes this an issue of 
potential systemic importance. 
In many respects, this uncertainty should come as no surprise. Driven by a potent cocktail of technology and 
regulation, trading in financial markets has evolved dramatically during the course of this century. Platforms 
for trading equities have proliferated and fragmented. And the speed limit for trading has gone through the 
roof. Technologists now believe the sky is the limit. 
This rapidly-changing topology of trading raises some big questions for risk management. There are good 
reasons, theoretically and empirically, to believe that while this evolution in trading may have brought 
benefits such as a reduction in transaction costs, it may also have increased abnormalities in the distribution 
of risk and return in the financial system. Such abnormalities hallmarked the Flash Crash. This paper 
considers some of the evidence on these abnormalities and their impact on systemic risk. 
Regulation has thin-sliced trading. And technology has thin-sliced time. Among traders, as among stocks on 
6 May, there is a race to zero. Yet it is unclear that this race will have a winner. If it raises systemic risk, it is 
 
1
 For a regulatory perspective see CFTC-SEC (2010), for a market perspective see Automated Trader (2010) and for an academic 
perspective see Easley et al (2011b). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
3 
3
possible capital markets could be the loser. To avoid that, a redesign of mechanisms for securing capital 
market stability may be needed. 
 
2. The Topology of Trading 
During the course of this century, financial market trading has undergone a transformation. This has been 
driven in part by technology and in part by regulation. The key words are structure and speed. Both the 
structure of the markets in which participants operate, and the behaviour of those participants, has 
undergone a phase shift. That highly adaptive topology of trading has made understanding markets a more 
hazardous science than a decade ago. 
Chart 1 plots equity market capitalisation relative to nominal GDP in the United States, Europe and Asia 
through this century. On the face of it, it paints a rather unexciting picture. Equity market values relative to 
GDP in the US are roughly where they started the century. In Asia there is evidence of some deepening of 
equity markets relative to the whole economy but it is pretty modest. 
Measures of equity market capitalisation to GDP have often been used as proxies for the contribution of 
financial development to economic growth.2
 These effects are typically found to be significant. By that 
metric, the contribution of equity markets to economic growth in the US, Europe and Asia has been static, at 
best mildly positive, during the course of this century. 
Yet that picture of apparent stasis in equity markets conceals a maelstrom of activity beneath the surface. 
To see this, Chart 2 plots stock market turnover in the US, Europe and Asia over the same period. It shows 
a dramatic rise, especially in the world’s most mature equity market, the United States. Equity market 
turnover in the US has risen nearly fourfold in the space of a decade. Put differently, at the end of the 
second World War, the average US share was held by the average investor for around four years. By the 
start of this century, that had fallen to around eight months. And by 2008, it had fallen to around two months. 
What explains this story? Regulation is part of it. Two important, and almost simultaneous, regulatory 
developments on either side of the Atlantic changed fundamentally the trading landscape: in the US, 
Regulation NMS (National Market System) in 2005; and in Europe, MiFID (Markets in Financial Instruments 
Directive) in 2004. Though different in detail, these regulatory initiatives had similar objectives: to boost 
competition and choice in financial market trading by attracting new entrants. 
Central exchanges for the trading of securities evolved from the coffee houses of Amsterdam, London, New 
York and Paris in the 17th century. From those foundations emerged the physical exchanges which 
dominated the period from the 18th right through to the 20th centuries. Central trading exchanges maintained 
 
2
 Arestis and Demetriades (1997). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
4 
4
their near-monopoly status for 300 years. In the space of a few years, that monopoly has been decisively 
broken. 
A diverse and distributed patchwork of exchanges and multilateral trading platforms has emerged in its 
place. These offer investors a range of execution characteristics, such as speed, cost and transparency, 
typically electronically. Equity market trading structures have fragmented. This has gone furthest in the US, 
where trading is now split across more than half a dozen exchanges, multilateral trading platforms and “dark 
pools” of anonymous trading (Charts 3 and 4). Having accounted for around 80% of trading volume in 
NYSE-listed securities in 2005, the trading share of the NYSE had fallen to around 24% by February 2011. 
A similar pattern is evident across Europe. In the UK, the market share of the London Stock Exchange has 
fallen from two-thirds in June 2008 to less than one third today (Charts 5 and 6). The same pattern is found 
in Germany and France. In Asia, there is as yet less fragmentation. 
Accompanying this structural evolution in trading has been a technological revolution. Electronic trading is 
not new. The first electronic exchange (NASDAQ) is already over forty years old. But advances in 
computing power have shifted decisively the frontier of electronic, and in particular algorithmic, trading over 
the past few years. That frontier is defined by speed. 
The average speed of order execution on the US NYSE has fallen from around 20 seconds a decade ago to 
around one second today. These days, the lexicon of financial markets is dominated by talk of 
High-Frequency Trading (HFT). It is not just talk. As recently as 2005, HFT accounted for less than a fifth of 
US equity market turnover by volume. Today, it accounts for between two-thirds and three-quarters. 
The picture is similar, if less dramatic, in Europe. Since 2005, HFT has risen from a tiny share to represent 
over 35% of the equity market. In Asia and in emerging markets, it is growing fast from a lower base. What 
is true across countries is also true across markets. HFT is assuming an ever-increasing role in debt and 
foreign exchange markets. In some futures markets, it already accounts for almost half of turnover. In the 
space of a few years, HFT has risen from relative obscurity to absolute hegemony, at least in some markets. 
HFT itself is far from monolithic, comprising a range of strategies.3
 Some involve high-speed liquidity 
provision, which is akin to market-making. Others involve statistical arbitrage, using trading algorithms to 
detect and exploit pricing anomalies between stocks or markets. Because these anomalies tend to be 
eliminated quickly, HFT algorithms have to be highly adaptive, not least to keep pace with the evolution of 
new algorithms. The half-life of an HFT algorithm can often be measured in weeks. 
 
3
 Mackenzie (2011) provides a brilliant recent account of the role of HFT, in particular in the context of the Flash Crash. HFT is a 
sub-set of a broader class of algorithmic trading strategies. See also Aldridge (2010). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
5 
5
One variant of these arbitrage strategies exploits pricing differences between common securities quoted on 
competing trading platforms. For that reason, HFT firms tend to have their tentacles spread across multiple 
trading venues, arbitraging tiny differences in price (Chart 7). These strategies have grown up as a direct 
response to the fragmentation of trading infrastructures. In other words, HFT is at least in part the (possibly 
unplanned) progeny of regulators pursuing competitive ends. 
The ascent of HFT goes a long way towards explaining the rise in equity market turnover in the major equity 
markets and in particular the rise in number, and fall in the average size, of trades executed. Put differently, 
the trading behaviour of HFT has contributed to the downward fall in the average duration of stock holdings. 
HFT holding periods lie in a narrow time range. The upper bound is perhaps around one day. The lower 
bound is a perpetual downward motion machine, as computing capacity compresses the timeline for trading. 
A decade ago, execution times on some electronic trading platforms dipped decisively below the one second 
barrier. As recently as a few years ago, trade execution times reached “blink speed” – as fast as the blink of 
an eye. At the time that seemed eye-watering, at around 300-400 milli-seconds or less than a third of a 
second. But more recently the speed limit has shifted from milli-seconds to micro-seconds – millionths of a 
second. Several trading platforms now offer trade execution measured in micro-seconds (Table 1). 
As of today, the lower limit for trade execution appears to be around 10 micro-seconds. This means it would 
in principle be possible to execute around 40,000 back-to-back trades in the blink of an eye. If supermarkets 
ran HFT programmes, the average household could complete its shopping for a lifetime in under a second. 
Imagine. 
It is clear from these trends that trading technologists are involved in an arms race. And it is far from over. 
The new trading frontier is nano-seconds – billionths of a second. And the twinkle in technologists’ 
(unblinking) eye is pico-seconds – trillionths of a second. HFT firms talk of a “race to zero”. This is the 
promised land of zero “latency” where trading converges on its natural (Planck’s) limit, the speed of light.4
 
The race to zero seems like a million miles from the European coffee shop conversations of the 17th century 
and the noisy trading floors of the 18th, 19th and 20th centuries. The dawn of electronic trading coincided with 
the dusk for floor trading. Physical proximity lost its allure. As soon as computers processed faster than 
traders talked, the time was up for human interactions on physical exchanges. Trading became virtual, 
distance a dinosaur. 
Or so it seemed. Latterly, however, HFT is turning that logic on its head. The race to zero has encouraged 
traders to eke out the last pico-second. And one way to do that is by limiting physical distance. The shorter 
 
4
 “Latency” refers to the time it takes from sending an order to it being executed. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
6 
6
the cable to the matching engine of the trading exchange, the faster the trade. Every 100 miles might add a 
milli-second to estimated execution times. For HFT, that is the difference between the tortoise and the hare. 
The recognition of that has led to a phenomenon known as “co-location”. HFT firms have begun to relocate 
their servers as close as physically possible to the trade-matching engine. That allows them to eke a pico 
over their (non co-located) competitors. For a price, a number of exchanges now offer co-located services, 
with a perimeter strictly enforced, including the NYSE, Euronext and the London Stock Exchange. 
This has added a new dimension to the “adverse selection” problem in economics – of uninformed traders 
suffering at the hands of the informed. Being informed used to mean being smarter than the average bear 
about the path of future fundamentals – profits, interest rates, order flow and the like. Adverse selection risk 
meant someone having a better informed view on these fundamentals. 
Adverse selection risk today has taken on a different shape. In a high-speed, co-located world, being 
informed means seeing and acting on market prices sooner than competitors. Today, it pays to be faster 
than the average bear, not smarter. To be uninformed is to be slow. These uninformed traders face a 
fundamental uncertainty: they may not be able to observe the market price at which their trades will be 
executed. This is driving through the rear-view mirror, stock-picking based on yesterday’s prices. 
Co-location speeds up the clock. But it has also had the effect of turning it back. Location matters once 
more. The race to zero has become a physical, as well as a virtual, one. Distance matters more than ever. 
The dinosaur has been resurrected, this time in high definition. In some ways, it is the ultimate contradiction. 
In sum, through this century changes in the structure of trading, and in the behaviour of traders, have gone 
hand in hand. Liberalisation and innovation have delivered fragmentation of structure and transformation of 
speed. Both structure and speed have experienced a high-velocity revolution. So what impact has this race 
to zero had on market dynamics? 
3. From Microstructure to Macrostructure 
This is difficult detective work. But there are theoretical clues and empirical fingerprints. The theoretical 
clues come from a literature that flourished after the stock market crash of 1987 – the so-called market 
microstructure literature.5
 This departs from the notion of frictionless trading and information-efficient prices. 
In its place it introduces frictions in the price-setting process, which arise from characteristics of market 
participants (such as their trading speed) and of the trading infrastructure (such as its degree of 
fragmentation). 
 
5
 For example, O’Hara (2004). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
7 
7
Frictions in pricing arise from the process of matching buyers and sellers. Here, the role of market-makers is 
key. The market-maker faces two types of problem. One is an inventory-management problem – how much 
stock to hold and at what price to buy and sell. The market-maker earns a bid-ask spread in return for 
solving this problem since they bear the risk that their inventory loses value.6
 
Market-makers face a second, information-management problem. This arises from the possibility of trading 
with someone better informed about true prices than themselves – an adverse selection risk. Again, the 
market-maker earns a bid-ask spread to protect against this informational risk.7
 
The bid-ask spread, then, is the market-makers’ insurance premium. It provides protection against risks from 
a depreciating or mis-priced inventory. As such, it also proxies the “liquidity” of the market – that is, its 
ability to absorb buy and sell orders and execute them without an impact on price. A wider bid-ask spread 
implies greater risk in the sense of the market’s ability to absorb volume without affecting prices. 
This basic framework can be used to assess the impact of the changing trading topology on systemic risk, 
moving from analysing market microstructure to market macrostructure. Take the effects of fragmentation. 
That heightens competition among trading platforms, offering differing execution options and wider access to 
participants. This would tend to attract liquidity providers, including cross-market arbitraging HFT. As new 
liquidity-providers enter the market, execution certainty and price efficiency might be expected to improve. 
Inventory and information risk would thereby fall and, with it, bid-ask spreads. 
Some of the early empirical fingerprints suggest just such a pattern. For example, Brogaard (2010) analyses 
the effects of HFT on 26 NASDAQ-listed stocks. HFT is estimated to have reduced the price impact of a 
100-share trade by $0.022. For a 1000-share trade, the price impact is reduced by $0.083. In other words, 
HFT boosts the market’s absorptive capacity. Consistent with that, Hendershott et al (2010) and Hasbrouck 
and Saar (2011) find evidence of algorithmic trading and HFT having narrowed bid-ask spreads. 
Chart 8 plots a measure of bid-ask spreads on UK equities over the past decade, normalising them by a 
measure of market volatility to strip out volatility spikes. It confirms the pattern from earlier studies. Bid-ask 
spreads have fallen by an order of magnitude since 2004, from around 0.023 to 0.002 percentage points. 
On this metric, market liquidity and efficiency appear to have improved. HFT has greased the wheels of 
modern finance. 
But bid-ask spreads can sometimes conceal as much as they reveal. For example, by normalising on 
volatility, Chart 8 air-brushes out what might be most interesting: normalising volatility might normalise 
 
6
 Stoll (1978). 
7
 Glosten and Milgrom (1985). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
8 
8
abnormality. It risks falling foul of what sociologists call “normalisation of deviance” – that is, ignoring small 
changes which might later culminate in an extreme event.8
 
So is there any evidence of increasing abnormality in market prices over the past few years? Measures of 
market volatility and correlation are two plausible metrics. 9 Chart 9 plots the volatility of, and correlation 
between, components of the S&P 500 since 1990. In general, the relationship between volatility and 
correlation is positive. Higher volatility increases the degree of co-movement between stocks. 
Now consider how this volatility/correlation nexus has changed. This can be seen from the difference 
between the mass of blue dots (covering the period 1990 to 2004) and red dots (covering the period 2005 to 
2010) in Chart 9. Two things have happened since 2005, coincident with the emergence of trading platform 
fragmentation and HFT. 
First, both volatility and correlation have been somewhat higher. Volatility is around 10 percentage points 
higher than in the earlier sample, while correlation is around 8 percentage points higher. Second, the slope 
of the volatility / correlation curve is steeper. Any rise in volatility now has a more pronounced cross-market 
effect than in the past. Another way of making the same point is to plot measures of “excess correlation” – 
measured market correlation in excess of volatility. This is currently at historic highs (Chart 10). 
Taken together, this evidence points towards market volatility being both higher and propagating further than 
in the past. Intraday evidence on volatilities and correlations appears to tell a broadly similar tale. Overnight 
and intraday correlations have risen in tandem.10 And intra-day volatility has risen most in those markets 
open to HFT.11 
Coincidence does not of course imply causality. Factors other than HFT may explain these patterns. Event 
studies provide one way of untangling this knitting. Recent evidence from the Flash Crash pinpoints the 
particular role played by HFT using transaction-level data. The official report on the Flash Crash, while not 
blaming HFT firms for starting the cascade, assigns them an important role in propagating it. For example, 
many HFT firms significantly scaled back liquidity and overall HFT firms were net sellers of stock.12 
Taken together, this evidence suggests something important. Far from solving the liquidity problem in 
situations of stress, HFT firms appear to have added to it. And far from mitigating market stress, HFT 
appears to have amplified it. HFT liquidity, evident in sharply lower peacetime bid-ask spreads, may be 
illusory. In wartime, it disappears. This disappearing act, and the resulting liquidity void, is widely believed 
 
8
 This term has its origins in work by Diane Vaughan on NASA’s decision-making in the run-up to the space shuttle Challenger disaster 
in 1986, where repeated oversight of small problems culminated in a big problem (Vaughan (1996)). It has since been found in a much 
broader range of phenomena, where small cognitive biases have had disastrous physical consequences (Cliff (2010), Harford (2011)). 9
 See also Brogaard (2010) and Zhang (2010). 
10 Lehalle et al (2010b). 11 Lehalle et al (2010a). 12 CFTC-SEC (2010), Kirilenko, Kyle, Samadi and Tuzun (2011). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
9 
9
to have amplified the price discontinuities evident during the Flash Crash.13 HFT liquidity proved fickle under 
stress, as flood turned to drought. 
In some respects, this may sound like old news. For example, an evaporation of liquidity, amplified by 
algorithmic trading, lay at the heart of the 1987 stock market crash. And it is also well-known that stock 
prices exhibit non-normalities, with the distribution of asset price changes fatter-tailed and more persistent 
than implied by the efficient markets hypothesis at frequencies of years and months, perhaps weeks and 
days.14 But these abnormalities were thought to disappear at higher frequencies, such as hours and 
minutes. Over shorter intervals, efficient market pricing restored itself. 
Recent studies point, however, to a changing pattern. Non-normal patterns in prices have begun to appear 
at much higher frequencies. A recent study by Smith (2010) suggests that, since around 2005, stock price 
returns have begun to exhibit fat-tailed persistence at 15 minute intervals. Given the timing, these 
non-normalities are attributed to the role of HFT in financial markets. 
The measure of stock price abnormality used by Smith is the so-called “Hurst” coefficient.15 The Hurst 
coefficient is named after English civil engineer H E Hurst. It was constructed by plotting data on the 
irregular flooding patterns of the Nile delta over the period 622-1469 AD. Hurst found that flooding exhibited 
a persistent pattern. Large floods were not only frequent, but came in clumps. They had a long memory. 
The Hurst coefficient summarises this behaviour in a single number. For example, a measured Hurst equal 
to 0.5 is consistent with the random walk model familiar from efficient markets theory. A Hurst coefficient 
above 0.5 implies fatter tails and longer memories. In his study, Smith finds that the Hurst coefficient among 
a selection of stocks has risen steadily above 0.5 since 2005. In other words, the advent of HFT has seen 
price dynamics mirror the fat-tailed persistence of the Nile flood plains. 
To illustrate, Chart 11 plots the path of three simulated price series with Hurst coefficients of 0.5, 0.7 and 0.9. 
A higher Hurst coefficient radically alters the probability of sharp dislocations in prices. It also prolongs these 
dislocations. Prices become de-anchored and drift; their variance grows over time and is unbounded. If this 
long-memory property of prices is emerging at ever-higher frequencies, it might provide an important clue to 
how HFT affects systemic risk. 
4. A Sketch Model of Market Macrostructure 
To see that, consider a sketch model of market-making. This builds on an analytical insight which is already 
more than 40 years old. It owes to the late Benoit Mandelbrot, French-American mathematician and 
 
13 For example, Jarrow and Protter (2011, Cvitanic and Kirilenko (2010). 14 For example, Gopikrishnan et al (1999), Bouchaud et al (2009). 15 Blackledge (2008). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
10 
10
architect of fractal geometry. Mandelbrot found that a great many real-world topologies exhibited a fractal 
pattern. By this he meant that the basic pattern repeated itself, whatever the scale at which it was observed. 
They were “self-similar”. Self-similarity appears to be present throughout the physical world, from coastlines 
to cauliflowers, from snowflakes to lightning bolts, from mountain ranges to river deltas.16
One of Mandelbrot’s earliest applications of fractal geometry was to stock prices. In a 1967 paper, he 
argued that stock prices could best be understood by distinguishing between two measuring rods: clock time 
and volume time.17 While empirical studies typically used the first measuring rod (days, hours, seconds, 
milli-seconds), stock prices were better understood by using the second. 
Mandelbrot’s explanation was relatively simple. If trading cannot occur within a given time window, price 
movements can only reflect random pieces of news – economic, financial, political. So, consistent with 
efficient market theory, price changes would be drawn from a normal distribution with a fat middle and thin 
tails when measured in clock time. They were a random walk. 
But as soon as trading is possible within a period, this game changes. Strategic, interactive behaviour 
among participants enters the equation. Volumes come and go. Traders enter and exit. Algorithms die or 
adapt. Behaviour within that time interval may then no longer be random noise. Rather trading volumes will 
exhibit persistence and fat tails. This will then be mirrored in prices.18 So when measured in clock time, 
prices changes will have thinner middles and fatter tails, just like a cauliflower, a coastline, or a cosmos.19 
Subsequent studies have shown that this clock time / volume time distinction helps explain equity price 
dynamics, especially at times of market stress. For example, Easley et al (2011) show that the distribution of 
price changes during the Flash Crash was highly non-normal in clock time, with fat tails and persistence. But 
in volume time, normal service – indeed, near-normality – resumed. This fractal lens can be used to explain 
why market liquidity can evaporate in situations of market stress, amplifying small events across time, assets 
and markets. Fractal geometry tells us that what might start off as a snowflake has the potential to snowball. 
(a) Behaviour of High Frequency Traders 
HFT has had three key effects on markets. First, it has meant ever-larger volumes of trading have been 
compressed into ever-smaller chunks of time. Second, it has meant strategic behaviour among traders is 
occurring at ever-higher frequencies. Third, it is not just that the speed of strategic interaction has changed 
but also its nature. Yesterday, interaction was human-to-human. Today, it is machine-to-machine, 
 
16 Peters (1994). 17 Mandelbrot and Taylor (1967). See also Clark (1973). 18 This finding can be given a variety of behavioural interpretations, including persistence in gaps in the limit order book (Bouchard et al 
(2009)). Empirical support for this hypothesis is found in Easley et al (2011a). 19 Andrew Lo’s “adaptive market hypothesis” is a more recent manifestation of essentially the same story. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
11 
11
algorithm-to-algorithm. For algorithms with the lifespan of a ladybird, this makes for rapid evolutionary 
adaptation. 
Cramming ever-larger volumes of strategic, adaptive trading into ever-smaller time intervals would, following 
Mandelbrot, tend to increase abnormalities in prices when measured in clock time. It will make for fatter, 
more persistent tails at ever-higher frequencies. That is what we appear, increasingly, to find in financial 
market prices in practice, whether in volatility and correlation or in fat tails and persistence. 
This change in price dynamics will in turn influence market-making behaviour. Consider the problem facing 
an HFT market-maker. They face inventory risk from market fluctuations and information risk from adverse 
selection. Pricing these risks means forming a guess about the future path of prices. The greater the 
potential range of future prices, the larger the insurance premium they will demand. 
The future price range (maximum – minimum) for a price series at some future date T, R(T), can be written 
generically as:20
(1) ܴሺܶሻ ൌ ߪߢ்ܶு 
where κ is a constant and σT is the standard deviation of the process up to time T. H is our old friend the 
Hurst coefficient, dredged up from the Nile; it can be thought to summarise the degree of fat-tailedness and 
persistence in prices. 
When the holding period is short (say T=1), as with HFT, the future price range is the volatility of the series 
magnified by the Hurst coefficient. In other words, the fatter and more persistent the tails (H), the greater the 
market risk and the wider the bid-ask spread of the HFT. 
This has implications for the dynamics of bid-ask spreads, and hence liquidity, among HFT firms. During a 
market crash, the volatility of prices (σ) is likely to spike. From equation (1), fractality heightens the risksensitivity of HFT bid-ask spreads to such a volatility event. In other words, liquidity under stress is likely to 
prove less resilient. This is because one extreme event, one flood or drought on the Nile, is more likely to be 
followed by a second, a third and a fourth. Reorganising that greater risk, market makers’ insurance 
premium will rise accordingly. 
This is the HFT inventory problem. But the information problem for HFT market-makers in situations of 
stress is in many ways even more acute. Price dynamics are the fruits of trader interaction or, more 
accurately, algorithmic interaction. These interactions will be close to impossible for an individual trader to 
observe or understand. This algorithmic risk is not new. In 2003, a US trading firm became insolvent in 16 
 
20 Equation (1) holds asymptotically in expectation for large T and for a broad range of statistical processes (see Peters (1994) and 
Qian and Rasheed (2004)). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
12 
12
seconds when an employee inadvertently turned an algorithm on. It took the company 47 minutes to realise 
it had gone bust.21 
Since then, things have stepped up several gears. For a 14-second period during the Flash Crash, 
algorithmic interactions caused 27,000 contracts of the S&P 500 E-mini futures contracts to change hands. 
Yet, in net terms, only 200 contracts were purchased. HFT algorithms were automatically offloading 
contracts in a frenetic, and in net terms fruitless, game of pass-the-parcel. The result was a magnification of 
the fat tail in stock prices due to fire-sale forced machine selling.22
These algorithmic interactions, and the uncertainty they create, will magnify the effect on spreads of a market 
event. Pricing becomes near-impossible and with it the making of markets. During the Flash Crash, 
Accenture shares traded at 1 cent, and Sotheby’s at $99,999.99, because these were the lowest and highest 
quotes admissible by HFT market-makers consistent with fulfilling their obligations. Bid-ask spreads did not 
just widen, they ballooned. Liquidity entered a void. That trades were executed at these “stub quotes” 
demonstrated algorithms were running on autopilot with liquidity spent. Prices were not just information 
inefficient; they were dislocated to the point where they had no information content whatsoever. 
(b) Behaviour of Low Frequency Traders 
If the story stopped here the ending would be an unhappy, but perhaps not critical, one. After all, the Flash 
Crash was done and dusted within an hour. But the framework developed suggests these effects need not, 
in general will not, be transient. To the contrary, these effects might actually magnify. To see why, consider 
now the behaviour of low frequency traders (LFT). 
They face the same set of risks as the HFT market-maker – inventory and information risk. But persistence 
and fat tails in short-term price movement amplify these risks. From equation (1), the greater the holding 
period, T, the greater the potential dispersion in future prices. Intuitively, with fat-tailed persistence, a large 
shock is both more likely and its effects will linger longer. Slower trading increases the distance prices can 
travel once de-anchored. 
Chart 12 illustrates this. The distribution of returns observed at a low frequency has far greater variance and 
kurtosis than that at a higher frequency. For example, the variance of prices faced by a LFT firm (trading at 
T=50) is around nine times greater than for a HFT firm (trading at T=1), for H=0.9.23 
Given these price dynamics, HFT aggravates the market-making problem for LFT firms by magnifying the 
market risk they face.24 And it is not just that LFT firms are slower to execute. In situations of stress, they 
 
21 Clark (2010). 22 CFTC-SEC (2010). 23 Even once an adjustment has been made for the degree of time scaling associated with a standard Brownian motion. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
13 
13
may not even be able to see the prices at which they can trade. Co-located traders may have executed 
many thousands of trades before LFT firms have executed their own. LFT firms face intrinsic uncertainty 
about prices. When volumes and price movements are large, LFT market-making is driving in the dark, 
stock-picking with a safety-pin. 
 
During the Flash Crash, many traders suffered just this problem. Message traffic resulted in delays in 
disseminating quotes for over 1000 stocks. These delays lasted for up to 35 seconds. As a result, 
discrepancies emerged between the prices of common stocks trading on different exchanges (Chart 13). 
Faced with such uncertainty, a number of market participants paused or halted trading. The equilibrating 
force of long-term investors went missing. Bargain-hunting shoppers simply had no price list. 
The combined effects of these inventory and information problems is to widen the bid-ask spreads LFT 
market-makers charge. Greater execution risk and uncertainty calls for a larger insurance premium. This, 
too, may have an adverse feedback effect on financial market pricing. That is because it is likely to render 
uncompetitive LFT firms relative to HFT firms able to charge tighter spreads. Market-making will increasingly 
congregate around HFT firms proffering these lower spreads. 
If the way to make money is to make markets, and the way to market markets is to make haste, the result is 
likely to be a race – an arms race to zero latency. Competitive forces will generate incentives to break the 
speed barrier, as this is the passport to lower spreads which is in turn the passport to making markets. This 
arms race to zero is precisely what has played out in financial markets over the past few years. 
Arms races rarely have a winner. This one may be no exception. In the trading sphere, there is a risk the 
individually optimising actions of participants generate an outcome for the system which benefits no-one – a 
latter-day “tragedy of the commons”.25 How so? Because speed increases the risk of feasts and famines in 
market liquidity. HFT contribute to the feast through lower bid-ask spreads. But they also contribute to the 
famine if their liquidity provision is fickle in situations of stress. 
In these situations, backstops sources of longer-term liquidity ought to ride to the rescue. But HFT has also 
affected this outside option. LFT market-making has been squeezed-out by competitive pressures from 
HFT. And those LFT market makers that remain are at an acute informational disadvantage in situations of 
stress. The result is a potentially double liquidity void. 
 24 Indeed, with fractal price dynamics the variability of prices will potentially grow without bound over time. 25 Hardin (1968). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
14 
14
(c) Behaviour across Assets and Markets 
So far the story has been confined to behaviour of one stock trading on a single exchange. But the changing 
structure of markets and trading mean it is unlikely that any stock-specific price dislocation will be localised. 
The new topology of trading makes contagion more of a potential bogeyman now than in the past. 
One source of contagion is between stock prices and derivatives written on these stocks. HFT activity 
means that arbitrage opportunities between these markets are likely to be quickly removed. Or, put 
differently, price dislocations in the cash market are likely to be transmitted instantly to futures markets and 
vice-versa. That was the story of the Flash Crash, with order imbalances propagated through the futures 
market due to pass-the-parcel dynamics before ricocheting back to affect prices in the cash market.26 
A second channel of contagion is between different exchanges and trading platforms. Here, too, HFT has 
changed the landscape. Because HFT strategies have emerged to arbitrage differences, price transmission 
across exchanges and platforms is near-instantaneous. It also has the potential to be more disruptive. 
Liquidity on these exchanges is no longer pooled and centralised. Instead it is distributed and localised, 
increasing the potential for illiquidity premia to emerge in periods of stress. 
A third potential contagion channel is across stocks. HFT algorithms tend to amplify cross-stock correlation 
in the face of a rise in volatility due to their greater use of algorithmic trend-following and arbitrage strategies. 
That is consistent with the evidence in Chart 9, with heightened correlation per unit of volatility relative to the 
past. 
Taken together, these contagion channels suggest that fat-tailed persistence in individual stocks could 
quickly be magnified to wider classes of asset, exchange and market. The micro would transmute to the 
macro. This is very much in the spirit of Mandelbrot’s fractal story. Structures exhibiting self-similarity 
magnify micro behaviour to the macro level. Micro-level abnormalities manifest as system-wide instabilities. 
In many respects, this is an unsurprising conclusion. The emergence of HFT in fragmented trading platforms 
makes for a financial market place exhibiting complexity and contagion. As the race for zero has gathered 
pace, those two features are likely to have grown in importance. The trading system has become 
increasingly complex and tightly knit. From the work of Charles Perrow, these are the two essential 
ingredients in the recipe for systemic failure.27 
 
26 CFTC-SEC (2010). 27 Perrow (1999), Harford (2011). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
15 
15
5. Market Macrostructure and Public Policy 
The rapidly-adapting topology of trading poses a challenging set of questions for policymakers, particularly 
those safeguarding the resilience of financial markets. How is this changing market macrostructure best 
understood and monitored? And how, if at all, can it be modified to bolster systemic resilience? 
This is unlikely to be an easy regulatory task. For the first time in financial history, machines can execute 
trades far faster than humans can intervene. That gap is set to widen. In some respects the 2010 Flash 
Crash and the 1987 stock market crash have common genes – algorithmic amplification of stress. But they 
differ in one critical respect. Regulatory intervention could feasibly have forestalled the 1987 crash. By the 
time of the Flash Crash, regulators might have blinked – literally, blinked – and missed their chance. 
Nonetheless, the experience of managing large-scale, non-financial systems suggests risk-reducing 
regulatory intervention is possible. Drawing on that experience, two sets of policy tool are worth developing 
further. 
(a) Mapping Market Macrostructure 
Many securities regulators collect transactional data for the markets they oversee, typically to help them 
detect instances of market abuse. But the transactional data collected internationally is about to be 
transformed. In future, a much larger array of over-the-counter transactions will be cleared through central 
counterparties (CCP). And transactions in a number of non-centrally cleared markets will be recorded and 
warehoused in so-called trade repositories.28
This presents a real opportunity. Prospectively over the next decade, securities regulators internationally will 
have at their disposal a database which is far richer in its frequency and scope than anything previously. For 
the first time ever, many of the raw ingredients will exist to map price and quantity dynamics within, and 
across, markets in close to real time. 
To date, relatively little thought has been given to how best to put these data to work. They will certainly be 
useful as an archaeological site, allowing a detailed forensic sifting of the genealogy of historical market 
events. As with the Flash Crash, this may enable regulatory detectives to solve the whodunit: who did what 
when and, perhaps, why. It will allow DNA sampling of the victim. 
 
28 In the United States, the Office of Financial Research, created under the Dodd Frank Act, is charged with collecting and analysing 
data submitted by US firms. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
16 
16
An altogether bigger prize would be to put these data to work before the fact, identifying the next victim 
pre-autopsy. That could mean using transactions data to help detect early warnings of systemic fault-lines 
and stresses. This is a potentially massive analytical and technical challenge. 
The technical challenge is certainly surmountable. Advances in computer power over the past decade mean 
that storing and processing huge volumes of data poses no technological barrier. The answer lies in the 
clouds – conveniently enough, since they too exhibit fractal properties. If we can search and track the world 
wide web in close to real time, we can certainly do the same for its financial sub-component. 
The analytical challenge is altogether greater. In essence, it is to find summary measures of billions of 
transactions data which are informative about impending market stress. There is serious needle-in-haystack 
risk with such an endeavour. And experience in economics and finance of finding robust early warning 
indicators is mixed. But here again, the market microstructure literature offers some tantalising clues. 
For example, Easley et al (2011b) have suggested that measures of “order imbalance” may provide early 
warning signs of liquidity voids and price dislocations. Their measure of imbalance follows closely in 
Mandelbrot’s footprints. It uses a volume-based metric of the proportion or orders from informed traders. 
Any imbalance towards informed traders causes potential liquidity problems down the line as a result of 
adverse selection risk. Easley et al show that their imbalance measure rose sharply ahead of the Flash 
Crash, contributing to an eventual evaporation of liquidity. 
A more ambitious approach still would be to develop a system-wide model of financial market interaction. 
Cliff (2010) describes the trading infrastructure as an example of a “socio-technical system of systems”. 
These involve a complex fusion between technology and human behaviour. This interaction increases the 
system’s vulnerability to catastrophic failure. He proposes a “test rig” for such systems, using simulation 
techniques to unearth potential systemic fault-lines. 
These techniques have already been applied to other large socio-technical systems, such as defence, space 
and weather systems. In each case, there have been some successes. The lessons from these exercises 
seem to be twofold. First, that although technology may pose a problem, it may also provide the solution. 
Second, that even if it ain’t broke, there is a compelling strong case for fixing it. Not to do so today runs too 
great a risk of catastrophic failure tomorrow. We do not need to await a second Flash Crash to establish it 
was no fluke. To wait is to normalise deviance. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
17 
17
(b) Modifying Market Macrostructure 
Regulators in the US and Europe are in the process of reviewing regulatory rules for trading. In the US, 
some changes to market rules have already been implemented, while others are the subject of ongoing 
research.29 In Europe, a review is underway of MiFID with an early set of proposals tabled.30 It is interesting 
to assess those proposals using the framework developed here. 
 (i) Market-making guidelines: In principle, a commitment by market-makers to provide liquidity, whatever 
the state of the market, would go to the heart of potential price discontinuity problems. Market-making 
commitments would not forestall the arms race. But they would lessen the chances of liquidity droughts and 
associated fat tails and persistence in prices. They would, in effect, lower the impact of H. Perhaps for that 
reason, there have been proposals in both the US and Europe for such a set of market-making 
commitments.31
The difficulty appears, first, in specifying these commitments in a precise enough fashion; and, relatedly and 
just as importantly, enforcing them. In a sense, even the market-makers offering their stub quotes on 6 May 
were fulfilling a market-making commitment, albeit a paper-thin one. If hard law commitments are too difficult 
to define or enforce, an alternative may be a set of soft law guidelines. A number of electronic broking 
platforms, notably in foreign exchange markets, have codes or rules of conduct - for example, around 
price-making and price-taking which control the extent to which any one firm can steal a technological march 
on others. If these codes were extended across trading platforms and assets, perhaps in time 
market-making behaviour might adapt. 
(ii) Circuit-breakers: Circuit-breakers already exist on US and Europe exchanges. Indeed, circuit-breakers 
played an important role in stalling the Flash Crash. In the face of pass-the-parcel algorithmic dynamics, the 
Chicago Mercantile Exchange imposed an automatic 5-second pause on trading in its S&P 500 E-mini 
futures contracts.32 It worked, providing time for human traders to take stock – and, as importantly, buy 
stock. 
The rationale for such rules is well-understood. They temporarily impose a minimum execution time on 
trading, T. By calling a halt to trading, circuit-breakers provide a means of establishing a level informational 
playing field for all traders, long and short, fast and slow. In other words, they help solve the asymmetric 
information (risk and uncertainty) problem across traders of different types. The changing topology of 
trading, both speed and structure, has made these informational frictions more acute. So the case for 
circuit-breakers is perhaps stronger now than in the past. 
 
29 European Commission (2010), CFTC-SEC (2011). 30 CFTC-SEC (2011). 31 European Commission (2010) 32 MacKenzie and Demos (2011). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
18 
18
Circuit-breakers are also a means of heading-off suicidal spiralling, when algorithms join hands and chase 
each other over a cliff edge, as during the Flash Crash. Because they are automatic, circuit-breakers allow 
time for human intervention to catch-up with, and forestall, machine (summary) execution. They close the 
gap between man and machine, if only temporarily. Because HFT has widened this gap, the case for 
deploying circuit-breakers is stronger now than in the past. 
Historically, circuit-breaking rules have been security or exchange-specific. But in a world of multiple 
exchanges, and increasingly correlated securities, rules of this type may not be restrictive enough. 
Contagion across securities and exchanges has become the rule. Recognising that, the US authorities have 
recently revised their circuit-breaking rules with all trading in a specified set of securities and futures now 
halted, irrespective of where trading is taking place.33 
Although the pattern of trading fragmentation in Europe is similar to the US, current MiFID proposals do not 
envisage an automatic cross-market trading halt. But in a world of location-free, tightly arbitraged trading, 
cross-market circuit-breakers might become increasingly important. Indeed, these rules may potentially 
need to cross continents, as well as countries and platforms, if price dislocations are not to be redirected. 
(iii) Resting rules: Circuit-breakers are an ex-post, state-contingent intervention rule, specifying a minimum T 
for a temporary period. A more ambitious proposal still would be to impose a minimum T, or resting period 
for trades, at all times. Minimum resting periods are an ex-ante, non-state contingent intervention rule. They 
tackle the arms race at source by imposing a speed limit on trading. Though mooted in both the US and 
Europe, they have not been implemented in either trading jurisdiction. 
In some respects, it is easy to see why. From equation (1), imposing a minimum T will tend to result in a 
higher average bid-ask spread in all states. By increasing the per period transaction cost, the imposition of a 
minimum resting period would tend to widen bid-ask spreads and damage market liquidity in peacetime. 
HFT would be constrained in its offering to the liquidity feast. 
That is of course only one side of the coin. Setting a minimum T would also tend to reduce the risk of 
liquidity drought. While raising the average bid-ask spread, it might also lower its variability at times of 
stress. Liquidity would on average be more expensive but also more resilient. So in determining whether 
there is a role for minimum resting periods, this trade-off between market efficiency and stability is key. 
In calibrating this trade-off, a judgement would need to be made on the social value of split-second trading 
and liquidity provision and whether this more than counterbalances the greater market uncertainty it 
potentially engenders. At times, the efficiency of financial markets and their systemic resilience need to be 
 
33 CFTC-SEC (2010). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
19 
19
traded off. This may be one such moment. Historically, the regulatory skew has been heavily towards the 
efficiency objective. Given today’s trading topology, it may be time for that to change. 
5. Conclusion 
The Flash Crash was a near miss. It taught us something important, if uncomfortable, about our state of 
knowledge of modern financial markets. Not just that it was imperfect, but that these imperfections may 
magnify, sending systemic shockwaves. Technology allows us to thin-slice time. But thinner technological 
slices may make for fatter market tails. Flash Crashes, like car crashes, may be more severe the greater the 
velocity. 
Physical catastrophes alert us to the costs of ignoring these events, of normalising deviance. There is 
nothing normal about recent deviations in financial markets. The race to zero may have contributed to those 
abnormalities, adding liquidity during a monsoon and absorbing it during a drought. This fattens tail risk. 
Understanding and correcting those tail events is a systemic issue. It may call for new rules of the road for 
trading. Grit in the wheels, like grit on the roads, could help forestall the next crash.

Is the world becoming short-sighted? As individuals, it sometimes feels that way. Information is streamed in 
ever greater volumes and at ever rising velocities. Timelines for decision-making appear to have been 
compressed. Pressures to deliver immediate results seem to have intensified. Tenure patterns for some of 
our most important life choices (marriage, jobs, money) are in secular decline.1
 Some have called this the 
era of “quarterly capitalism”.2
 
These forces may be altering not just the way we act, but also the way we think. Neurologically, our brains 
are adapting to increasing volumes and velocities of information by shortening attention spans. 
Technological innovation, such as the world wide web, may have caused a permanent neurological rewiring, 
as did previous technological revolutions such as the printing press and typewriter.3
 Like a transistor radio, 
our brains may be permanently retuning to a shorter wave-length. 
If these forces are real, they might be expected to be particularly important in capital markets. These are a 
key conduit for choice over time. An efficient capital market transfers savings today into investment 
tomorrow and growth the day after. In that way, it boosts welfare. Short-termism in capital markets could 
interrupt this transfer. If promised returns the day after tomorrow fail to induce saving today, there will be no 
investment tomorrow. If so, long-term growth and welfare would be the casualty. 
Yet, despite its potential importance for long-term growth, studies of short-termism in capital markets are 
relatively thin on the ground. There is a sharp disconnect between popular perception of rising myopia, 
driven by technology and neurology, and empirical evidence.4
 This paper aims to provide some evidence on 
short-termism drawing on equity market experience. It is planned as follows. 
Section 2 reviews existing evidence on short-termism. Section 3 describes the theory underlying our test of 
short-termism and its adverse implications for investment choice. Section 4 presents the empirical results, 
drawing on cross-sectional and time-series data. Section 5 draws out the investment implications of the 
results and sets out a potential menu of policy options. 
Our evidence suggests short-termism is both statistically and economically significant in capital markets. It 
appears also to be rising. In the UK and US, cash-flows 5 years ahead are discounted at rates more 
appropriate 8 or more years hence; 10 year ahead cash-flows are valued as if 16 or more years ahead; and 
cash-flows more than 30 years ahead are scarcely valued at all. The long is short. Investment choice, like 
 
1
 Haldane (2010). 
2
 Barton (2011). 
3
 Carr (2008). 
4
 A recent interim report on short-termism by a UK government department concludes thus: “Overall, respondents believe that shorttermism exists in equity markets, but provided little evidence to demonstrate the scale of the consequences for companies and 
investors” (Department for Business, Innovation and Skills (2011)).
All speeches are available online at www.bankofengland.co.uk/publications/speeches
2
2
other life choices, is being re-tuned to a shorter wave-length. Public policy intervention might be needed to 
correct this capital market myopia. 
2. The Short-Termism Debate 
The short-termism debate is not new. Excess discounting of future outcomes was a familiar theme among 
Classical economists. For Jevons, “the untutored savage, like the child, is wholly occupied with the 
pleasures and troubles of the moment; the morrow is dimly felt; the limit of his horizon is but a few days 
off”.5
 For Marshall, people acted like “children who pick the plums out of their pudding to eat them at once”.6
 
For Pigou, it demonstrated a “defective telescopic faculty” such that “we see future pleasures on a 
diminished scale”.7
 
And nowhere were these problems more acute than in financial markets. Keynes, himself part-time 
speculator, was well-aware of the perils of short-termism in investment choice, both moral and financial: “It is 
from time to time the duty of a serious investor to accept the depreciation of his holdings with equanimity 
without reproaching himself. Any other policy is anti-social, destructive of confidence and incompatible with 
the working of the economic system”. 8
In the US, these sentiments were echoed in the immediate post-war era by Benjamin Graham, the original 
‘value investor’ and yesteryear investment guru to today’s investment guru, Warren Buffett: “A serious 
investor is not likely to believe that the day-to-day or even month-to-month fluctuations of the stock market 
make him richer or poorer”.9
 And, famously, “in the short run, the market is a voting machine but in the long 
run, it is a weighing machine”. Whether an untutored savage, defective telescope or anti-social voting 
machine, something sounded amiss. 
Thus far, however, this evidence was largely anecdotal. It was not until the 1960s that the short-termism 
hypothesis was first tested empirically. This drew on survey evidence from investing firms. It found that 
investors typically expected full pay-back on an investment within 3 to 5 years. At the time, the average life 
of plant and equipment was often 10 times that.10 Firms played short even when they desired long. 
The first quantitative evidence that discount rates might be high began to appear in the early 1970s. For 
example, King (1972) examined investment in plant and machinery in the UK. Empirical estimates 
suggested the internal discount rate implied by firms’ corporate investment decisions may be up to 25%.11 
 
5
 Jevons (1871). 
6
 Marshall (1890). 
7
 Pigou (1920). 
8
 Keynes (1938).
9
 Graham (1949). 
10 Neild (1964) and the National Economic Development Office (1965) used questionnaire-based evidence. A large proportion of firms 
in the sample claimed to use a pay-back criterion and of these the modal pay-back period was 3-5 years. Census evidence from this 
period indicated that the average useful lifespan of machines was over 15 years. The distribution of plant and equipment lives in Dean 
and Irwin (1964) implied a mean economic life of 34 years.
11 Sumner (1974) reaches similar conclusions. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
3
3
This literature failed to catch fire. Starting in the mid-1970s, it was doused by a torrent of papers testing – 
and typically failing to reject – the efficient markets hypothesis. 
 
This new wave swamped empirical finance for the better part of a decade. In the late 1970s and early 
1980s, the efficient markets paradigm appeared all-conquering as a description of asset price movements in 
practice.12 Research on the inefficiencies of capital markets became something of a backwater. The voting 
machine appeared to be delivering outcomes both democratic and socially beneficial. 
But beginning in the 1980s, a whole sequence of “puzzles” in empirical finance began to emerge. These 
were puzzles only in the sense of being deviations from efficient markets. For Mr Keynes and the 
Classicists, they would have been anything but.13 For example, an early set of papers found “excess 
volatility” in asset prices relative to future dividends and earnings.14 Investor myopia was one interpretation, 
with too great a weight on near-term dividends causing even transitory changes to affect valuation.15
Using an augmented version of this basic asset pricing framework, Miles (1993) tested formally for excessive 
discounting of future cash-flows using company-level equity price data from the UK between 1980 and 1988, 
finding evidence of short-termism over this period. Similar approaches applied to longer time-series across a 
range of countries reached broadly similar conclusions.16 
Yet, latterly, the quantitative evidence appears, as in the mid-1970s, to have dried up. This time the efficient 
markets hypothesis cannot be held responsible, for it has come under increasingly critical scrutiny. Instead 
we have seen scraps of evidence drawn from the types of surveys familiar from the 1960s. For example, 
among asset managers, a 2004 MORI survey of members of the Investment Managers Association (IMA) 
and the National Association of Pension Funds (NAPF) asked if investment mandates created short-termism. 
A third of NAPF members and two-thirds of IMA members agreed. 
In 2006, a CFA (Chartered Financial Analyst) symposium of financial institutions concluded “the obsession 
with short-term results by investors, asset management firms, and corporate managers collectively leads to 
the unintended consequences of destroying long-term value, decreasing market efficiency, reducing 
investment returns, and impeding efforts to strengthen corporate governance”. Echoes, here, of Graham’s 
anti-social voting machine. 
Short-termist behaviour among investors appears to have rubbed-off on companies. Poterba and Summers 
(1995) surveyed Chief Executive Officers (CEOs) at Fortune-1000 firms. They found that the discount rates 
applied to future cash-flows were around 12%, much higher than either equity holders’ average rate of return 
 
12 Fama (1970) provides a survey of the early papers. 13 Hicks (1937). 14 The important papers are LeRoy and Porter (1981), Shiller (1981) and Campbell and Shiller (1988). 15 Several other of the empirical finance “puzzles”, including the dividend smoothing and serial correlation puzzles, can also potentially 
be attributed to myopia (Haldane (2010)). 
16 Cuthbertson, Hayes and Nitzsche (1997), Black and Fraser (2002). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
4
4
or the return on debt. This excessive discounting implied that some firms were rejecting positive net present 
value (NPV) projects. Echoes, here, of Pigou’s defective telescope. 
Graham, Harvey and Rajgopal (2005) surveyed 401 executives. They found three striking results. First, 
managers would reject a positive-NPV project if that lowered earnings below quarterly consensus 
expectations. Second, over 75% of the sample would give up economic value in order to smooth earnings. 
Third, managers said that this was driven by the desire to satisfy investors. Echoes, here, of Marshall’s plum 
pudding problem. 
Most recently, in 2011 PriceWaterhouseCoopers conducted a survey of FTSE-100 and 250 executives, the 
majority of which chose a low return option sooner (£250,000 tomorrow) rather than a high return later 
(£450,000 in 3 years). This suggested annual discount rates of over 20%. Recently, Matthew Rose, CEO of 
Burlington Northern Santa Fe (America’s second biggest rail company), expressed frustration at the focus on 
quarterly earnings when locomotives lasted for 20 years and tracks for 30 to 40 years. Echoes, here, of 
“quarterly capitalism”. 
This evidence – anecdotal, survey, quantitative – is broadly consistent with popular perceptions. Capital 
market myopia is real. It may be rising. For at least some of the jury, however, it remains inconclusive. In 
2010, Richard Saunders (Chief Executive of the IMA) summed it up thus: “Now red lights start flashing for 
me when people talk about short-termism, particularly when shareholders feature in the same sentence. 
What do people mean when they claim that shareholders behave in a short-term fashion? And what 
evidence do they have for it? I have yet to hear a convincing answer to either question”. 
3. Testing for Short-Termism 
In the quest for some concrete, quantitative evidence, our test of short-termism uses the forward-looking 
asset price framework of Miles (1993). A simple example illustrates the basic approach to testing myopia 
and its implications for project choice. 
(a) A Simple Example 
Consider an investment project costing $60. This investment is riskless and pays $10 at the end of each of 
10 years. The present value of the project is simply the sum of the cash-flows discounted by the risk-free 
rate, r: 
(1) ܸܲ௥௔௧௜௢௡௔௟ ൌ $10
ሺ1 ൅ ݎሻ ൅
$10
ሺ1 ൅ ݎሻଶ ൅…൅
$10
ሺ1 ൅ ݎሻଵ଴
All speeches are available online at www.bankofengland.co.uk/publications/speeches
5
5
With a discount rate of 9%, the project’s cash-flows are worth $65 today and its NPV is $5. A firm or investor 
offered this project should rationally undertake the investment. 
 
Short-termism implies that agents may discount “excessively” future cash-flows, over and above the risk-free 
rate. Denote that short-termism parameter, x. The present value under myopic discounting then becomes: 
(2) ܸܲ௠௬௢௣௜௖ ൌ $10ݔ
ሺ1 ൅ ݎሻ ൅
$10ݔଶ
ሺ1 ൅ ݎሻଶ ൅…൅
$10ݔଵ଴
ሺ1 ൅ ݎሻଵ଴
If x is less that unity, then the project’s cash-flows are discounted too heavily. For example, assume x = 0.95 
so that one period ahead cash-flows are underestimated by 5%. Even with this modest degree of myopia, 
NPV calculations are affected significantly. A $10 return received at the end of year 5 should be worth $6.65 
today. With myopia, it is worth $5.14. Discounted cash-flows on the project are now worth $52, meaning 
that the NPV of the project is negative. A myopic investor would walk away from this NPV-worthy project. 
Imagine instead that an investor were making choices based on average payback periods, rather than NPV. 
Under rational discounting, the project has a payback period of 9 years. Under myopic discounting, the 
payback period rises to 15 years. An investor might now think twice before investing their money, for their 
money is committed for almost twice as long. 
So short-termism implies that projects with positive returns, or a relatively short payback, may be 
misperceived as being negative return or having a relatively lengthy payback. These projects would fail to 
receive financing. Investment and, ultimately, growth would be lower than optimal. In fact, the potential 
capital misallocation problem is greater still. To see that, consider the three projects summarised in Table 1. 
Table 1 Short-termism and capital planning 
Project A B C 
Cash-flows (CF) $28 pa in years 6-10 $10 pa every year $16 pa in years 1-5 
Cumulative CF $140 $100 $80 
NPV (rational) $73 $66 $63 
Ranking (rational) 1 2 3 
NPV (myopia) $49 $52 $55 
Ranking (myopia) 3 2 1 
In the absence of short-termism, project A is selected. Its payouts are back-loaded but significant; it 
generates a net excess return of 22%.17 Short-termism hits such long duration projects hardest. The 
impatient investor chooses project C. This project delivers lower cash-flows but these are front-loaded. In 
NPV terms, the project selected is the worst on offer, whereas the rationally optimal project ranks last. 
 
17 The return is $73 - $60 = $13, divided by the cost of investment. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
6
6
Capital allocation is not just sub-optimally low; it is also skewed towards sub-optimally short-duration 
projects. 
(b) Asset Pricing 
Consider now a formal model of multi-period equity price determination. Finance theory typically assumes 
that investors care about both the level and uncertainty of their wealth and are risk averse. In this world, 
agents require a premium to invest in a company. More formally, the expected return can be written as the 
sum of the risk free rate and a company-specific risk premium for company j:18 
(3) ܧ௧ ൫ܴ௝௧൯ൌ ܴ௙௧ ൅ ߨ௝௧
The actual return on an investment is the sum of the capital gain and the dividend yield: 
(4) ܴ௝௧ ൌ ܲ௝௧ ା ଵ െ ܲ௝௧
ܲ௝௧
൅
ܦ௝௧ ା ଵ
ܲ௝௧
Assuming an efficient market, actual returns differ only from expected returns due to a forecast error which is 
uncorrelated with expected returns.19 Using this assumption, we can substitute (4) into (3) to give an 
equation for the equity price. 
(5) 
ܲ௝௧ ൌ ܧ௧ ൫ܲ௝௧ାଵ ൅ ܦ௝௧ାଵ൯
ܴ௙௧ ൅ ߨ௝௧ ൅ 1
So the price of the security is simply the expected price and dividend in the next period, discounted by the 
sum of the risk-free rate and the company-specific risk premium. 
By repeated substitution, this asset pricing equation can be written as a generalised form of (1): 
(6) 
ܲ௝௧ ൌ ∑ ܧ௧ ൫ܦ௝௧ା௜൯ ே
௜ୀଵ
൫1 ൅ ݎ௧ଵ,௧ା௜ ൅ ߨ௝௧൯
௜ ൅
ܧ௧ ൫ܲ௝௧ାே൯
൫1 ൅ ݎ௧ଵ,௧ାே ൅ ߨ௝௧൯
ே
The current share price is a function of future discounted dividend streams and a discounted terminal share 
price, where we have used: 
 
18 This is the case with the capital asset pricing model (CAPM) (Lintner (1965), Sharpe (1964)) and arbitrage pricing theory (Ross 
(1976)). Under the CAPM, for example, the company specific risk premium is equal to the company specific beta multiplied by the 
market risk premium ߨ௝௧ ൌ ߚ௝௧ ൫ܴ௠௧ െ ܴ௙௧൯. This means ܧ௧ ൫ܴ௝௧൯ ൌ ܴ௙௧ ൅ ߚ௝௧ ൫ܴ௠௧ െ ܴ௙௧൯. 19 That is, we assume that ܴ௝௧ ൌ ܧ௧ ൫ܴ௝௧൯൅ ߝ௝௧. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
7
7
(7) ܧ௧ሺߨ௝௧ା௞ሻൌ ߨ௝௧ ,׊݇
(8) ܧ௧ሺܴ௙௧ା௞ሻൌ ݎ௧ଵ,௧ା௞ ,׊݇
Equation (7) says that the expected company-specific risk premium is constant and pre-determined based 
on period t information. Equation (8) says that expectations of future risk-free rates are defined by the path 
of the risk-free forward rate curve observed at time t. 
Equation (6) can be modified with a myopia coefficient to give a generalised version of (2): 
(9) 
ܲ௝௧ ൌ ∑ ܧ௧ ൫ܦ௝௧ା௜൯ ே
௜ݔ ୀଵ௜
൫1 ൅ ݎ௧ଵ,௧ା௜ ൅ ߨ௝௧൯
௜ ൅
ܧ௧ ൫ܲ௝௧ାே൯ݔே
൫1 ൅ ݎ௧ଵ,௧ାே ൅ ߨ௝௧൯
ே
The null hypothesis – no short-termism – implies x = 1. Drawing on evidence across time and industrial 
sectors, it is this restriction we now test. 
4. Testing for Short-Termism 
The data comprises a panel of 624 firms listed on the UK FTSE and US S&P indices over the period 1980-
2009.20 These span a broad range of industrial sectors, as shown in Table 2. 
Table 2 Number of firm level observations in each industry segment
Index Consumer Energy 
& 
Utilities 
Financials Health IT Industrials Materials Total 
S&P 117 65 78 47 73 47 23 450 
FTSE 52 14 42 5 34 18 9 174 
The core inputs to the analysis are firm-level measures of dividends and equity prices. The average 
dividend-price ratio in each industry segment is shown in Table 3. The mean dividend-price ratio across the 
panel is 2.6%. But there is a fairly significant degree of cross-sectoral and time-series dispersion. For 
example, dividend-price ratios are almost twice as high in the energy and utilities sector as the health and 
pharmaceuticals sector. And mean dividend-price ratios were two thirds third higher in the 1990s compared 
to the 1980s. 
 
20 Data are from Thomson Reuters Datastream. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
8
8
Table 3 Mean dividend-price ratio for firms in each industry segment 
 Consumer Energy & 
Utilities 
Financials Health IT Industrials Materials 
S&P 1.94 2.82 3.19 1.87 1.69 2.55 2.22 
FTSE 4.12 3.96 2.63 1.38 2.92 3.81 3.16 
To estimate (9), we require a selection of quantitative inputs. Taking these in turn: 
(a) Company-Specific Risk Premium 
Following Miles (1993), the company risk premium is modelled based on firm-specific characteristics, in 
particular the company beta and the level of gearing: 
(10) ߨ௝௧ ൌ ߙଵߚ௝௧ ൅ ߙଶܼ௝௧
where ܼ ൌ ஽
ா
.
21 Betas are estimated using daily return data for firms listed on the S&P 500 and FTSE, 
together with daily data for the indices themselves.22 Mean estimated betas are shown in Table 4. These 
average below one for both UK and US firms. As Charts 1 and 2 illustrate, however, the distribution of betas 
is fairly wide, with over a third of US firms and almost a fifth of UK firms having a beta in excess of unity. 
Table 4 Estimated betas
Index
Number of firms 
Number of 
observations 
Mean Median S.D 
S&P 401 10,140 0.91 0.86 0.49 
FTSE 168 3,765 0.63 0.62 0.45 
 
21 Because a firm’s beta ought also to be a function of its business and financing decisions, we also estimate a restricted version of (10): 
ߨ௝௧ ൌ ߙଵߚ௝௧. 
22 We exclude observations where the estimated beta is greater than 5 in absolute value, which in practice is only 7 firms. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
9
9
Chart 1: Distribution of betas – US Chart 2: Distribution of betas - UK 
Sources: Thomson Reuters Datastream and Bank calculations. 
Notes: Shows estimated betas for the US firms. The chart is 
drawn for betas lying between -1 and 3 with 400 bins. 
Sources: Thomson Reuters Datastream and Bank calculations. 
Notes: As for Chart 1 but for UK firms. 
The second component of the firm-specific discount factor is company gearing. This was constructed using 
annual Thomson Reuters Datastream data for book value per share, the number of shares outstanding and 
debt outstanding. Other things equal, higher gearing would suggest a higher company-specific discount 
factor.23 The final element in the firm-level discount factor calculation is the risk-free rate. The yield on 
government securities was used, based on data from the Federal Reserve and Bank of England. 
Having estimated (10) using pooled US and UK regressions over 20 years, a firm-specific risk premium can 
be calculated. The average premium across the sample is 5.9%. As Chart 3 shows, both gearing and beta 
contribute to the company risk premium estimates. 
 
23 The gearing variable is of poorer quality than others in the data, leading to negative gearing observations for some firms. Any firmyear observations with negative gearing are excluded from the analysis (a total of 37 observations). 
0
20
40
60
80
100
120
-1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0
S&P500 Observations
Estimated beta
0
5
10
15
20
25
30
35
40
45
-1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0
FTSE Observations
Estimated beta
All speeches are available online at www.bankofengland.co.uk/publications/speeches
10
10
Chart 3: Estimates of company risk premium 
Sources: Thomson Reuters Datastream and Bank calculations. 
Notes: Uses the estimated coefficients from the pooled UK and 
US regression together with the mean annual values of beta and 
gearing for firms in the US and UK. 
(b) Expected dividends and prices 
The right-hand-side of equation (9) defines the stream of future dividends and terminal prices. To generate 
these, consider a simplified version of (9) which abstracts from discount rates, company-specific risk premia 
and dividends: 
(11) ܲ௝௧ ൌ ܧ௧ ൫ܲ௝௧ାே൯ݔே
Following Wickens (1982), the rational expectation t+N periods ahead are formed on the basis of information 
available at time t. For each company j, these expectations differ from the realised values by a forecast error 
(ܷ௝௧ାே) unpredictable at time t: 
(12) ܧ௧ ൫ܲ௝௧ାே൯ൌ ܲ௝௧ାே ൅ ܷ௝௧ାே
Adding and subtracting the average forecast error across all companies (ܷഥ௝௧ାே) gives: 
(13) ܲ௝௧ ൌ ܧ௧ ൫ܲ௝௧ାே൯ݔே ൌ ܲ௝௧ାேݔே െ ܷഥ௝௧ାேݔே െ ሺܷ௝௧ାே െ ܷഥ௝௧ାேሻݔே
Actual prices cannot be used in the estimation of (13) as these are not known at time t and are correlated 
with the error term (ܷ௝௧ାே െ ܷഥ௝௧ାேሻ . But consistent estimation of (13) is possible using a set of instruments 
correlated with ܲ௝௧ାே but which are independent of the company-specific excess forecasting errors. In the 
20
10
0
10
20
30
40
50
60
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
Gearing Risk
Beta Risk
Company Risk Premium
Per cent
-
+
All speeches are available online at www.bankofengland.co.uk/publications/speeches
11
11
estimation, lagged share prices, lagged dividends per share and lagged earnings per share are used as 
instruments for future dividends and equity prices. These are known at time t but are uncorrelated with the 
error term. Five lags of each variable (price, dividends, earnings) are used as instruments. 
(c) Generating estimates of short-termism 
Given estimates of company beta and gearing, risk-free rates and the instrumented variables, equation (9) 
can be estimated to generate estimates of the short-termism parameter, x. This was achieved using nonlinear least squares on a set of cross-sectional regressions for each of the years 1985 to 2004.24
Chart 4 shows point estimates of x for each of these years. Short-termism estimates which are statistically 
significantly below unity (at the 5% confidence level) are shown in red. The simple average of x across the 
20-year period is very close to one (0.9935). On the face of it, this does not suggest that short-termism has 
been a particular problem among this cross-section of firms. 
Chart 4 Short termism estimates over time Chart 5 Industry estimates 
Source: Bank of England. 
Notes: Bars show the estimated x parameter for regressions for 
the UK and US pooled data. Estimates where the 95% 
confidence interval spans 1 are coloured grey. 
Source: Bank of England. 
Notes: Bars show the industry estimates for the sample split into 
(1) 1985-1994, and (2) 1995-2004 for each industry. S&P and 
FTSE industry classifications were used. Bars are coloured based 
on the significance of the estimates at the 5% level. 
But this masks some important within-period variation. In 13 of the 20 years, x is lower than 1. And in 9 of 
these years, x is statistically significantly below unity. Moreover, there is evidence of a rising tide of myopia: 
8 of these 9 years occur in the final decade of the sample. 
 
24 Because the estimation expected values up to five periods ahead, the estimates only run to 2004. 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
x > 1 x < 1
0
0.2
0.4
0.6
0.8
1
1.2
12 12 12 12 12 12 12
x > 1 x < 1
Consumer
Energy & 
utilities
Financials
Health
Industrials
IT
Materials
All speeches are available online at www.bankofengland.co.uk/publications/speeches
12
12
To illustrate, Table 5 shows point estimates of x over two decadal sub-samples (1985-1994 and 1995-2004) 
and over the full sample. Estimates are significantly below unity in the second sub-sample, but not the first. 
The point estimate of x over the second sub-sample is 0.94.25
Table 5 Short-termism estimates for the US and UK 
Year x Standard error Evidence of short-termism?
Full sample (1985-2004) 0.937 (0.004) Yes 
1985-1994 1.001 (0.008) No 
1995-2004 0.938 (0.005) Yes 
Notes: The significance column refers to a test of x<1 at the 5% confidence level. 
Table 6 shows estimates of x over the same three samples on a sectoral basis. It echoes the message from 
Table 5. There is statistically significant evidence of short-termism in the second half of the sample for all 
seven industrial sectors. And in all of these sectors except health and materials, x is lower in the second half 
of the sample than the first – in those two sectors x is below unity throughout the sample. 
Although short-termism appears to be a consistent theme across industrial sectors, there are nonetheless 
some interesting patterns in the degree of short-termism across sectors. For example, the financial sector 
does not appear especially short-termist over the full sample. By contrast, the health and materials sectors 
exhibit short-termism throughout. 
Table 6 Sectoral short-termism estimates for the US and UK 
Full Sample 
1985-2004 
1985-1994 1995-2004 
Industry x Significant? x Significant? x Significant? 
Consumer 0.939 Yes 1.007 No 0.94 Yes 
Energy and Utilities 0.939 Yes 1.05 No 0.934 Yes 
Financials 0.965 Yes 1.087 No 0.963 Yes 
Health 0.940 Yes 0.857 No 0.940 Yes 
IT 0.902 Yes 0.957 No 0.892 Yes 
Industrials 0.926 Yes 1.018 No 0.925 Yes 
Materials 0.875 Yes 0.871 Yes 0.874 Yes 
Notes: The significance columns refer to a test of x<1 at the 5% confidence level. 
 
25 Various robustness checks were conducted. These included dropping gearing from the estimation of the risk premium and varying
the effects of taxes. These did not alter significantly the empirical estimates. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
13
13
5. Short-Termism and Public Policy 
These tests of short-termism point to two key conclusions. First, there is statistically significant evidence of 
short-termism in the pricing of companies’ equities. This is true across all industrial sectors. Moreover, there 
is evidence of short-termism having increased over the recent past. Myopia is mounting. 
Second, estimates of short-termism are economically as well as statistically significant. Empirical evidence 
points to excess discounting of between 5% and 10% per year. To illustrate the impact of this on investment 
choice, consider the earlier project with an annual income stream of $10. 
Chart 6 shows the present value of those income streams under three counter-factual assumptions: rational 
discounting; myopic discounting – lower bound (5%); and myopic discounting – upper bound (10%). The 
cumulative impact is fairly dramatic. Ten-year ahead cash-flows under rational discounting are valued 
similarly to between six-year (lower bound) and four-year (upper bound) ahead cash-flows under myopic 
discounting. The long is shortened. 
Chart 6: Present value of future cash-flows Chart 7: Cumulative present value of future 
cash-flows 
Notes: The chart assumes $10 is paid at the end of each year. 
The rational discount rate used is 1.085. 
Notes: The cumulative NPV of $10 cash-flows rises to $61 in 
year 9 under rational discounting. With mild myopia (x=0.95) it 
only passes $60 at year 15. With severe myopia (x=0.90) the 
investor calculates that payback is not achieved. 
$0
$1
$2
$3
$4
$5
$6
$7
$8
$9
$10
0 1 2 3 4 5 6 7 8 9 10
x=1.00 x=0.95 x=0.90
Years
$0
$20
$40
$60
$80
$100
$120
$140
0 5 10 15 20 25 30 35 40 45 50
x = 1.00 x = 0.95
x = 0.90 Investment cost
Years
All speeches are available online at www.bankofengland.co.uk/publications/speeches
14
14
Table 7 Point in future at which residual discounted cash-flow falls below (years) 
10% 1% 0.1% 
Rational (x=1) 29 57 85 
Mild myopia (x=0.95) 18 35 52 
Strong myopia (x=0.90) 13 25 37 
Notes: The number in the table refers to the first year that a $10 cash-flow falls below 10%, 1% and 0.1% of its actual value in present 
value terms. The rational discount uses an average risk free rate from our cross sectional data sample (1.085). 
This is illustrated even more clearly if we consider payback periods. Under rational discounting, payback 
occurs in 9 years (Chart 7). Under upper bound myopic discounting, the investor today would erroneously 
assume that payback would never be made. These differences have the potential to alter radically project 
choice. The net present value of this project evaluated over 50 years falls from $56 under rational 
discounting to a loss of $11 under extreme myopia. In other words, a NPV-positive project would be 
resoundingly rejected. 
To put the point more starkly, Table 7 asks at what point in the future the residual value of a future cash-flow 
hits a level of 10%, 1% and 0.1% of its face value, under rational and myopic discounting. Under rational 
discounting, cash-flows even 50 years ahead retain more than 1% of their face value. Under strong myopic 
discounting, this residual threshold is reached after 25 years. Virtually zero weight – less than 1000th of the 
face value of the cash-flow – is placed on projects with income streams much beyond 35 years. The long is 
dramatically shortened. 
This is a market failure. It would tend to result in investment being too low and in long-duration projects 
suffering disproportionately. This might include projects with high build or sunk costs, including infrastructure 
and high-tech investments. These projects are often felt to yield the highest long-term (private and social) 
returns and hence offer the biggest boost to future growth. That makes short-termism a public policy issue. 
But what would be an appropriate public policy response to this capital market failure? A number of 
proposals have been suggested by various authors. These include: 
(a) Transparency: The lightest touch approach would be to require greater disclosures by financial and 
non-financial firms of their long-term intentions – for example, their long-term performance, strategy 
and compensation practices.26 For financial firms, this might include metrics of portfolio churn. This 
could be accompanied by a programme of educating managers, investors and advisors of their 
fiduciary responsibilities. 
 
26 Aspen Institute (2009), CFA (2006). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches
15
15
(b) Governance: A more intensive approach would involve acting directly on shareholder incentives 
through their voting rights. For example, fiduciary duties could be expanded to recognise explicitly 
long-term objectives.27 More concretely, shareholder rights could be enhanced for long-term 
investors, perhaps with a duration-dependent sliding scale of voting rights.28
(c) Contract Design: There have been various attempts over the past few years to make compensation 
contracts more sensitive to long-term performance and risk. This includes employment contracts 
conditioned on long-term performance, or with deferral or clawback. Changes in the compensation 
instrument can also help – for example, remunerating in equity is better than in cash and 
remunerating in junior or convertible debt might be better than either.29
(d) Taxation / Subsidies: Authors have suggested a variety of ways in which government could penalise 
short-duration holdings of securities, or incentivise long-duration holdings, using tax and / or subsidy 
measures. These measures differ in detail, but the underlying principle is to link them to the duration 
of an investor’s holdings or the length or nature of a company’s investment.30 
Some of these initiatives have been tried and tested in differing degrees, at different times and in different 
countries. They have not obviously arrested the short-termism trend. It might be time to increase the level of 
policy ambition if the telescope is to be corrected, the voting machine socialised, the savage civilised. Public 
policy could help keep the plums in the pudding. Without intervention, the long could become shorter still.

Global capital markets are engaged in a footrace. On the one foot, capital markets in emerging market 
countries are widening and deepening in order to improve domestic capital intermediation. On the other, 
international investors in developed countries are diversifying their portfolios in order to boost returns and 
spread risk. Both are typically seen as essential ingredients of global financial liberalisation and integration. 
Yet their implications for global financial stability are potentially quite different. The first determines the 
absorptive capacity of high-return, capital-recipient countries. The second defines the distributive capacity of 
low-return, capital-providing countries. Any imbalance between the rate at which international capital is 
distributed and absorbed may spillover to global capital markets. Reducing these spillovers may require a 
rethink of the rules of the international financial system. 
Experience during 2010 provides a good illustration. Gross portfolio equity inflows to emerging markets 
surged as investors substituted out of low growth developed markets (Chart 1). Gross equity inflows to 
certain emerging markets were large relative to domestic market capitalisation (Chart 2). The resulting 
indigestion problem caused bubbly behaviour in emerging asset markets. In response, a number of 
countries have thrown grit in the wheels of finance through macro-prudential measures or capital flow 
restrictions. 
These surges in foreign capital flows, and associated asset market spillovers, are far from new. Through 
this century, a new wave of inflows to emerging markets has been building. Although halted temporarily by 
the financial crisis, its effect has been to buoy emerging asset prices (Chart 3). Earlier centuries have seen 
many similar episodes of capital flow surges into emerging markets, from the South Sea bubble in 1720 to 
the East Asian crisis of the 1990s (Reinhart and Rogoff (2008)). 
Historically, as these waves have broken they have often caused quite a splash, with sharp capital and asset 
price reversals (IMF (2011)). Some commentators have called those capital reversals “sudden stops” 
(Dornbusch, Goldfajn and Valdés (1995)). But the underlying problem may be as much the start as the stop. 
The seeds of emerging market crises are sown in the build-up phase, as inflows dwarf the absorptive 
capacity of recipient countries’ capital markets. 
Capturing that dynamic requires a different metaphor - the “Big Fish Small Pond” (BFSP) problem. The Big 
Fish here are the large capital-exporting, advanced countries. The Small Ponds are the relatively modest 
financial markets of capital-importing emerging countries. Past experience suggests that as big fish enter 
the small pond, this can cause ripples right across the international monetary system, never more so than in 
today’s financially inter-connected world. 
This paper considers the BFSP problem. It plots the evolution of the international financial network over the 
past quarter-century. And, within that, it assesses recent shifts in portfolio choice by international investors 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
3 
3
which may have accentuated today’s BFSP problem (Section 2). More speculatively, it then considers 
potential future trends in portfolio choice and assesses their implications for tomorrow’s BFSP problem 
(Section 3). 
If recent trends are continued, the BFSP problem may become more acute over time. In the race between 
capital market deepening in emerging markets and capital diversification in advanced economies, the latter 
may gain the upper hand (or foot). The BFSP problem during 2010 may have been the thin end of a sizable 
wedge. That, in turn, may intensify the debate on appropriate policy responses, such as the pace of capital 
liberalisation and the case for capital restrictions (Section 4). 
2. Today’s BFSP Problem 
A useful panorama on financial globalisation is provided by mapping the evolution of the global financial 
network over time. The topology of this network provides a simple, graphical visualisation of liberalisation 
patterns. In particular, network graphics can be used to visualise “node size” – as a measure of the capitalabsorptive capacity of emerging markets – the Small Pond; and “link scale” – as a measure of the capital 
distribution of advanced economies – the Big Fish. 
As a first cut, we use a dataset developed at the Bank of England on stocks of cross-border capital, spanning 
18 advanced and emerging economies (Kubelec and Sa (2010)).1
 This enables us to track the growth in, 
and financial linkages between, countries over time. Chart 4 plots this international network map at five-year 
intervals from 1980 to 2005. The node size for each country is measured as the absolute sum of its external 
assets and liabilities, while the width of the links between nodes measures bilateral gross external assets. 
Both are in (log) dollar terms. 
Taken over the past quarter-century as a whole, there is significant evidence of both capital market 
deepening (increased node size) and integration (increased link size). On average, total stocks of external 
assets have increased seventeen-fold over this period, rising from around $1.5 trillion in 1980 to $26 trillion 
by 2005. Relative to these countries’ GDP, they have risen from 18% in 1980 to over 70% by 2005. This is 
consistent with a period of intense financial globalisation. 
Yet, despite this, the balance of global financial power has not altered markedly. The financial core countries 
at the beginning of the period (such as the US and UK) remain core at the end; the offshore financial 
centres (Hong Kong and Singapore) remain significant throughout; while emerging markets remain, in the 
main, on the financial periphery. 
 
1
 A similar database has been developed by Lane and Milesi-Ferretti (2007). Von Peter (2007) looks at the pattern of the international 
banking network. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
4 
4
To illustrate that, the share of intra-advanced country external asset stocks in the global total was 77% in 
1980. By 2005, that fraction had actually risen slightly to around 80%. Over the same period, the 
contribution of advanced to emerging market external asset stocks in the total fell from 13% to 5%, at the 
same time as the GDP share of emerging markets rose from 13% to 16%. So despite rising significantly in 
absolute terms, financial globalisation has not obviously closed the gap between core and periphery. 
These aggregate patterns mask two important, more recent trends. The first is the well-documented 
accumulation of official foreign exchange reserves by emerging market countries, in particular since 2005. 
The stock of global official reserves stood at $10 trillion by end-2010, having more than doubled since 2005. 
These flows are in some respects an oddity of international finance, with capital flowing “uphill” from 
emerging to advanced economies and with emerging market investors exhibiting a “foreign bias” for debt 
securities (Prasad, Rajan and Subramanian (2007)). 
At the same time, a less well-documented trend in private-sector portfolio allocation has been acting in the 
exact opposite direction. This has involved capital flowing “downhill” and is associated with investors 
seeking instead to reduce “home bias” in their investment portfolio. To see it, Chart 5 plots the global 
network map focussing on a subset of external asset stocks, namely portfolio equity flows. The same pattern 
of intensifying global integration is evident. Indeed, the intensification is altogether more dramatic: total 
holdings of external equity have risen more than 200-fold over the past quarter-century, an increase of $4.6 
trillion. 
But unlike for total external assets, the gap between core and periphery has been closing. The relative 
contribution of intra-advanced country external portfolio asset stocks fell from around 99% to 84% between 
1980 and 2005. Over the same period, the relative contribution of advanced to emerging market external 
portfolio asset stocks rose from virtually nothing to over 8%. That is a fairly dramatic shift in international 
investors’ portfolio choice. 
What explains it? One plausible candidate is increased portfolio diversification by institutional investors in 
advanced countries. It is well-known that international investment portfolios exhibit an acute home bias, with 
portfolio shares much more heavily skewed towards the home market than would be implied by conventional 
asset pricing models (French and Poterba (1991), Forbes (2010)). There is a home-bias puzzle. 
To illustrate, Table 1 constructs some equity home bias estimates for the G20 countries between 1995 and 
2007. Home bias is measured here as an index lying between zero and one. 2 So a value of unity indicates 
complete home bias – that is, a country holds no foreign equity in its investment portfolio. A value of zero, 
meanwhile, suggests home bias has been completely eliminated, with a country investing in proportion to the 
 
2
 Home bias for a given country is given by: 
1 – Country’s holdings of foreign equity compared to country’s total global equity holdings 
Other countries’ total share of world equity market capitalisation 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
5 
5
world market portfolio. Intermediate values tell us the proportion a country is underweight foreign assets 
relative to a world portfolio benchmark. 
The following points are noteworthy: 
 Equity home bias remains very significant across every country in the world. The un-weighted 
average home bias across the G20 in 2007 was around 0.78. Weighted by market capitalisation, 
average home bias was 0.66. So, in weighted terms, G20 countries’ holdings of foreign equity are 
around a third of the value they would be in the theoretically optimum market portfolio. 
 This home bias is lower among advanced than emerging market economies. The un-weighted home 
bias index in 2007 was 0.63 for advanced countries, while for emerging market G20 economies it 
was 0.90. For those emerging economies which maintain capital restrictions, such as India and 
China, home bias remains almost complete. 
 On average, there has been a decline in equity home bias across G20 countries over time. Among 
advanced G20 economies, the un-weighted home bias index has fallen from 0.75 in 1995 to 0.63 in 
2007. Weighted by market capitalisation, it has fallen from 0.79 to 0.61 over the same period. 
Foreign equity holdings have gone from a fifth to a third of their theoretical benchmark. 
 While seemingly modest in index terms, the implied portfolio reallocations amount to very sizable 
portfolio equity flows. That is because stocks of outstanding equity assets are enormous. For 
example, a fall of 0.1 in the weighted home bias index for advanced countries in 2007 relative to 
2006 would have equated to a portfolio reallocation to foreign markets of around $4.5 trillion. That is 
a big fish. To give some context, the market capitalisation of all G20 emerging market countries in 
2007 was $6.8 trillion. That is a smallish pond. 
 For individual countries, some of the falls in home bias have been fairly dramatic – for example, in 
Germany, France and Italy. Changes in US patterns of home bias are also very significant in 
financial terms. The fall of 0.17 in measured US home bias between 1995 and 2007 corresponds to 
an annual average portfolio reallocation to the rest of the world of around $370 billion (or 1.4% of 
non-US global GDP). 
 The pace at which home bias has been reduced is all the more striking because, over the same 
period, emerging markets have become a larger slice of the global portfolio pie. Between 1995 and 
2010, the emerging G20 countries more than quadrupled their share of the world equity market 
portfolio, from 4% to 21% (Chart 6). Had advanced country portfolio allocations remained constant 
over this period, measured home bias would have risen rather than fallen. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
6 
6
These trends in international portfolio allocation have been a key driver behind the emerging BFSP problem 
during the course of this century. Portfolio reallocation by the big fish has outpaced growth in the small pond 
of emerging capital markets. The upshot has been large and mounting financial ripples. These 
strengthening waves of capital inflows have, in turn, prompted some emerging countries to construct 
protective policy seawalls, such as capital flow restrictions and macro-prudential policies. Recently, these 
measures have included: 
 In Hong Kong, the authorities imposed a cap on loan-to-value (LTV) ratios and a maximum loan 
amount on certain properties, alongside an increase in stamp duty on sales in October 2009. 
 The Chinese authorities in 2010 increased taxes on properties resold within five years and offered 
greater administrative guidance on financing, including lower LTV ratios for second and third homes 
and a mandated increase in mortgage rates for second homes. 
 In Brazil, the authorities reintroduced in October 2009 a tax on non-resident portfolio equity and debt 
inflows, with a broader coverage and a higher rate. They also increased the tax on the margin 
payments required on foreign exchange futures and other derivatives. In early 2011, reserve 
requirements on short foreign exchange positions were introduced. 
 In South Korea, four sets of measures have been introduced: stronger foreign currency liquidity 
standards for banks (November 2009); reduced short-term external bank debt and limiting foreign 
currency bank loans (June 2010); a macro-prudential stability levy, planned to be introduced in 
2011, on banks’ non-deposit foreign currency liabilities (late 2010); and the reintroduction of a 
withholding tax on foreign purchases of treasury bonds (January 2011). 
 In Turkey, the central bank raised reserve requirements, and reduced policy rates, to address 
accelerating credit growth and financial stability. 
 In Thailand, a withholding tax was imposed in October 2010 on non-resident interest earnings and 
capital gains on state bonds. 
 In Indonesia, the central bank introduced in June 2010 a package of measures, including a onemonth holding period requirement for central bank securities. 
 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
7 
7
3. Tomorrow’s BFSP Problem 
From a public policy perspective, one key question is whether the frequency and amplitude of these global 
financial ripples will rise or fall in the period ahead. In other words, will tomorrow’s BFSP problem be greater 
or lesser than today’s? To answer that, we need to return to the global financial footrace. Judging that race 
involves balancing two powerful forces of financial liberalisation: 
 Portfolio diversification among advanced economies; 
 Financial deepening among emerging markets. 
Plotting the future path of these factors is far from straightforward. But some quantitative thoughtexperiments can help clarify the balance of these forces. To keep these experiments as transparent as 
possible, consider the simplest possible one-factor model of financial liberalisation. The one factor is the 
level of economic development of the G20 economies, as proxied by GDP per capita. 
Theoretically, we would expect financial liberalisation to be shaped by economic development. This is 
consistent with the economic growth literature emphasising the positive, two-way relationship between 
financial deepening and economic growth (Levine (2005)). The cross-section and time-series evidence 
presented here supports that hypothesis. 
To generate projections for the single factor – GDP per capita – we use a calibrated cross-country 
convergence model, in the spirit of Barro and Sala-i-Martin (1992) and Mankiw, Romer and Weil (1992). 
These GDP per capita projections are then adjusted for expected future changes in population growth to give 
a set of cross-country projections for GDP from 2010 to 2050. Chart 7 shows the global distribution of G20 
GDP at decadal intervals over this period. These projections are necessarily tentative, but are broadly in line 
with external estimates.3
Nominal GDP in the emerging G20 exceeds that in the advanced G20 by 2030. The advanced economies’ 
share of G20 GDP halves between 2010 and 2050, from 70% to 36%, while the emerging G20 economies’ 
share rises from 30% to 64%. Over the next 40 years, advanced and emerging countries will most likely 
swap GDP shares. By 2050, China’s share of G20 GDP is 22% and India’s 18%, with the US falling to 20%. 
In the thought-experiment, this shift in the balance of global activity has important implications for the 
balance of global capital flows. 
 
 
3
 For example, O’Neill (2011). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
8 
8
(a) The Evolution of Home Bias 
With mixed success, theoretical studies have sought to explain cross-country patterns of home bias (Lewis 
(1999) and Karolyi and Stulz (2003)). Perhaps the most cogent explanations are grounded either in legal 
restrictions (including capital controls and lack of property rights) and information asymmetries between 
home and host countries (Gehrig (1993)). Globalisation is likely to have eroded both of these market 
frictions somewhat over the past decade, in particular among emerging markets. That is consistent with the 
evidence in Table 1. 
Predicting the future course of these (legal and information) factors is far from easy. But one plausible 
instrument for financial diversification is economic development. Cross-country evidence suggests so. Chart 
8 plots a cross-section of measured equity home bias in 58 countries against levels of GDP per capita in 
those countries in 2005. The correlation is strong and significant: GDP per capita accounts for almost 60% 
of the cross-sectional variation in home bias. As a ready-reckoner, every $10,000 per capita rise in GDP is 
associated with a fall in home bias of around 0.1.4
Using economic development as an instrument for portfolio diversification, GDP per capita projections can be 
used to simulate the possible future course of home bias. Futurology of this kind of course comes with large 
caveats and confidence intervals. Given the simplicity of the model, it is a thought-experiment rather than a 
forecast. And underlying that experiment is an assumption that advanced countries continue to grow and 
diversify their equity portfolios, but at a slower rate than catch-up emerging market economies. 
Table 2 shows the evolution of equity home bias at decadal intervals out to 2050 for the G20 in this thoughtexperiment. Some clear points emerge: 
 All G20 countries see a reduction in home bias. The un-weighted G20 average home bias index 
falls by 0.33 between 2010 and 2050. That is roughly the same rate of descent as over the past 10 
years or so. 
 Among a number of advanced countries, home bias has been significantly eroded by 2050, in some 
cases perhaps implausibly so. For example, in the US home bias falls from 0.51 to 0.09. 
 Emerging market home bias remains in many cases significant even by the end of the period, 
reflecting their higher starting point. Average emerging market home bias in 2050 is not dissimilar to 
advanced country home bias in 2010. 
 
4
 This cross-sectional relationship was also used to counter-factually predict time-series patterns of home bias in a number of G20 
countries. GDP per capita was a reasonable predictor of historical movements in home bias, in particular among advanced economies. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
9 
9
 The implied reductions in home bias would have a potentially significant impact on the pattern of 
global capital flows. For example, if home bias in the G20 was to fall as projected, then the sum of 
gross inflows to individual emerging markets – including flows from both advanced and other 
emerging markets – is on average $6.3 trillion per year over the period. 
It would be easy to quibble with the detail of these projections. For example, cross-country information 
frictions, and hence home bias, may never be fully eliminated. East or West, home may remain best. 
Nonetheless, the slow, secular fall in this bias does not appear implausible if recent portfolio patterns are a 
reliable guide to the future. 
 (b) The Evolution of Financial Depth 
Theory would predict a positive, two-way relationship between financial depth and economic development. 
Not only does greater development spur the deepening of capital markets to finance investment projects, but 
deeper capital markets themselves may act as a spur for growth as they improve the efficiency of the 
financial intermediation process (Levine (2005)). 
Cross-country evidence again supports that hypothesis. Chart 9 plots the cross-sectional relationship 
between the log of GDP per capita and financial depth (measured as the ratio of equity market capitalisation 
to GDP). The correlation is positive and reasonably strong: GDP per capita accounts for almost half of the 
cross-sectional variation in the market capitalisation-to-GDP ratio. On average, a 10% rise in GDP per 
capita is associated with an increase in financial depth of around 1.5 percentage points. 
In principle, the relationship in Chart 9 could be used to simulate forward financial depth in the G20 
economies. But, unconstrained, such a simulation would imply a potentially unbounded relationship between 
market capitalisation and a country’s GDP. Because market capitalisation is the discounted value of future 
profit streams from companies, that seems implausible. 
Charts 10 plots equity financial depth for the US and UK since 1929 and 1965 respectively. Between 1929 
and 1990, the financial depth ratio in the US was roughly flat, at around 50%. As the US liberalised this ratio 
shifted upwards, averaging around 100%; it has remained roughly at these levels since. The UK has 
followed a similar pattern. To reflect these findings, in the simulations an (arbitrary) cap of 100% is placed 
on countries’ market capitalisation to GDP ratio, once they have reached current levels of US GDP per 
capita. 
Using the relationship in Chart 9 and the GDP projections, equity financial depth in the G20 countries can be 
projected forward. Many of the advanced countries have only a short distance to travel to reach 100%, given 
their starting point. Among the emerging G20, the average financial depth ratio rises from around 50% in 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
10 
10
2010 to around 100% by 2050 (Chart 11). In other words, the pace of financial deepening in the emerging 
G20 countries is rather gradual. If anything, this may understate the likely pace of change. 
Nonetheless, these trends imply a rather striking shift in the composition of the global market portfolio. Chart 
12 shows how this evolves, measured at 10-year intervals.5
 Some notable points are: 
 The share of G20 emerging markets in global equity market capitalisation rises from 21% in 2010 to 
56% by 2050. Over the same period, the contribution of advanced G20 economies falls from 66% to 
31%. 
 Within this, China’s share of the market portfolio almost triples to around 20% in 2050. India’s share 
rises five-fold to 15%. Together, these two countries’ market capitalisation is larger in 2050 than in 
all G20 advanced economies combined. 
 In money terms, the stock market capitalisation of the G20 emerging markets rises to around $300 
trillion in 2050, while the G20 advanced economies amount to $170 trillion. The global market 
portfolio pie increases fourteen-fold between 2010 and 2050. 
4. The BFSP Problem and Public Policy 
Having projected paths for home bias and market capitalisation among the G20 countries, the two can now 
be combined to give an illustrative general equilibrium projection of global flows of funds. This general 
equilibrium is a relatively complex one, for it involves the intersection of three forces in global finance: 
 First, the tendency towards increased diversification among advanced country investors. This 
causes more water to flow downhill – a capital flow substitution effect; 
 Second, the same trend among emerging economies but from a higher base. This would tend to 
push water uphill – a countervailing capital flow substitution effect; and 
 Third, a change in the composition of the global market portfolio as emerging economies account for 
a larger slice of the pie – a capital flow income effect. 
By combining these three factors, we can form a proximate picture of the emerging pattern of global capital 
flows. These projections are based on the same restrictive set of modelling assumptions. Nonetheless, they 
provide a transparent window on possible emerging patterns. Are the forces of financial deepening in 
emerging capital markets sufficient to outpace the reduction in advanced country portfolio home bias, 
 
5
 Non-G20 market cap is assumed to grow in line with the forecast for total G20 market capitalisation. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
11 
11
lessening the BFSP problem? Or does portfolio reallocation by advanced economies risk swamping 
embryonic emerging capital markets exacerbating this problem? 
Chart 13 looks at the ratio of annual average capital flows to G20 emerging markets relative to their market 
capitalisation over the period 2010 to 2050. It is the analogue of Chart 2, using projections of both 
numerator and denominator. The highest and lowest annual inflows over the projection period are also 
shown to capture potential annual variation in capital flows. Chart 14 plots annual gross capital inflows to 
emerging markets over the same period. 
Some notable points include: 
 Over the period 2010 to 2050, the average scale of capital inflows, relative to market capitalisation, 
for the G20 emerging markets is around 8% per year. That is greater than the peak inflow 
experienced in the recent past, including in 2010, during which time asset prices rose significantly 
and protective policy measures were put in place. 
 Given the likely variation in these inflows year by year, the picture is even more striking. Potential 
“peak” inflows are into double digits for the majority of G20 emerging markets. As context, peak 
portfolio equity inflows to East Asian countries in the three years preceding their 1990s crisis 
averaged 4% of market capitalisation. 
 For some individual emerging market countries, these capital inflow patterns are dramatic. For 
example, over the period 2010-2050, average inflows to India and Indonesia exceed 10% of their 
equity market capitalisation. 
Taken together, these projections do not suggest a dissipation of the BFSP problem. More likely, they 
suggest it could intensify, perhaps significantly, in the period ahead. If capital markets operated efficiently, 
these portfolio flows need not cause waves in emerging credit and asset markets. In practice, frictions in the 
functioning of these markets mean that asset market spillovers seem very likely to persist. In other words, 
the splashes from the big fish risk causing waves every bit as great, and potentially greater, than those seen 
recently. 
On this run of history, the global flow of funds could become an increasingly powerful generator of global 
financial instabilities. In that event, pressures could mount on policymakers to protect against the rising tide 
of capital flow-induced instability, including through capital restrictions and macro-prudential 
measures. Some of these measures, if not unthinkable, would have been frowned on as recently as a 
decade ago. When Malaysia imposed restrictions on capital movements in 1998, it was castigated by the 
international community. And when the Hong Kong authorities intervened in their stock market in the same 
year, many international policymakers blew a raspberry. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
12 
12
What a difference a decade makes. What a difference a crisis in advanced economies makes. Ideological 
aversion to throwing grit in the wheels of international finance appears, if not to have disappeared, then to 
have moderated. Where once it was raspberries, today’s policymakers are blowing kisses. Capital 
restrictions and macro-prudential policies have entered the policy bloodstream, if not yet the 
mainstream. The debate today is how best to integrate such tools into established macroeconomic policy 
frameworks. 
The issues here are thorny ones. They include: 
 Should capital restrictions be used by emerging markets as a last resort once conventional 
macroeconomic policies have been exhausted? Or should they instead play a supporting role in all 
seasons, to avert excessive capital surges and accompanying fluctuations in financial activity? And 
what guidelines, if any, should apply to the policies of advanced countries as the source of such 
capital surges? 
 Are capital flow restrictions better conceived as a temporary or permanent measure? On the one 
hand, temporary measures might better reflect the need to respond with exceptional measures only 
in exceptional circumstances. On the other, nesting capital restrictions within a pre-agreed 
framework could improve the credibility and predictability of these measures among international 
investors. 
 Should differing rules of the game apply to different sources of capital inflow (foreign direct 
investment versus portfolio flows versus banking flows)? And should different rules apply to flows of 
different maturity (short versus long–term)? Different sources and types of capital are believed to 
behave very differently during capital upswings and downturns (IMF (2011)). It would be odd for 
public policy to ignore those behavioural differences. 
 What are the respective roles of capital restrictions and macro-prudential policy? At present, the two 
are viewed rather differently, with views on capital restrictions mixed, while macro-prudential policies 
are increasingly the consensus. Yet these two sets of tool may differ more in detail than in 
principle. Macro-prudential policies may also have a role to play in moderating credit growth in the 
source of capital surges – the advanced economies. 
 What are the respective roles of capital restrictions and financial market deepening as responses to 
the BFSP problem? One policy option for emerging economies is to strengthen and deepen their 
domestic capital market infrastructure, so that it is better able to absorb and intermediate incoming 
foreign capital. A reduction in those capital market frictions would tend to lower the asset market 
spillover costs of the BFSP problem. 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
13 
13
 Finally, what guideposts should be provided by the international community in tackling the BFSP 
problem? The IMF recently began to explore such a framework (IMF (2011)). This raises a set of 
key questions about global financial governance. Are global financial network externalities 
sufficiently large to justify the international community imposing rules of the financial road? And how 
much driver discretion should instead be left to nation states which ultimately may bear the costs of 
the BFSP problem? 
These are big public policy questions. They are by no means new ones. If the quantitative experiments 
presented here are even roughly right, these questions may assume a new urgency in the period 
ahead. The BFSP problem is real. It may be rising. The result would be growing waves of global financial 
exuberance, punctuated by crashing capital busts. This roller-coaster has the potential to leave some nation 
states feeling queasy. They may even wish to get off. What is at stake may be more than just the future 
stability of the international financial system.

The Basel Committee sets capital standards for international banks. There are now three vintages of these 
standards: so-called Basel I dating from 1988, Basel II dating from around 2004 and Basel III which was 
agreed at the end of last year.1
 These international capital standards are supported by three pillars. Pillar I 
defines the regulatory rules, Pillar II provides scope for supervisory discretion, while Pillar III seeks to foster 
market discipline through disclosure. In countering systemic shocks, three supporting pillars have 
understandably been felt to be better than one. 
But the success of international capital standards in forestalling banking distress has been mixed. Basel I 
regulatory rules were arbitraged due to their risk insensitivity. This gave rise to Basel II with its greater focus 
on risk calibration. But Basel II buckled under the weight of the recent crisis. Repairs have since been 
applied through Basel III. Historical experience suggests this is unlikely to be the end of road. 
This paper assesses and suggests means of improving the robustness of this regulatory framework. The 
quest for risk sensitivity in Pillar I rules caused regulatory complexity and opacity to blossom. This may have 
inhibited the effectiveness of supervisory discretion and market discipline (Section 1). In consequence, Pillar 
I may have borne too much of the load and Pillars II and III too little. Here I focus on Pillar III. 
There may be straightforward ways to rebalance the Basel scales, re-injecting market discipline. Having 
banks issue a graduated set of contingent convertible (‘CoCo’) securities, which are responsive to early signs 
of market stress, is one possible way of doing so (Sections 2 and 3). That would have the practical effect of 
reinforcing Pillar I with Pillar III, so delivering a potentially more balanced and robust regulatory edifice. 
Getting from here to there may take time. But some modest adjustment to dividend and bonus distribution 
policies by banks would help (Section 4). Section 5 concludes. 
The First Pillar 
The recently-agreed Basel III package delivers a material strengthening of regulatory standards. Though the 
details are complex, the essence of these reforms is easily described: “more of the same – and better”. So 
there will be more bank capital and in future it will be higher quality. Banks will be required, for the first time 
in an international accord, to hold liquid assets, and in future will be more resilient. Risk management will be 
more extensive and in future it will be more robust. And bank supervisors will be more plentiful and in future 
they will be smarter. 
That has been the response to virtually every financial crisis of the past fifty years. It has not arrested the 
crisis cycle – if anything, the incidence of crises appears to have risen.2
 It may not even have slowed this 
cycle, as the massive costs of this time’s crisis attest. So although the recent Basel III package may be 
 
1
 Goodhart (2011 forthcoming). 
2
 Bordo et al (2001).
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
3 
3
necessary, there must be a real question about whether it will be sufficient to cope with next time’s crisis. 
History, at least, provides grounds for pessimism. 
As a thought experiment, imagine instead we were designing a regulatory framework from scratch. Finance 
is a classic complex, adaptive system. What properties would a complex, adaptive system such as finance 
ideally exhibit to best insure about future crises? Simplicity is one. There is a key lesson, here, from the 
literature on complex systems. Faced with complexity, the temptation is to seek complex control devices. In 
fact, complex systems typically call for simple control rules. To do otherwise simply compounds system 
complexity with control complexity.3
 Uncertainty would not then divide, it would multiply. 
Robustness would be a second. This has a particular meaning in the context of complex systems: resilience 
given ignorance.4
 More often, this is called Knightian uncertainty or simply model error. The dynamics of 
complex systems, such as large banks or interconnected financial webs, are not well understood. That 
means uncertainty needs to be taken seriously if financial regulatory frameworks are to be robust. 
Timeliness would be a third criterion. Complex systems often exhibit a knife-edge property, with 
discontinuities and tipping points a naturally-recurring feature.5
 Those same features have punctuated the 
present financial crisis. In physics as in finance, once over the cliff-edge there is little chance of full recovery. 
That underscores the importance of timely, pre-emptive regulatory intervention if financial disaster is to be 
averted. 
(a) Simplicity 
How do existing regulatory rules compare against these criteria – simplicity, timeliness, robustness? Take 
simplicity. By any standard, existing regulatory rules are far from simple. For large banks, they can be 
highly complex. They have become more so over time given the evolution of the Basel framework. 
Back in the days of Basel I, calculating a regulatory capital ratio was not especially taxing or time-consuming. 
It involved little more than half a dozen calculations. These calculations could be conducted on the back of a 
small envelope by a competent clerk. Possessing envelopes and clerks, banks, regulators and market 
participants were able to perform those calculations. They were transparent and verifiable. In that way, 
regulatory rules (Pillar I) provided a solid platform for supervisory discretion by regulators (Pillar II) and 
market discipline by investors (Pillar III). The Basel pillars were mutually reinforcing. 
Basel II changed that calculus. In part to avert regulatory arbitrage, there was a quest for greater risksensitivity. Regulatory capital rules became more finely calibrated to banks’ underlying portfolio of risks. In 
 
3
 For example, Perrow (2007) and Cliff (2011). 
4
 Hansen and Sargent (2007). 
5
 Haldane (2009), Haldane and May (2011). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
4 
4
practice, that meant two things. First, the number of independent categories of risk assets increased. And 
second, greater use was made of banks’ own internal models to generate the risk metrics associated with 
each asset class. The consequences of this regulatory shift for system complexity have been extraordinary. 
For large and complex banks, the number of risk categories has exploded. To illustrate, consider the 
position of a large, representative bank using an advanced internal set of models to calibrate capital. Its 
number of risk buckets has increased from around seven under Basel I to, on a conservative estimate, over 
200,000 under Basel II. To determine the regulatory capital ratio of this bank, the number of calculations has 
risen from single figures to over 200 million. The quant and the computer have displaced the clerk and the 
envelope. 
At one level, this is technical progress; it is the appliance of science to risk management. But there are 
costs. Given such complexity, it has become increasingly difficult for regulators and market participants to 
vouch for the accuracy of reported capital ratios. They are no longer easily verifiable or transparent. They 
are as much an article of faith as fact, as much art as science. This weakens both Pillars II and III. For what 
the market cannot observe, it is unlikely to be able to exercise discipline over. And what the regulator cannot 
verify, it is unlikely to be able to exercise supervision over. Banks themselves have recently begun to voice 
just such concerns. 
 
(b) Robustness 
A further cost to complexity arises from model error. Model uncertainty, as distinct from risk, is rarely taken 
into account when interpreting reported capital ratios. But without some idea of uncertainty, it is difficult to 
know how much confidence to place in reported solvency measures. How large are the confidence intervals 
investors and regulators ought to place around them? 
Calibrating confidence intervals around capital would involve aggregating across myriad assets and multiple 
models. Rather tellingly, that is too complex a calculation for anyone to have carried out. But by drawing on 
banks’ own published model output, it is possible to gauge uncertainty around some of the key balance 
sheet components – for example, the retail credit portfolio, the wholesale credit portfolio and the trading 
book. 
For the retail credit portfolio, consider a simple, single source of model error – calculating the probability of 
default (PD) for different classes or borrower. Under the advanced model-based approach to calculating 
retail PDs, each loan is placed in a risk bucket. So there is a degree of intrinsic uncertainty both about where 
in the risk bucket the true PD lies and indeed about whether it lies in the assigned risk bucket at all. 
To gauge the importance of these uncertainties, the mortgage portfolios of two large banks were examined 
and capital calibrated using their reported models. These capital ratios were then simulated under two 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
5 
5
counterfactual assumptions: (a) that true PDs were uniformly distributed within the risk bucket; (b) that true 
PDs were higher than recorded PDs by one risk bucket. 
The results are illustrated in Charts 1 and 2. Chart 1 looks at the distribution of the reported capital ratio, 
indexed to 100, assuming model uncertainty only exists within each risk bucket and is symmetric. Even 
under this assumption the confidence intervals are significant, with the “true” capital ratio lying anywhere 
between 15% above or below the reported ratio. Once we allow for the possibility of PDs being 
systematically under-estimated, the degree of uncertainty is greater still. The “true” ratio then lies up to 35% 
below the reported ratio. That equates to several percentage points of capital. 
A different way of gauging the effects of model uncertainty is to look at how different banks value essentially 
identical exposures. For UK banks’ wholesale credit portfolio, the FSA conducted just such an exercise in 
2009.6
 A hypothetical portfolio was constructed based on 64 externally rated corporate, bank and sovereign 
exposures. Banks were then asked to use their models to generate PDs and capital for this hypothetical 
portfolio, which could be compared across banks 
. 
The range of reported capital requirements held against this common portfolio was striking. For wholesale 
exposures to banks, capital requirements differed by a factor of over 100%. For corporate exposures, they 
differed by a factor of around 150%. And for sovereign exposures, they differed by a factor of up to 280%. 
Those differences could equate to a confidence interval around reported capital ratios of 2 percentage points 
or more. 
A final means of gauging potential model error is to consider past evidence. During the crisis, model error 
was largest and most egregious in the trading book. Charts 3 and 4 compare UK banks’ pre-crisis capital 
held against the trading book to trading book losses during the crisis. Losses were up to six times greater 
than pre-crisis trading book capital (Chart 3). And capital ratios would have needed to be up to 2.5 
percentage points higher to accommodate this model risk (Chart 4). A fundamental review of the trading 
book is underway to address this problem. 
This evidence only provides a glimpse at the potential model error problem viewed from three different 
angles. Yet it suggests that model error-based confidence intervals around reported capital ratios might run 
to several percentage points. For a bank, that is the difference between life and death. The shift to 
advanced models for calibrating economic capital has not arrested this trend. More likely, it has intensified it. 
The quest for precision may have come at the expense of robustness. 
Hayek titled his 1974 Nobel address “The Pretence of Knowledge”.7
 In it, he highlighted the pitfalls of 
seeking precisely measurable answers to questions about the dynamics of complex systems. Subsequent 
 
6
 Financial Services Authority (2010). 
7
 Hayek (1974). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
6 
6
research on complex systems has confirmed Hayek’s hunch. Policy predicated on over-precision risks 
catastrophic error. Complexity in risk models may have perpetuated Hayek’s pretence in the minds of risk 
managers and regulators. 
(c) Timeliness 
One of the purported benefits of model-based calibration is that it increases the sensitivity of capital 
requirements to changes in risk. Reported regulatory capital ratios should better reflect risk and thus should 
in principle offer timely advance warnings of impending bank stress. 
The data tell a somewhat different story. To see that, consider the experience of a panel of 33 large 
international banks during the crisis. This panel conveniently partitions itself into banks subject to 
government intervention in the form of capital or guarantees (“crisis banks”) and those free from such 
intervention (“no crisis banks”). 
Chart 5 plots the reported Tier 1 capital ratio of these two sets of banks in the run-up to the Lehman Brothers 
crisis in September 2008. Two observations are striking. First, the reported capital ratios of the two sets of 
banks are largely indistinguishable. If anything, the crisis banks looked slightly stronger pre-crisis on 
regulatory solvency measures. Second, regulatory capital ratios offer, on average, little if any advance 
warning of impending problems. These conclusions are essentially unchanged using the Basel III definitions 
of capital. 
 
This visual evidence can be formalised by constructing some Type I (false positive) and Type II (missed 
crisis) error estimates for the same 33-bank panel. Assume, by way of illustration only, that if a bank’s Tier 1 
capital ratio dips below 8%, this is deemed to signal distress (Table 1). The probability of a Type I error 
using regulatory capital ratios is 50%, while the probability of a Type II error is around 43%. On those 
assumptions, this suggests regulatory capital ratios do about as well in predicting crises as a coin toss. They 
are essentially uninformative about future bank stress. 
Taken together, this does not paint an altogether encouraging picture. A critic might argue that regulatory 
capital ratios have become too complex to verify, too error-prone to be reliably robust and too leaden-footed 
to enable prompt corrective action. From a first principles perspective, they score poorly as an optimal 
control device over a complex system such as finance. 
 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
7 
7
Resurrecting the Third Pillar 
What could be done to strengthen the framework? As a thought experiment, consider dropping risk models 
and instead relying on the market. Market-based metrics of bank solvency could be based around the 
market rather than book value of capital. The market prices of banks are known to offer useful 
supplementary information to that collected by supervisors when assessing bank health.8
 And there is also 
evidence they can offer reliable advance warnings of bank distress. 
9
To bring these thoughts to life, consider three possible alternative bank solvency ratios based on market 
rather than accounting measures of capital: 
 Market-based capital ratio: the ratio of a bank’s market capitalisation to its total assets. 
 Market-based leverage ratio: the ratio of a bank’s market capitalisation to its total debt. 
 Tobin’s Q: the ratio of the market value of a bank’s equity to its book value. 
The first two are essentially market-based variants of regulatory capital measures, the third a well-known 
corporate valuation metric. 10 How do they fare against the first principles of complex, adaptive systems? 
They clearly offer the advantage of simplicity and transparency. 200 million separate calculations would 
condense to a simple, single sum. The clerk would make a glorious return and displace the quant. Marketbased measures could be observed and verified in real-time by regulators and market participants. That 
could help in enhancing both supervisory discretion and market discipline. Market-based capital ratios could 
support all three Pillars, helping to rebalance the Basel scales. 
Market-based solvency metrics offer two further advantages. First, they are not reliant on myriad, misspecified models. They are largely model-free, if not error-free.11 They are robust to model error and 
ignorance. Second, history suggests that, at least in the latest crisis, they would have given far timelier 
signals of impending stress, and so a better guide to prompt corrective action, ahead of the crisis cliff-edge 
being reached. 
To illustrate that, Charts 6–8 look at the three market-based measures of solvency for the 33-bank panel, 
again broken down between “crisis” and “no crisis” banks. There is now clear blue water between the 
solvency ratios of the crisis and non-crisis banks, with the second materially weaker. In the two years prior 
 
8
 Cannata and Quagliariello (2005), Gunther et al (2001), Krainer and Lopez (2004). 
9
 Berger et al (2000). 
10 Calomiris and Herring (2011) prefer a variant of the first measure. 11 Largely, because even marked-based measures of solvency will rely on disclosures by banks, which will themselves be modeldependent to some degree.
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
8 
8
to the Lehman Brothers crisis, the average market-based capital ratio was around 5 percentage points lower 
for crisis than for no crisis banks. 
 
Second, market-based measures of capital offered clear advance signals of impending distress. For 
example, the market-based capital ratio of crisis banks began to fall in April 2007, well over a year ahead of 
the Lehmans crisis. To formalise that visual impression, Table 1 shows Type I and II errors for the three 
market-based capital measures. These now comfortably outperform a coin toss. Replacing the book value 
of capital with its market value lowers errors by a half, often much more. Market measures provide both 
fewer false positives and more reliable advance warnings of future banking distress. 
Taken together, then, market-based solvency metrics perform creditably against first principles: they appear 
to offer the potential for simple, timely and robust control of a complex financial web. 
Contractual Prompt Corrective Action 
But how should these market-based metrics be put to use in the design of a regulatory framework? Market 
warnings are useless unless they are acted on, either by the market itself or by regulators. It is not too 
difficult to devise a reconfiguration of banks’ capital structure that would bake-in the benefits of simplicity, 
robustness and timeliness. This would involve a cocktail of revised regulatory capital standards fortified by 
market discipline. It might work as follows. 
Alongside equity, banks would be required to issue a set of contingent convertible instruments – so-called 
“CoCos”. These instruments have attracted quite a bit of attention recently among academics, policymakers 
and bankers, though there remains uncertainty about their design.12 In particular, consider CoCos with the 
following possible design characteristics. 
 Triggers are based on market-based measures of solvency, as in Charts 6–8. 
 These triggers are graduated, stretching up banks’ capital structure.13 
 On triggering, these claims convert from debt into equity. 
Although novel in some respects, CoCos with these characteristics would be simple to understand. They 
would be easy to monitor in real time by regulators and investors. And they would alter potentially quite 
radically incentives, and thus market dynamics, ahead of banking stress becoming too acute. 
To see why, imagine a bank whose expected future profits, and hence market capitalisation, have slumped. 
If this erosion of profits is sufficiently material, conversion at the highest trigger occurs. Upon triggering, a 
 
12 For example, Flannery (2010), Flannery and Perotti (2011), Duffie (2011), Calomiris and Herring (2011). 13 Alternatively, there could be graduated tranches of CoCos operating with a single trigger. The economic impact of these two 
structures would be very similar.
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
9 
9
chunk of that bank’s debt converts into equity, automatically recapitalising the bank and providing it with an 
extra equity cushion. This equity infusion ought to help restore market confidence in the bank’s soundness. 
If the first conversion does not do the trick, or if the profits shock is sufficiently large, there are other rungs in 
the ladder. Lower triggers provide a graduated safety net. As these triggers are pulled, converting CoCos 
offer progressively greater stuffing for the cushion. This is a double boon. It confers the benefits of 
(contractually pre-agreed contingent) equity. But knowledge of the graduated safety net ought also to help 
stabilise investors’ confidence in the bank. 
Under this capital structure, banks’ insurance contract would be fundamentally different than at present. 
Instead of equity being provided at haste under stress, the safety net would extend automatically in advance. 
And instead of being provided by the state ex post, insurance would come from private creditors ex ante. 
Timely self-insurance would replace laggardly public insurance. There would be prompt corrective action. 
But it would operate on autopilot, using a contractual, market-based navigation system. 
This contractual automaticity would provide a shot in the arm to incentives and thus market discipline. First, 
for the incentives of investors. Knowing that a trigger might be close to being pulled, and their claims 
converted, CoCo investors are likely to sit up and take notice. If conversion takes place below the prevailing 
market price, the incentives among existing shareholders are similarly sensitised. Early signs of 
deteriorating profits or sentiment are likely to result in greater investor activism. Such activism was absent in 
the run-up to the crisis, in part because there was no early morning wake-up call for investors. 
Second, for the incentives of management. If investors are grumpy about their early morning wake-up call, 
management of the bank are likely to hear their hoarse protests. And anticipating these protests, 
management are less likely to sail close to the wind or at least will be quicker to tack when fearing a squall. 
Management, too, would be provided with incentives to remain shipshape. 
 
Third, for the incentives of regulators. In the depths of crisis, as recent experience has shown, the 
temptation to forbear or bail-out is very strong. It is no surprise that the authorities often opt for the greater 
certainty of bail-out ex post, whatever their preferences ex ante. This policy time-consistency problem lies at 
the heart of the current regulatory debate. 
Better resolution tools, which would be needed if CoCos are not enough, can lessen the dilemma and are an 
essential part of the reform programme. They give the authorities extra options such as creditor bail-in 
through write-downs or debt-for-equity conversions. They provide a statutory backstop, enabling bail-in of 
private sector creditors. As such, they are a necessary ingredient for restoring market discipline. 
CoCos buttress market discipline and help lift the authorities from the horns of the time-consistency dilemma. 
They augment regulatory discretion at the point of distress with contractual rules well ahead of distress. 
Capital replenishment is contractual and automatic; it is written and priced ex-ante and delivered without 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
10 
10
temptation ex-post. Because intervention would be prompt, transparent and rule-based, the scope for 
regulatory discretion would be constrained. For that reason, the time-consistency problem ought to be 
reduced, perhaps materially. A contractual belt is added to the resolution braces. 
To achieve these benefits, change would be needed to banks’ capital structure. But this reconfiguration 
would not be especially dramatic. Indeed, it would be as simple as CDE: C(oCos) + D(debt) + E(quity). The 
layer of bank equity would be augmented with a layer of CoCos, with slices defined by market-based 
triggers. Banks’ capital would comprise multi-sliced equity, some actual, some contingent. In some 
respects, this model is not greatly dissimilar to the one recently proposed by the Swiss banking 
commission.14
In achieving this capital structure, one option would be for regulators to require it, as in Switzerland. But it is 
possible that banks themselves might find such a capital structure in their own best interests. To see why, 
consider a standard model of optimal capital choice by a value-maximising firm. The firm faces a trade-off. 
Debt confers the benefits of tax deductibility, while equity offers the benefits of lower expected bankruptcy 
costs.15 The trade-off between these two sets of benefits defines an optimal capital ratio for a firm to 
maximise its expected value (Chart 9).16 
Now consider adding CoCos to the mix. They are, in effect, a hybrid of debt and equity whose payoffs 
depend on the state of nature. When nature is kind and times are good, they offer the upswing (taxdeductibility) benefits of debt. And when nature is cruel and times are bad, they offer the downswing 
(bankruptcy costs) benefits of equity. They are, in the language of economists, a form of “contingent” 
contract when the contingency in question is crisis. 
As Chart 10 illustrates, the optimal fraction of CoCos in banks’ optimal debt structure is likely to be non-zero. 
In other words, a CDE capital structure might be a smart option for value-maximising investors and 
managers, even without the need for regulatory intervention. The social value of such a capital structure 
might, of course, be greater still – for example, because the social costs of crisis are higher than the private 
costs. That would justify a regulatory capital backstop. 
 
This capital structure shares the risk-sharing benefits of some other reform proposals. For example, it has 
similar risk-shifting properties to a mutual fund or limited purpose banking model. 17 When required, risk is 
shared across the capital structure, as in a mutual fund whose equity claims adjust in value. But riskspreading would only kick-in when it needs to. Banks are banks when nature is kind and mutual funds when 
it is cruel. They can be butterflies in summer, provided they are hedgehogs in winter. 
 
 
14 Commission of Experts for Limiting the Economic Risks Posed by Large Companies (2010). 15 Debt could also provide some incentive benefits in disciplining management (Jensen (1986)). 16 Leland (1994). 17 For example, Kotlikoff (2010). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
11 
11
From Here to There 
If such a structure is for the best in most states of the world, why does it not already exist? At least two 
legitimate concerns have been raised. First, might market-based triggers invite speculative attack by shortsellers? The concern is that CoCo holders may be able to short-sell a bank’s equity to force conversion, 
then using the proceeds of a CoCo conversion to cover their short position. 
There are several practical ways in which the contract design of CoCos could lean against these speculative 
incentives. Perhaps the simplest would be to base the conversion trigger on a weighted average of equity 
prices over some prior interval – say, 30 days.18 That would require short-sellers to fund their short positions 
for a longer period, at a commensurately greater cost. It would also create uncertainty about whether 
conversion would indeed occur, given the risk of prices bouncing back and the short-seller suffering a loss. 
Both would act as a speculative disincentive. 
A second potential firewall against speculative attack could come from imposing restrictions on the ability of 
short-sellers to cover their positions with the proceeds of conversion. Restrictions on naked short-selling are 
applied around the time of seasoned equity offerings in some jurisdictions.19 A rule to prevent the covering 
of short positions with the proceeds of a CoCo conversion could provide a further disincentive to destabilising 
short-selling of banks’ equity 
. 
A related concern is that CoCos alter the seniority structure of banks’ capital, as holders of CoCos potentially 
suffer a loss ahead of equity-holders. But provided the price at which CoCos convert to equity is close to the 
market price, conversion does not transfer value between existing equity-holders and CoCo investors. And 
provided conversion is into equity it need not imply investor loss. If a market move really is unjustified, prices 
will correct over time towards fundamentals. The holder of a converted CoCo will then garner the upside. 
 
So while CoCos are susceptible to market aberrations, these can in my view be managed. In this respect, 
market errors are fundamentally different to model errors. Market errors are temporary risk, while model 
errors are permanent uncertainty. Market error can be managed, while model error cannot. Put differently, 
with market-based CoCos the cost of Type I errors may be relatively modest for end-investors. 
A second key practical issue is whether it is plausible to imagine an investor base for CoCos emerging. 
Recent signs have been encouraging. Two major international banks have issued CoCos totalling around 
US$10bn since the start of the year. Credit Suisse recently issued a $2bn CoCo in a public auction, which 
was reportedly 11-times over-subscribed. Some market commentary suggests the CoCo market could grow 
to around $1 trillion over the next few years.20 
 
18 For example, Flannery (2010). 19 Safieddine and Wilhelm (1996), for example. 20 Standard & Poor’s (2010). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
12 
12
Despite that progress, investor demand for CoCos remains uncertain. Tellingly, none of the CoCos issued 
so far have had market-based triggers, none have a graduated ladder of triggers and, in my view, none 
extend very far up banks’ capital structure. In short, the case for a CDE capital structure is, at present, 
unproven. 
But there may be simple, evolutionary ways to catalyse market demand to achieve this capital structure. 
Perhaps the simplest way to do so would be to require banks to make discretionary distributions to staff and 
shareholders in CoCos. In this way, an investor base for CoCos would emerge organically and grow in line 
with banks’ profits 
. 
As well as catalysing the CoCo market, this distributive approach would have two further benefits. First, it 
would boost the resilience of banks by keeping revenues in the business and augmenting banks’ capital 
base. Given the large dividends and bonuses paid out by banks historically, distributing in CoCos could 
increase banks’ capital base materially. 
 
Consider UK banks. Imagine that 50% of bonuses had been paid in CoCos rather than cash from 2000 to 
2006. By 2007 at the start of the crisis, UK banks’ capital ratios would have been around 1 percentage point 
higher (Chart 11). Had 50% of dividends in addition been CoCoed, and assuming CoCos counted as Tier 1 
capital, capital ratios would have been 3 percentage points higher. That is roughly £70 billion, or around the 
amount of external capital UK banks raised during the crisis. 
Second, CoCo payouts potentially better align risk-taking incentives among staff and shareholders than cash 
or even equity payouts. The crisis demonstrated all too visibly some of the downsides of equity-based 
remuneration. One is the temptation to gamble for resurrection when poised on the brink, given the 
gambler’s option embedded in equity. A second is the perverse incentive not to seek external equity 
because of the dilutive impact it might have on managerial wealth. Both of these adverse side-effects were 
evident at Bear Stearns and Lehman Brothers. 
Remunerating management in CoCos removes this temptation, in much the same way as would credibly 
loss-absorbing sub-ordinated debt.21 In peacetime, management is no longer offered the upside of equity, 
only the downside. The asymmetry of payoffs embedded in peacetime bonus packages – “heads I win, tails 
society loses” – is neutralised somewhat by paying in CoCos. This better aligns risk for shareholders and 
staff with the risks for society at large. 
There is an old lesson, here, about eating your own cooking. This ought to help discipline the chefs, 
reducing the temptation to pursue risky recipes in the first place. Encouragingly, banking practice may 
 
21 Calomiris (2010) and Gordon (2010). 
All speeches are available online at www.bankofengland.co.uk/publications/speeches 
13 
13
already be beginning to match the theory. Several banks have recently announced they will in future 
consider remunerating shareholders and staff in CoCos. 
Conclusion 
Tackling complex banking through complex regulation is to fight fire with fire. This is unlikely to work in 
theory. Crisis experience suggests it has not worked in practice. A regulatory framework is needed in which 
the state is neither omniscient Walrasian auctioneer (shouting out hundreds of millions of risk prices) nor 
deep-pocketed financier (doling out hundreds of billions of taxpayers’ money). A regulatory Gosplan will 
work no better for bankers than it did for tractors. 
The role of regulation is instead to set the overarching rules of the game. In tackling banking stress, that 
means the framework for banks’ capital structure. As far as possible, that framework should aim to leave the 
pricing of risk ex-ante, and the consequences of risk ex-post, to the market. The framework outlined here 
could be one simple, robust and timely way to help achieve that. It is different. But it is far from radical. 
Nothing could be less radical than returning banks, and banking risk, to the market.

Credit lies at the heart of crises. Credit booms sow the seeds of subsequent credit crunches. This is 
a key lesson of past financial crashes, manias and panics (Minsky (1986), Kindleberger (1978), 
Rogoff and Reinhart (2009)). It was a lesson painfully re-taught to policymakers during the most 
recent financial crisis. 
This time’s credit cycle has been particularly severe and synchronous. In 2006, private sector credit 
across the UK, US and euro area rose by around 10%. During 2009, private credit in these 
countries fell by around 2%.1
 The knock-on consequences for real growth were equally severe and 
synchronous. Peak to trough, G7 real output fell by 3.6% during the Great Recession. 
In response, there have been widespread calls for remedial policy action. These proposals come in 
various stripes. Some have proposed a more active role for monetary policy in addressing financial 
imbalances (Taylor (2010), White (2009)). Others have suggested using new macro-prudential 
tools to rein in credit excesses (Borio and Lowe (2002), Bank of England (2009), Kashyap et al
(2010)). Others still have proposed a radical root-and-branch reform of the structure of banking 
(Kay (2009), Kotlikoff (2010)). 
Evaluating the merits of these proposals requires a conceptual understanding of the causes of the 
credit cycle and an empirical quantification of its dynamic behaviour. What is the underlying 
friction generating credit booms and busts? Are credit cycles distinct from cycles in the real 
economy? And how have they evolved, both over time and across countries? Answers to these 
questions should help frame public policy choices for curbing the credit cycle. 
To fix ideas, Section 2 sketches a model of the credit cycle. In this model, a lack of information 
provides incentives for banks to expand their balance sheets to boost profits and signal their ability 
to investors. This gives rise to a coordination failure, as banks collectively risk-up. That, in turn, 
generates a systematic credit boom and subsequent bust when risk is realised. 
Sections 3 and 4 present some empirical evidence on the credit cycle. Across countries and across a 
sweep of history, credit cycles are both clearly identifiable and regular. Typically, they presage 
 
1 See IMF (2010).
2 
banking crises. In their frequency and amplitude, credit cycles are distinct from the business cycle. 
There is also evidence of them arising from coordination failures, which generate spillover effects 
across banks and countries. 
Drawing on this evidence, Section 5 identifies some implications for the design of public policy. It 
suggests that neither monetary nor micro-prudential policy may be well-equipped to tackle the 
credit cycle. Instead, some new policy apparatus may be needed which (unlike monetary policy) 
targets bank balance sheets directly but which (unlike micro-prudential policy) does so 
systematically. This is one key dimension of so-called macro-prudential policy.2
 
Various international macro-prudential policy committees are, or are about to be, put in place – in 
the US the Financial Stability Oversight Committee, in the euro-area the European Systemic Risk 
Board and in the UK the Financial Policy Committee. These provide one element of a macroprudential policy framework. Other elements remain to be put in place. Knowledge of the sources 
and dynamics of the credit cycle will be important in assembling those missing pieces. This paper 
is intended to be a contribution towards that goal. 
2. A Model of the Credit Cycle 
We begin by sketching a model which captures some key features of past, and in particular the 
present, credit cycle. There are a number of existing models of the credit, or leverage, cycle. In all 
of these models, cyclicality in financial variables is aggravated by various micro-economic 
frictions. These frictions typically then amplify fluctuations in the real economy. Broadly, these 
models can be classified according to the underlying micro-economic friction. 
For example, a well-established body of literature has looked at the effects of asymmetric 
information between borrowers and lenders in placing limits on credit (Bernanke, Gertler and 
Gilchrist (1996), Holstrom and Tirole (1997)). These constraints can be loosened by the borrower 
pledging collateral to the lender, in effect as a substitute for information (Kiyotaki and Moore 
(1997)). This solves one problem, but at the potential expense of another: movements in the prices 
of collateral then have the potential to aggravate cycles in leverage and credit (Geanakoplos 
(2010)). These cycles can in turn act as a “financial accelerator” for the business cycle. These are 
typically models of a representative bank and credit-constrained investor. 
 
2
 There are other potential non-cyclical instruments of macro-prudential policy, including instruments to tackle 
systemically important financial institutions (SIFIs). 
3 
A second potential source of credit market friction arises from coordination failures among lenders 
(Gorton and He (2008)). In these models, banks are heterogeneous and their behaviour strategic. 
The individually rational actions of heterogeneous lenders can generate collectively sub-optimal 
credit provision in both the upswing (a credit boom) and the downswing (a credit crunch), perhaps 
through herding (Acharya (2009), Acharya and Yorulmazer (2008)). This is the result of a 
collective action, or co-ordination, problem among banks. 
In credit markets, these co-ordination failures are far from new. Keynes memorably noted: 
“A sound banker, alas, is not one who foresees danger and avoids it, but one who, when 
he is ruined, is ruined in a conventional and orthodox way with his fellows, so that no 
one can really blame him” (Keynes (1931)). 
Eighty years later, Chuck Prince, then-CEO of Citibank, captured the collective action problem 
thus: 
“As long as the music is playing, you’ve got to get up and dance. We’re still dancing” 
(Prince (2006)). 
As Prince’s quote attests, these incentives were a key driver of risk-taking behaviour in the run-up 
to the crisis. In the face of stiffening competition, banks were increasingly required to keep pace 
with the returns on equity offered by their rivals – a case not so much of “keeping up with the 
Joneses” as “keeping up with the Goldmans”. To achieve these higher returns, it was individually 
rational for banks to increase their risk profiles. They did so in various ways including through 
higher leverage, marked to market gains on trading books and writing contracts with deep out-ofthe-money option payoffs (Alessandri and Haldane (2009)). 
These strategies had the desired effect. They generated high and synchronous reported returns 
(Chart 1). But they did so at the expense of higher risk in aggregate – a case of a competitive coordination failure. Reported returns were, in this sense, risk illusory. As those risk illusions were 
shattered, all of the pre-crisis gains in banks’ reported returns were lost. Our model captures the 
spirit of these dynamics, with short-run risk-taking to preserve reputation and boost returns (a credit 
boom) in time giving way to longer-term collective costs when the music ceases (a credit crunch). 
4 
Chart 1: Price to Book ratios for UK, US and European institutions (a) 
Sources: Bloomberg, Thomson Datastream and Bank calculations 
(a) Chart shows the ratio of share price to book value per share. Simple averages of the 
ratio in each peer group are used. The chart plots the three month rolling average. 
(b) Excludes Nationwide and Britannia from Major UK Banks peer group. 
2.1 The set up 
We develop a simple framework, in the spirit of Rajan (1994), to capture these collective action 
failures. Rajan’s (op.cit.) model generates multiple equilibria. We use a version of the Morris and 
Shin (2003) ‘global games’ framework to pin down a unique equilibrium for the coordination game 
among banks, which then allows an evaluation of policy options.3
 
The set up is as follows. Each period there is a continuum of agents, indexed by ݅ א ሾ0,1ሿ, all of 
whom work as financial intermediaries for a single period. In other words, bankers have, by 
assumption, short horizons. They aim to maximise their reputation in the market at the end of this 
period. 4 At the beginning of each period, each bank originates a risky asset, the return on which 
 
3
 For other applications of the “global games” framework to banking, see inter alia, Rochet and Vives (2004) and 
Goldstein and Pauzner (2005). Broadly speaking, these models use the framework to pin down uniqueness in the 
coordination game played by the depositors of fragile banks, a la Diamond and Dybvig (1983). For a survey of 
coordination games in macroeconomics, see Cooper (1999) and Morris and Shin (2000). 
4 Individuals might receive intrinsic utility from being thought of as high ability, or it might matter materially if future 
job prospects (including wages) depend on current reputation (see Scharfstein and Stein (1990)). The threat of corporate 
takeovers might also generate shortened managerial horizons, or agents might engage in hyperbolic discounting, which 
generates a short-term bias. 
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
91 92 93 94 95 96 97 98 99 00 01 02 03 04 05 06 07 08 09 10
European LCFIs
US LCFIs
Major UK banks (b)
Ratio
5 
depends on (i) the banker’s ability, which can be high or low, and (ii) the macro state, which can be 
good or bad. Both are unobservable to the market. 
The macro state is good with probability ߠ .High bank ability and a good macro state increase the 
probability of positive asset returns. That is, when the macro state is bad, assets turn out to be bad, 
irrespective of bankers’ ability. But when the macro state is good, both high and low ability 
bankers achieve high returns with some probability, with high ability types always achieving high 
returns and low ability types achieving high returns with probability ݍ൏1. 
Asset returns are realised in the middle of each period and are observed only by banks. If the asset 
is good, banks make a profit (normalised to be a negligible amount). If the asset is bad, banks face 
a loss of -1. Because of their short horizons, bankers care about both the present value of their asset 
returns and their reputation ݌ in the market, where ݌ is the probability assigned by the market to a 
banker being high ability. 
The market is unable to observe banks’ actions directly, so instead infers bankers’ ability from 
observed bank earnings. Banks’ earnings are affected by their choices when returns are low. If an 
asset turns out to be bad, banks can hide negative earnings in the short run by engaging in risky 
policies to boost returns – in effect, engaging in risk illusion. When a bank engages in a risky 
policy, an immediate loss is avoided with probability ܽ. But risky policies always involve longer 
term expected losses of ܿ൐ܽ. 
If instead of setting risky policies banks choose immediately to liquidate assets, they set a tight 
policy, realising a loss of -1 for sure. So banks’ choices when faced with low asset returns 
determine their risk profiles. If returns are low, under a risky policy banks can increase short term 
earnings with some positive probability, but must incur a long term loss; while under a tight policy, 
banks can accept low short term earnings for sure now, avoiding losses in the future. 
Suppose banks attach weight ߛ א ሾ0,1ሿ to their reputation in the market and ሺ1 െ ߛሻ to the net present 
value of future profits. When the asset is good, banks earn a return normalised to zero. When the 
asset is bad, two strategies are possible. If a bank chooses a risky (superscript ݎ (policy, it obtains 
ݑ௥ሺߠ݈ ,ሻ ൌ ሺ1െߛሻሾെሺ1െܽሻ െ ܿሿ ൅ ݌ߛ௥ሺߠ݈ ,ሻ,
6 
where a proportion ݈ of all banks also set a risky policy. The bank makes a loss of 1 with 
probability 1െܽ, incurring long term cost ܿ while enjoying reputation ݌௥ሺߠ݈ ,ሻ.
5,6
When the loan is bad, setting a tight (superscript ݐ (policy yields 
ݑ௧ሺߠ݈ ,ሻ ൌ ሺ1െߛሻሺെ1ሻ ൅ ݌ߛ௧ሺߠ݈ ,ሻ, 
as the probability of a loss is unity under a tight policy and the bank obtains reputation ݌௧ሺߠ݈ ,ሻ.
7
The evolution of reputation is key for determining behavioural dynamics in the model. The 
marginal effect on reputation of banks adopting risky rather than tight policies is given by 
݌ሺߠ݈ ,ሻ ؠ ݌ ௥ሺߠ݈ ,ሻ െ ݌௧ ሺߠ݈ ,ሻ,
We posit ݌ఏ ൐ 0: improvements in the macro state increase the incentive banks face to liberalise 
their risk management policies. When the macro state is good, any negative earnings are more 
likely to be attributed to low ability. This is the “Prince constraint”. A concern for reputation 
incentivises banks to seek risky ventures (to keep dancing), the more so the better is the market’s 
prior on the macro state (the louder the music). 
We also posit ݌ ൐ ௟0: as a larger proportion of banks set risky credit policies, the larger the incentive 
to pursue similarly risky policies. When others are posting positive earnings, the reputational loss 
from foreclosing and taking losses is that much greater. This is the “Keynes constraint”. It is better 
for your reputation to fail conventionally than to succeed unconventionally. 
These conditions generate the potential for credit booms and busts. The better is the macro state, 
the greater banks’ incentive to pursue risky projects to preserve reputation, as the market attributes 
low earnings to low ability. But one bank’s announcement of positive earnings encourages others 
to announce positive earnings by setting risky policies too. In Rajan’s model this strategic 
complementarity between banks generates multiple equilibria: in sufficiently good states, banks 
coordinate on risky policies (“credit booms”), while in bad states they coordinate on tight policies 
(“credit crunches”). In between, either equilibrium is possible. 
 
5 We assume no discounting, as in Rajan (1994). The qualitative results would not change were we to consider it.
6
݌௥ሺߠ݈ ,ሻ is the probability assigned by the market of the bank being high ability, conditional on the bank setting a risky 
policy (unobserved by the market), the asset being bad, the macro state being ߠ and proportion ݈ other banks also setting 
risky policies.
7
݌௧ሺߠ݈ ,ሻ is defined analogously to ݌௥ሺߠ݈ ,ሻ, but is conditioned on tight rather than risky policies.
7 
Define the payoff function ߨሺߠ݈ ,ሻ ؠ ݑ௥ሺߠ݈ ,ሻ െ ݑ௧ሺߠ݈ ,ሻ. Substituting gives: 
ߨሺߠ݈ ,ሻ ൌ െሺ1െߛሻሺܿെܽሻ ൅ ݌ߛሺߠ݈ ,ሻ.
This captures the marginal return to adopting a risky policy. It allows us to define the regions of 
fundamentals over which risky policies and tight policies dominate respectively. A risky policy 
dominates a tight policy, even when no other banks set risky policies (݈ൌ0), when ߠ൐ߠҧ where 
݌ሺߠҧ, 0ሻ ൌ ሺܿെܽሻ ൬
ߛ1െ
.൰ ߛ
A tight policy dominates a risky policy, even when other banks set risky policies (݈ൌ1), when ߠ൏ߠ
where 
݌ሺߠ ,1ሻ ൌ ሺܿ െ ܽሻ ൬1െߛ
.൰ ߛ
In the intermediate range ߠ൏ ߠ൏ߠҧ, there are multiple equilibria, with risky (tight) credit policies 
optimal when others adopt risky (tight) policies too. 
2.2 The model in a ‘global game’
Next, we apply the technology of Morris and Shin (2003) to define a unique equilibrium from this 
game. Assume that banks observe fundamentals with some small amount of noise, with bank ݅
receiving signal 
,0 ൐ ߪ ,௜ߝߪ ൅ ߠ ൌ௜ ݔ
where the noise terms ߝ ௜are distributed in the population with continuous density ݃ሺ. ሻ with support 
on the real line. Our model satisfies the conditions set out in Morris and Shin (2003) for there to 
exist a unique equilibrium in this game. In particular, each bank's strategy ݏሺݔሻ conditional on its 
signal ݔ satisfies ݏሺݔሻ ൌ{Tight} when in receipt of a low signal falling short of some critical level כߠ ,
and adopting strategy ݏሺݔሻ ൌ{Risky} when in receipt of a high signal exceeding some critical level 
כߠ .The condition determining this threshold כߠ is (see Appendix) 
න ݌ሺכߠ݈ ,ሻ݈݀
ଵ
௟ୀ଴
ൌ ሺܿെܽሻ ൬
ߛ1െ
ߛ ൰. ሺ1ሻ
8 
Condition (1) allows us to perform comparative statics on the unique coordination equilibrium. 
Using (1) it follows that the threshold level of fundamentals above which banks set risky policies is 
(a) increasing in the long-term cost of extending bad credit ܿ, and (b) decreasing in relative 
reputational concerns ߛ ,or ݀כߠ
כߠ݀ ,0/݀ܿ ൐ 
.0 ൏ ߛ݀/
Both results accord with intuition. First, increasing the cost to banks of pursuing risky strategies 
causes banks to coordinate on risky policies only at very high levels of fundamentals. Or, put 
differently, safety prevails over a larger range of fundamentals when risky policies are costly. 
Prudential policies (discussed further in Section 5) are one means of raising the cost to banks of 
pursuing such risky strategies. 
These prudential policies would have both direct effects and strategic effects in the model. A rise in 
long term costs raises the direct cost to bank ݅ of adopting a risky strategy. But it also leads bank ݅
to expect fewer other banks to adopt risky policies. So a rise in c has both a direct effect on banks’ 
actions and an effect through banks’ expectations of others’ actions.8
 This expectational channel is 
crucial from a policy perspective. 
Second, reputational concerns act in the opposite direction. As greater weight is placed on shortterm reputation, incentives are sharpened to signal high ability by pursuing risky policies, even 
when their signalling effect is relatively small (ߠ is low). As in Rajan (1994), a tight credit policy 
becomes optimal for all levels of fundamentals as ߛ՜0, or as reputational concerns vanish. 
Conversely, if increased competition and deregulation increase reputational concerns, this increases 
the propensity of the system to periodic credit booms and subsequent busts. 
2.3 Dynamics 
This model can generate credit cycles which amplify cycles in the real economy relative to a world 
without credit cycle frictions.9
 Without frictions, bad loans are liquidated immediately, allowing 
new assets to be originated next period to unencumbered borrowers. By contrast, extending credit 
to bad risks eventually results in balance sheet impairment for both borrowers and lenders. 
 
8
 These expectation effects are necessarily intra-temporal in our model. The actual operation of macro-prudential 
policy may well have effects through inter-temporal expectations as well. See Section 5 below.
9 The property that global games generate endogenous cycles in dynamic settings has been analysed in, inter alia, 
Steiner (2006) and Giannitsarou and Toxvaerd (2007). 
9 
We perform a simple simulation to illustrate these dynamics. Suppose the macro state moves 
stochastically around its trend as a result of a sequence of iid productivity shocks. The macro state 
is mean-reverting as long as banks set tight policies when they realise low asset returns. This 
occurs when reputational effects are switched off (ߛ ൌ 0ሻ. 
Next suppose reputational concerns are introduced, leading banks to risk up to signal high ability. 
Because of strategic complementarities, banks do this whenever ߠ௧ ൐ כߠ .This leads to subsequent 
impairment of the borrowing sector. These dynamics are captured in the simple specification 
ߠ௧ ൌ ߠߩ௧ିଵ ൅ ܫሺߠ௧ିଵ; כߠሻݑ௧ ൅ ߝ௧ , ሺ2ሻ 
where ݐ indexes time, 0൏ߩ൏1, ሼݑ௧, ߝ௧ሽ are iid normal shocks and ܫሺߠ௧ିଵ; כߠሻ is the indicator function 
taking value 1 when ߠ௧ିଵ ൐ כߠ and value 0 otherwise. As reputational concerns disappear, the term 
ܫሺߠ௧ିଵ; כߠሻݑ௧ goes to zero, leaving simple autoregressive dynamics. But with reputational 
externalities, productivity shocks become amplified by the credit cycle: a high realisation of ߝ in 
period ݐെ1 could push the macro state above the risky policy threshold, at which point the 
dynamics of ݑ௧ begin also to shape the real economy. 
The distribution of ݑ௧ is assumed to have a higher mean and variance than ߝ௧ to simulate the effects 
of the credit boom. In particular, we constrain the distribution of ݑ௧ relative to ߝ௧ such that expected 
risk-adjusted fundamentals are constant over time.10 So we have, in effect, a regime-switching 
model. When banks play safe strategies, fundamentals are relatively stable. When banks risk up, 
they create a temporary improvement in expected fundamentals at the cost of greater volatility. 
If fundamentals evolve according to (2), and bankers at time ݐ know this, past fundamentals act as 
public signals. Morris and Shin (2003) show that when past fundamentals are observed with 
sufficient noise, the global game described above continues to have a unique equilibrium. If 
fundamentals were high yesterday, bankers know that fundamentals are likely to be high today. 
Intuitively, this makes them more likely to choose the risky policy. In the annex, we sketch the 
public signal game and find its equilibrium. We show that, when the threshold is ככߠ in the public 
signal game, we have that ݀ככߠ݀/ߠ௧ିଵ ൏ 0 as long as the public signal contains information for the 
bankers. When this information disappears, the threshold becomes independent of past 
fundamentals (see Annex 1(b)). In addition, it continues to be the case that ݀ככߠ ൐ ܿ݀/0 and 
 
10 In particular, the Sharpe ratio (the expected value of fundamentals relative to their standard deviation) is constant 
across both regimes. 
10 
݀ככߠ݀/ߛ ൏ 0, such that higher costs of risk taking and lower reputational concerns decrease the 
probability of risky policies being adopted. 
This dynamic specification has some interesting properties. First, even small shocks can have 
disproportionately large effects on fundamentals depending on the initial conditions. In particular, 
if fundamentals happen to be close to the switching threshold, a further positive shock will lead all 
firms to coordinate on the high risk-taking strategy. That drives up both the expected value of 
fundamentals and its volatility. This generates a form of path dependence. Second, credit cycles 
are endogenous in this framework. As firms coordinate on high risk strategies, this drives up the 
expected value of fundamentals. But it also drives up volatility due to risk illusion. Because riskadjusted returns are constant, this must entail an increase in the probability of there being a severe 
negative shock, so the probability of a crash rises. Third, when past fundamentals are observed with 
sufficient accuracy, improvements in outturns can signal trouble ahead. It becomes more likely, 
then, that bankers will coordinate on the risk taking equilibrium. 
Chart 2 shows a simulation of the model when ܿ ൌ 1, ߛ ൌ 0.1, ߩ ൌ 0.9, ܽ ൌ 0.5 for 100 periods with 
normally distributed shocks. The frictionless benchmark model generates the path shown by the 
maroon line. As specified, the path is mean reverting, with movements in the macro state around 
trend. Allowing for reputational effects generates the blue path, giving a threshold level of 
fundamentals כߠ shown by the green dashed line. As the macro state hits the threshold, banks adopt 
risky policies to signal high ability. In this simulation this happens four times, in periods 3, 50, 68 
and 72. 
Consider the threshold breach in period 72. As during the Great Moderation, the good macro state 
at first persists. The credit boom fuels high fundamentals. But while headline fundamentals have 
improved, risk-adjusted fundamentals have not. Eventually, this results in a crunch in credit and a 
sharp deterioration in the macro state, as risk is realised. 
Now suppose that financial liberalisation generates heightened competition between banks, 
increasing the market premium on reputation. This raises the value of ߛ in the model. Chart 3 
shows the effects of the reputational weight being raised ሺߛ ൌ 0.3ሻ. The threshold for risky policies 
falls. Relative to the baseline, there is an additional large and persistent boom in the middle of the 
simulation, followed by a period of below counterfactual fundamentals. The path of the economy is 
more volatile, with deviations from trend larger and more persistent. In other words, the credit 
cycle has real and adverse consequences. 
11 
Chart 2: Model simulation with low reputational weight 
Source: Bank calculations 
Chart 3: Model simulation with high reputational weight 
Source: Bank calculations 
2.4 Empirical implications 
Several empirical implications follow from the model. First, strategic complementarities 
incentivise banks to adopt risky strategies in a coordinated fashion during the boom. So the 
20
0
20
40
60
80
100
120
0 20 40 60 80 100
θ
t
θ frictionless θ*
-
+
20
0
20
40
60
80
100
120
0 20 40 60 80 100
θ
t
θ (high γ) frictionless θ*
-
+
12 
dynamics of the model predict that we should observe cycles in financial activity at a macroeconomic level. Initial productivity improvements are amplified into lending booms, which are 
followed by credit busts and, potentially, crises. Second, at a micro-economic level, the 
coordination of risky strategies during the boom should compress the dispersion of bank earnings, 
as low ability banks masquerade as high ability banks during good times. But during the bust, when 
the macro state turns bad, the dispersion of banks’ earnings should increase as low ability types 
crystallise losses while high ability types do not. We turn next to the macro and micro evidence. 
3. Credit Cycle Dynamics 
To what extent are the macroeconomic credit cycle dynamics predicted by the model present in the 
real world? To assess that, we draw on a dataset recently developed by Schularick and Taylor 
(2009). This covers a lengthy time-series (often more than a century of data) across 12 developed 
countries.11 It enables us to identify lower (than business cycle) frequency movements in credit, 
just as Comin and Gertler (2006) do for post World War II US GDP.12 We concentrate on results 
for the UK and US, though broadly similar patterns are evident for the other ten countries. 
Table 1 presents some summary statistics (mean and standard deviation) of real GDP growth, real 
loan growth and real bank asset growth in the UK and the US since 1880.13 The sample is split into 
four periods: 1880-1913; 1914-1945; 1945-79; and 1980-2008. Charts 4 and 5, meanwhile, plot 
loan or asset to GDP ratios for the UK and US over the sample. Several features are clear: 
 Average real GDP growth is little changed either side of the wars (Table 1). But real credit 
has grown around twice as quickly since 1945. In consequence, loan/GDP ratios trend 
upwards from around 1945, consistent with financial liberalisation and deepening (Charts 4 
and 5). 
 
11 The countries covered are: Australia, Canada, Germany, Denmark, Spain, France, United Kingdom, Italy, 
Netherlands, Norway, Sweden and the United States.
12 They argue that medium term cycles in GDP reflect persistent responses of real activity to the high-frequency 
fluctuations normally associated with the “business cycle” (Comin and Gertler, op. cit.). In this sense, there is a direct 
analogy with our model, in which strategic complementarities in risk taking can generate persistent credit cycles in 
response to high-frequency fluctuations in fundamentals. 
13 The bank loans series consists of total domestic currency loans of banks and banking institutions to companies and 
households, while the assets series consists of total domestic currency assets of banks and banking institutions. For a 
full description of the data and its sources, see Schularick and Taylor (2009), Appendix B.
13 
 The same general pattern is evident in the volatilities of output and credit. The variability of 
real GDP growth has, if anything, fallen in the period since 1945. The standard deviation of 
real credit growth rose in the years immediately following 1945 and has remained above 
those levels, especially since 1980. 
 Since 1945, the standard deviation of real credit growth has been around five times that of 
real activity. 
Chart 4: Ratio of Loans to GDP and Assets to GDP (UK) 
Sources: Shularick and Taylor (2009) and Bank calculations. 
Chart 5: Ratio of Loans to GDP and Assets to GDP (US) 
Sources: Shularick and Taylor (2009) and Bank calculations. 
0
0.5
1
1.5
2
2.5
3
3.5
1870 80 90 1900 10 20 30 40 50 60 70 80 90 2000 10
Ratio loans:GDP
assets:GDP
0
0.2
0.4
0.6
0.8
1
1.2
1870 80 90 1900 10 20 30 40 50 60 70 80 90 2000 10
Ratio loans:GDP
assets:GDP
14 
On the face of it, these summary statistics are consistent with credit dynamics being distinct from 
GDP dynamics. It is possible to formalise this intuition by using filtering techniques to extract the 
cycle in credit. To do so, we apply a band-pass filter to the data.14 This isolates the cyclical 
component of a series operating in a frequency range specified by the user.15
We begin with estimates of the spectral density for real loan growth. Chart 6 plots one such 
estimate, along with approximate 90% confidence bands.16 The peak in the density at a (normalised) 
frequency of around 0.18 suggests a cycle with duration of around 11 years – a medium term cycle. 
The smaller peak at around 0.45 corresponds to a business cycle frequency of around 4.5 years. The 
relative size of these two peaks suggests that medium term fluctuations are an important source of 
overall variation in real loan growth. But the limited sample length means that the confidence bands 
around our estimated density are large. 
 
14 We also experimented with the Hodrick-Prescott filter. Although the choice of an appropriate smoothing parameter 
is not straightforward a priori, this approach produced broadly similar results to the band-pass filter. See e.g. Harvey 
and Jaeger (1993) and Canova (1998) for discussions. 
15 In what follows, we use Christiano and Fitzgerald’s (2003) optimal finite sample approximation to the band pass 
filter. An alternative to Christiano and Fitzgerald’s procedure is provided by Baxter and King (1999). Christiano and 
Fitzgerald provide evidence to suggest that their optimal finite sample approximation is preferable to Baxter and King’s 
when extracting cycles at lower frequencies.) Suppose the time series in question is a stationary stochastic process ሼݕ௧ሽ
with expectation ߤ .Its auto-covariance is ߛሺ߬ሻ ൌ ܧሾሺݕ௧ െ ߤሻሺݕ௧ିఛ െ ߤሻሿ. The properties of the time series can be captured in 
the frequency domain by taking a Fourier transform of the auto-covariances yielding the power spectrum 
݂ሺ߱ሻ ൌ 1
2ߨ ෍ ߛሺ߬ሻ
ஶ
ఛୀିஶ
݁ି௜ఠఛ,
where ߱ is the frequency (in radians) in the range ሾെߨ ,ߨሿ (and ݅ ൌ ሺെ1ሻଵ/ଶ). Since the power spectrum is a function of the 
auto-covariances (including the variance ߛሺ0ሻ), it can be viewed as a decomposition of the variance of the time-series in 
terms of frequency. When the power spectrum is normalised by the variance ߛሺ0ሻ, the resulting standardised function is 
known as the spectral density. The filter then isolates ‘bands’ in the frequency domain of the spectral density and 
returns the resulting series. 
16 We estimate the spectral density by smoothing the sample periodogram using a Parzen window with lag truncation 
parameter ܯ .We experimented with various values of ܯ .In general, this trades off the variance of the estimate of 
݂ሺ߱ሻ with the bias. Higher values of ܯ entail less bias, but a greater variance and wider confidence intervals. The 
confidence intervals are plotted using the ߯ଶ approximation discussed in Priestley (1999).
15 
Chart 6: Estimated spectral density for UK real loan growth, 1880-2008 
Notes: Approximate 90% confidence bands shown, using the ߯ଶ approximation discussed in Priestley (1999). A Parzen window with ܯ ൌ 35
was used to smooth the periodogram. 
Using the estimated spectral density to inform our search, Charts 7-8 show cyclical fluctuations in 
real loan growth in the UK and US. Since the estimated density is subject to uncertainty, we 
expand the frequency domain over which we plot the medium term cycle to 8-20 years. Consistent 
with the estimated density, shorter-term, business cycle fluctuations, say between 2-8 years, were 
typically not found to account for much of the overall cyclical variation in credit.17 In other words, 
credit growth exhibits a clear cyclical pattern with a medium-term orientation. We estimate the 
standard deviation of the 8-20 year cycle to be around 10%, with a 95% confidence interval of 
[7.00%,12.8%].18 Hence the credit cycle appears to be a well-defined empirical regularity. It also 
appears to have been operating for well over a century. And its frequency suggests factors other 
 
17 If the series for the UK are detrended, business cycle frequency fluctuations in real loan growth explained around 
10% of the overall variation in real loan growth, as measured by the coefficient of variation for this cycle. By contrast, 
the medium term cycle explains around 30% of the variation in detrended real loans for the UK. 
18 Following Comin and Gertler (2006), we estimated the standard error of the standard deviation of the [8,20] year 
cycle using the Delta method and GMM, where we used the Newey-West estimator of the covariance matrix. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Normalised frequency
Density
16 
than the business cycle may be responsible for driving it – including, for example, financial 
liberalisation and competition. 
Table 1: Summary statistics of real GDP growth, real loan growth and real 
bank asset growth in the UK and the US 
Mean Standard Deviation 
GDP Loans Assets GDP Loans Assets 
UK 
1880-1913 1.8% 2.7% 3.0% 4.1% 3.8% 3.3% 
1914-1945 1.2% -1.4% 2.0% 4.7% 12.3% 5.7% 
1946-1979 2.5% 6.4% 3.1% 2.0% 12.9% 10.3% 
1980-2008 2.2% 6.6% 7.3% 2.0% 4.5% 6.7% 
US 
1880-1913 3.5% 5.8% 5.9% 4.6% 4.2% 4.0% 
1914-1945 3.8% 0.2% 2.9% 8.2% 6.5% 4.9% 
1946-1979 3.1% 6.4% 4.7% 3.7% 5.4% 3.2% 
1980-2008 2.2% 3.6% 3.2% 2.1% 4.0% 3.5% 
Sources: Shularick and Taylor (2009) and Bank calculations. 
The credit cycle is distinct from the business cycle in amplitude as well as frequency. To show this, 
Charts 7-8 plot the medium-term cycles in real GDP for the UK and US alongside the credit cycle. 
The amplitude of the credit cycle is twice that of fluctuations in GDP over the medium term. It is 
roughly five times that of fluctuations in GDP at conventional business cycle frequencies. The 
peak-to-trough variation in the typical credit cycle has been around 20 percentage points in the UK. 
For real GDP, it is around 10 percentage points. As a result, ratios of credit to GDP themselves 
exhibit a distinct cyclical pattern in the UK and US (Charts 4-5). 
17 
Chart 7: Medium-term cycle in real GDP and credit (UK) 
Source: Bank calculations. 
Chart 8: Medium-term cycle in real GDP and credit (US) 
Source: Bank calculations. 
The same broad patterns are evident in the data when moving from credit to asset (equity and 
house) prices in the UK and US (Charts 9-10). A clear financial cycle is evident in both these 
series, distinct from the typical business cycle in its amplitude as well as frequency. The peak to 
trough variation in asset prices is, if anything, greater than for credit: around 40 percentage points 
for equities and around 15 percentage points for house prices over medium-term horizons in the 
UK. Charts 11 and 12 demonstrate those differences, plotting medium-term frequency cycles in 
GDP and asset prices in the UK and US since 1945. 
-20
-15
-10
-5
0
5
10
15
20
1870 80 90 1900 10 20 30 40 50 60 70 80 90 2000
Per cent rGDP
rLoans
20
15
10
5
0
5
10
15
20
1870 80 90 1900 10 20 30 40 50 60 70 80 90 2000
Per cent rLoans
rGDP
-
+
18 
Chart 9: Medium-term cyclical fluctuation in UK real equity and house prices 
Source: Bank calculations. 
Chart 10: Medium-term cyclical fluctuation in US real equity and house prices 
Source: Bank calculations. 
30
20
10
0
10
20
30
1870 80 90 1900 10 20 30 40 50 60 70 80 90 2000
Per cent
Equity prices
House prices
-
+
50
40
30
20
10
0
10
20
30
40
50
1870 80 90 1900 10 20 30 40 50 60 70 80 90 2000
Per cent
Equity Prices
House prices
-
+
19 
Chart 11: Medium-term frequency cycles in GDP and asset prices in the UK 
Source: Bank calculations.
Chart 12: Medium-term frequency cycles in GDP and asset prices in the US 
Source: Bank calculations. 
30
20
10
0
10
20
30
1940 1950 1960 1970 1980 1990 2000 2010
Per cent GDP House prices
Equity prices
-
+
30
25
20
15
10
5
0
5
10
15
20
25
1940 1950 1960 1970 1980 1990 2000 2010
GDP House prices Per cent
Equity prices
-
+
20 
In Annex 2 we show results for the other 10 countries, applying the same techniques to real GDP 
and real credit. Although the cycles are sometimes not as regular, the same general cyclical patterns 
in output and credit are present using the wider panel of countries. As Irving Fisher noted almost 
eighty years ago: 
“The old and apparently still persistent notion of “the” business cycle, as a single, simple, 
self-generating cycle,…is a myth. Instead of one cycle, there are many co-existing 
cycles, constantly aggravating or neutralising each other, as well as co-existing with 
many non-cyclical forces” (Fisher (1933)). 
Historically, the credit cycle appears to have been just such a phenomenon. 
But why should we care? One reason might be that credit booms and busts are systematically 
related to the incidence of crises, with their associated social costs. Using the filtered credit series 
for the 12 countries, and the dating of banking and currency crises from Bordo et al (2001), it is 
possible to test this hypothesis.19 Over the sample period, these countries were in a state of banking 
and/or currency crisis anywhere between 10% and 25% of the time. This broadly matches the 
frequency of the credit cycle.
Table 3: The credit cycle and subsequent crises 
Total peaks 
1880-2008* 
Crisis years** 
within 5 years 
following a peak 
% peaks with 
crisis years 
within the 
following 5 
years 
Banking crisis 
within 5 years 
following a peak 
% peaks with 
banking crisis 
within the 
following 5 years 
AUS 9 6 66.7% 2 22.2% 
CAN 11 6 54.5% 2 18.2% 
DEU 9 2 22.2% 1 11.1% 
DNK 10 4 40.0% 3 30.0% 
ESP 8 5 62.5% 2 25.0% 
FRA 5 3 60.0% 1 20.0% 
GBR 9 7 77.8% 3 33.3% 
ITA 11 8 72.7% 6 54.5% 
NLD 8 1 12.5% 1 12.5% 
NOR 13 5 38.5% 2 15.4% 
SWE 10 4 40.0% 2 20.0% 
USA 9 6 66.7% 5 55.6% 
112 57 50.9% 30 26.8% 
 
* Interwar data missing for most countries. Data coverage incomplete for other countries e.g. only post-1945 
data available for France. 
** Defined as years in which either a banking crisis or a currency crisis or both (“twin crisis”) occur. 
Source: Bordo et. al. (2001) and Bank calculations. 
 
19 We also used crisis dating from Lopez-Salido and Nelson (2010) for the US.
21 
Table 3 asks what proportion of crisis years occurred within 5 years of the peak in the credit cycle 
in those countries. On average, more than half of all financial crisis years across the 12 countries 
appear to have been preceded by a credit boom. Among Anglo-Saxon countries, such as the US, 
UK and Australia, closer to 75% of crisis years occurred following a credit boom. This is relatively 
concrete evidence of the credit cycle having real and damaging effects on output.20,21 
4. Credit Cycle Spillovers
This empirical evidence to date, operating at a macro-economic level, is consistent with the model 
in Section 2: a credit cycle is clearly discernible; its frequency is different and its amplitude larger 
than the business cycle; and its fluctuations may exacerbate the business cycle. But the model also 
has implications at a micro-economic level, arising from strategic complementarities, or spillover 
effects, across firms. In this section we consider empirical evidence on such credit spillover effects. 
The model implies that the cross-sectional distribution of returns to banking should be compressed 
during a credit boom, as banks seek to keep up with competitors by collectively boosting returns. 
This is a time-series phenomenon. A second implication is that the dispersion in returns may be 
smaller for financial than for non-financial companies to the extent that former are more susceptible 
to risk illusion. This is a cross-sectional phenomenon. 
To assess these hypotheses, Charts 13 and 14 look at the dispersion of returns for publicly listed US 
banks calculated in two ways: implied market returns on bank equity and reported returns on 
banks’ equity (ROE). For comparison, the dispersion of returns among the largest US non-financial 
companies is also shown, together with identified periods of credit boom. Two features are striking. 
First, measures of return dispersion are consistently lower for banks than for non-banks: simple 
t-tests reject the null of equal mean dispersion between the two types of institution at the 1% level. 
Given the much higher levels of leverage among banks than non-banks, that is surprising: high 
 
20 We also conducted some probit regressions using lagged real credit growth to predict the probability of banking 
crisis, following Schularick and Taylor (op.cit.). We replicated their results: lagged real credit growth (up to five or six 
years) was jointly significant in positively contributing to the probability of there being a subsequent banking crisis, and 
robust across sub-samples pre and post WWII. The joint significance of five-six annual lags of real credit growth is 
consistent with our depiction of the credit cycles as a medium term phenomenon in which sustained booms are 
statistically significantly related to banking crises. 
21 See also Bordo and Haubrich (2010), who show that more severe financial events are associated with more severe 
real effects in the US 1875—2007. 
22 
leverage should bias against finding lower dispersion in banking. It is consistent with stronger 
herding incentives in banking. 
Second, measures of return dispersion tend to fall during periods of credit boom, statistically 
significantly so at the 1% level. For example, measures of equity return dispersion hit all-time lows 
at the height of the recent credit boom in 2006-2007.22 At first sight this might seem to run counter 
to the conventional wisdom that the correlation between asset prices tends to increase dramatically 
during times of stress, while remaining muted in boom times. But our model suggests that there 
may be strong behavioural incentives for banks to adopt highly correlated portfolios in the boom, as 
low ability types attempt to masquerade as high ability types. Banks’ reported returns would then 
move together, even when prices across a range of different asset classes might not. The dispersion 
we find in the data in times of stress is a corollary of this. In the bust, low ability types get found 
out, while high types differentiate themselves. Dispersion moves counter-cyclically.23
Charts 15 and 16 look at the same metrics for UK banks. We have also examined return patterns 
among the largest global banking and non-financial firms.24 The results are much the same. On 
average, the dispersion in banking returns is statistically significantly lower than for non-banks, at 
the 1% level, both for UK and global institutions. And, in general, we observe a compression of 
returns during credit booms and a dispersion in busts. There is evidence of an increase in the 
degree of coordination of global banks’ activities after the financial liberalisation of the 1980s. This 
suggests credit cycles may have become increasingly synchronous globally. 
 
22 There are, of course, alternative explanations. See, for example, Gomes, Kogan and Zhang (2003), whose model 
predicts counter cyclical dispersion driven by productivity shocks. Our explanation differs both in that (a) we relate 
dispersion to lower frequency fluctuations in fundamentals than the business cycle and (b) we stress imperfect 
information, and in particular the incentive to convey type, which is absent in their model.
23 Counter-cyclical variation in equity return volatility is well documented in the finance literature. There may be a link 
between this and our concern with counter-cyclical dispersion. Our model suggests that the desire to signal ability, 
rooted in imperfect information and short horizons, may be a cause of counter-cyclical dispersion in banking returns. 
The crises generated by excessive risk taking in our model would generate heightened volatility in a time series sense. 
Empirically, Schwert (1989) finds no single macroeconomic variable that can explain low frequency movements in 
equity price volatility. Further evidence is provided in Hamilton and Lin (1996), Perez Quiros and Timmerman (2001), 
Brandt and Kang (2004) and Belratti and Morana (2006). These econometric studies leave the structural causes of 
counter-cyclical volatility unexplained. Theoretical explanations have been put forward by, among others, Campbell 
and Cochrane (1999) and Mele (2007), who show that habits and cyclically asymmetric risk premia respectively may 
generate counter cyclical volatility in asset pricing models. Ours relies on excessive risk taking in a world of imperfect 
information.
24 This global group of banking institutions included UK and US banks as a subset.
23 
To test this formally, we construct pair-wise correlations between countries’ credit cycles for two 
post-war sub-samples, 1945-79 and 1980-2008. We plot the cumulative distributions of these 
cross-country correlations in Chart 17. The same technique can be applied to correlations between 
countries’ medium term fluctuations in GDP (Chart 18). In each chart, a shift to the right of the 
cumulative distribution indicates an increase in the degree of cross-country correlation. 
Chart 13: Dispersion of equity returns of US banks and top 100 US PNFCs (by market 
cap) 
(1), (2) and (3) represent medium term credit booms. Outside of these areas represent medium term credit busts. 
Source: Thomson Reuters Datastream and Bank calculations. 
0
1
2
3
4
5
6
7
8
9
10
73 78 83 88 93 98 03 08
Per cent
US top 100 PNFCs
US Banks
19 20
(1) (2) (3)
24 
Chart 14: Dispersion of ROE of US banks and US top 20 PNFCs (by market cap) 
Source: Thomson Reuters Datastream and Bank calculations. 
Chart 15: Dispersion of equity returns of major UK banks and top UK 100 PNFCs (by 
market cap) 
Source: CapitalIQ and Bank calculations. 
0
5
10
15
20
25
30
35
79 84 89 94 99 04 08
Per cent
US Banks
US PNFCs top 20
20
0
2
4
6
8
10
12
14
97 00 02 05 08 11
Per cent UK PNFCs
UK Banks
19 20
25 
Chart 16: Dispersion of ROE of top 10 UK banks and top 10 UK PNFCs (by market cap) 
Source: CapitalIQ and Bank calculations. 
Consistent with Bordo and Helbling (2003, 2010), there is evidence of increasing synchronicity in 
medium term GDP fluctuations across countries since 1980: a Wilcoxon rank-sum test rejects the 
null that the two samples are drawn from the same distribution at the 1% level and Jenrich’s test 
rejects the null that the correlation matrices are equal.25,26 This increased synchronisation can be 
explained by increasing trade and financial liberalisation (see Bordo and Helbling (op. cit.)). 
We observe an analogous pattern for credit cycles, with the correlation between countries’ credit 
cycles higher on average after 1980. The Wilcoxon test rejects the null of equality at the 5% level 
and Jenrich’s rejects its null at the 1% level. This is consistent with the notion that global banks’ 
activities have become increasingly alike, possibly as a result of increased competition and crossborder lending. The increase in the cross-country correlation of the credit cycle suggests policy 
needs an international dimension if it is to curb effectively the credit cycle, to which we turn next. 
 
25 Wilcoxon’s test pools the correlation matrices from the two sub samples and ranks individual country-pair 
observations by magnitude. If one set of observations features systematically higher cross-correlations, the rank of each 
of the observations will be higher on average in the pooled sample. Using the rankings of each correlation in the pooled 
sample, a normally distributed test statistic is constructed, under the null that the two samples are drawn from the same 
distribution.
26 Jennrich’s test for the equality of two correlation matrices derives a chi-squared distributed test statistic from 
transformations of the correlation matrices, including a term correcting for the bounded support ([-1,1]) of the 
difference between two correlations. The null is equality of the matrices.
0
5
10
15
20
25
30
02 03 04 05 07 08 09 10
UK Banks
UK PNFCs
Per cent
20
26 
Chart 17: Cumulative distribution function of cross-country correlations of credit cycles 
Source: Bank calculations 
Chart 18: Cumulative distribution function of cross-country correlations between medium 
term GDP cycles 
Source: Bank calculations 
0.0
0.2
0.4
0.6
0.8
1.0
1.2
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
Cumulative 
probability
Correlation coefficient
1945-1979 1980-2008
0.0
0.2
0.4
0.6
0.8
1.0
1.2
-1.0 -0.5 0.0 0.5 1.0 1.5
1945-1979 1980-2008
Cumulative 
Probability
Correlation coefficient
27 
5. Curbing the Credit Cycle
Taking together the evidence from Sections 2-4, what are the implications for public policy? 
First, according to the model a credit cycle arises from a collective action failure among banks. The 
lending decisions banks take, while individually rational, are collectively sub-optimal. Specifically, 
individual banks may fail to internalise the reputational externalities their lending actions impose on 
others. The result is a periodic tidal wave of credit during the boom followed by protracted credit 
drought during the crunch. Chuck Prince’s disco inferno causes murder on the dance floor. 
These credit cycle externalities provide a justification for state intervention to help co-ordinate 
lending expectations and actions by banks. At least in principle, these externalities suggest a role 
for the state in enforcing collective lending action, to curb the peaks and troughs in the credit cycle. 
The case for policy action may have grown over recent decades as competition in banking, and 
associated externalities, have intensified. 
Second, it has been suggested that one means of curbing credit cycle frictions is through monetary 
policy – either by ensuring it moderates appropriately the business cycle (Taylor (2010)) or, more 
ambitiously, by having it play a wider role in curtailing financial imbalances (Borio and White 
(2004)). The evidence presented here is not especially encouraging on that front. The frequency 
and amplitude of the business and credit cycles is quite different. Monetary policy may be an 
inefficient tool for calming the credit cycle, if at the same time it is to moderate the business cycle. 
Recent history offers a good case study. Between 2000 and 2007, UK nominal GDP growth 
exhibited no signs of exuberance, with GDP growth at trend and inflation at target. Over the same 
period, UK banks’ balance sheets trebled. Using monetary policy to tame credit growth over this 
period would have come at the expense of a destabilisation of non-financial activity. Activist 
monetary policy would have resulted in instability migrating from the financial to the non-financial 
sector. In tackling the credit cycle, monetary policy may have, in Irving Fisher’s language, 
“aggravated rather than neutralised” the business cycle. 
Econometric evidence tends to support this view. Model-based simulations by the IMF suggest the 
need for two instruments to tackle efficiently real as well as financial imbalances (IMF (2009)). 
And the empirical evidence in Bean et al (2010) suggests that monetary policy may be a rather 
28 
ineffective instrument quantitatively in constraining credit growth. This suggests that assigning 
monetary policy the task of tackling financial imbalances may be inefficient, perhaps ineffective. 
Charts 19 and 20 plot credit cycles in the UK and US from 1880, together with the different 
monetary policy regimes which have operated over this period. Strikingly, credit cycles dynamics 
appear to be largely invariant to the monetary policy regime – fixed or floating, rules or discretion, 
lax or tight. This, too, is indicative evidence that monetary policy may not be the most effective 
tool for moderating credit fluctuations. 
Chart 19: UK Credit cycle across Monetary Regimes 
(1) Gold Standard (6) Bretton Woods; Sterling full external convertibility 
(2) Inter-war suspension (7) Monetary Targeting 
(3) Resumption of Gold Standard (8) Exchange Rate Mechanism 
(4) Sterling Area (9) Inflation Targeting 
(5) Bank of England nationalised; Bretton Woods (10) Bank of England independence 
Source: Bank calculations 
40
30
20
10
0
10
20
30
1870 1890 1910 1930 1950 1970 1990 2010
Per cent
(4)
-
+
(1) (2) (3) (5) (6) (7) (8) (9) (10)
29 
Chart 20: US Credit cycle across Monetary Regimes 
(1) Gold Standard (6) Treasury-Federal Reserve Accord (1951) 
(2) Inter-war suspension (7) End convertibility into gold (“Nixon Shock”) (1971) 
(3) Resumption of Gold Standard (8) Volker era 
(4) FDIC established; Federal Reserve reorganisation (9) Greenspan era 
(5) Bretton Woods (10) Bernanke era 
Source: Bank calculations 
Third, micro-prudential policy aimed at tackling financial imbalances in individual financial 
institutions may also be ineffective for dealing with aggregate credit cycles. That is because bankspecific actions will not, by themselves, internalise the spillovers that arise across banks over the 
credit cycle. At best, this means that micro-prudential actions may be impotent in curbing the credit 
cycle. At worst, however, it could mean that bank-specific prudential actions are counterproductive. They might actually boost risk-taking incentives among banks which are not subject to 
prudential intervention. 
To see that, consider a two-bank variant of the model described by the payoffs in Table 4, in which 
ߠ now references the market’s prior over the macro state, which it updates having observed a 
bank’s earnings. When ሺܿെܽሻሺ1 െ γሻ/γ is neither too big nor too small, there are two symmetric 
Nash equilibrium strategies in this game: {Risky, Risky} and {Tight, Tight}. When Bank 1 sets 
{Risky}, Bank 2 does not want to set {Tight} and declare negative earnings as this is more likely to 
signal low ability, provided the long term costs ܿ are not too big. On the other hand, if Bank 1 sets 
20
15
10
5
0
5
10
15
20
25
30
1870 1890 1910 1930 1950 1970 1990 2010
Per cent
-
+
(1) (2) (3) (9) (10) (4) (5) (6) (7) (8)
30 
{Tight}, Bank 2 sets tight too if long term costs ܿ are not too small. This results in a classic 
coordination game with multiple equilibria. 
Table 4: Payoffs in a Two-Bank Game
Bank 2 
Risky Tight 
Bank 1 
Risky ݑ௥ሺߠ ,1ሻ, ݑ௥ሺߠ ,1ሻ ݑ ௥൬ߠ ,1
2
൰,ݑ௧ ൬ߠ ,1
2
൰
Tight ݑ௧ ൬ߠ ,1
2
൰,ݑ ௥൬ߠ ,1
2
൰ ݑ௧ሺߠ ,0ሻ, ݑ௧ሺߠ ,0ሻ
Now suppose the regulator forces Bank 2 to set tight policies whenever it realises low returns. So if 
Bank 2 announces positive earnings, it must be because the macro state is good. But given this, 
Bank 1 does not want to signal low ability by playing {Tight} when it makes a bad loan since doing 
so would be taken by the market to imply low ability. If the spillover from Bank 2’s actions to the 
market’s assessment of the macro state is strong enough, {Tight, Tight} may no longer be an 
equilibrium. Instead, Bank 1 may play Risky. The resulting equilibrium is {Risky, Tight}. In this 
way, bank-specific intervention may have perverse consequences for risk-taking. 
Fourth, this co-ordination problem suggests systematic, across-the-system actions are needed to 
curtail effectively credit booms and busts. This is one dimension of macro-prudential policy. To be 
effective, these policies need to increase the long-term cost of credit extension to banks during 
booms and, as importantly, to lower these costs during busts. These actions would help smooth out 
credit supply over the cycle. There are a variety of macro-prudential tools which could have this 
effect, including pro-cyclical capital and liquidity requirements, or remuneration packages that tie 
individual earnings more closely to long term performance (Bank of England (2009), Kashyap et al 
(2010), G30 (2010)). 
Chart 21 illustrates the impact of increasing the long term cost of short term risky strategies, ܿ,
across the entire system in our simple model. The ‘low ܿ’ and frictionless paths are identical to 
Chart 2. But if the regulator raises the cost of risky policies, the signalling threshold rises to the 
green dashed line. In this example, the increase in this threshold is sufficient to prevent risky 
policies arising at any point over the horizon. In other words, the ‘high ܿ’ (macro-prudential) path 
31 
replicates the frictionless equilibrium. The large output swings caused by risky lending cycles are 
reduced significantly. The financial accelerator is defused. 
Chart 21: Model simulation high long term cost of short term risky strategies 
 
Source: Bank calculations 
Fifth, because credit cycles emerge from a failure to co-ordinate lending decisions, expectations are 
crucial for the effectiveness of macro-prudential policies. Perhaps even more than in a monetary 
policy context, macro-prudential policy works by acting on agents’ expectations. For example, 
raising ܿ has a direct effect on lenders’ behaviour, increased incentives to reduce risk. But as 
importantly, it also has an indirect expectational effect, as all lenders anticipate they will become 
subject to the same simultaneous squeeze. Anticipating that, lenders will co-ordinate their lending 
choices today provided policy is credible. 
These collective action dynamics underscore the importance of the expectations channel for macroprudential policy. This has important implications for the design of a macro-prudential framework. 
Without absolute clarity about the objectives of any macro-prudential policy framework and the 
policy rule necessary to deliver these objectives, expectations will not adjust and policy will be 
impotent. Any lack of transparency or failure of communications is likely to inhibit seriously the 
effectiveness of macro-prudential policy. 
20
0
20
40
60
80
100
120
0 20 40 60 80 100
θ θ (low c) θ (high c) = frictionless θ*
t
-
+
32 
Sixth, because macro-prudential policy is new in most developed countries, there are 
understandable concerns that policymakers’ knowledge of the transmission mechanism of policy is 
incomplete and imperfect. The model of the credit cycle developed here offers some grounds for 
optimism. Signalling is the key transmission channel. Quantitative estimates of the effects of, for 
example, higher capital ratios on banks’ cost of credit provision have wide confidence intervals 
(BIS (2010)). But this may not be fatal if macro-prudential policy by-passes these channels and 
works by acting, first and foremost, through expectations. 
Seventh, the credit cycle is increasingly an international phenomenon, as well as a national one. 
Credit spillovers occur across borders as well as across banks. This suggests macro-prudential 
policies need also to have an international dimension if they are to tackle credit externalities. This 
is recognised in the macro-prudential policy framework currently being discussed by the 
international regulatory community (BIS (2010)). This framework includes an explicit reciprocity 
provision. For example, judgements on local credit conditions determine the amounts of capital to 
be held by international banks on their exposures in those countries. This reciprocity feature should 
help to reduce the arbitrage risks posed by the internationalisation of the credit cycle. 
Eighth, a second potential source of regulatory arbitrage is the shadow banking system. Even ahead 
of the present crisis, this had grown to a scale potentially in excess of the conventional banking 
system (Adrian et al (2010)). Operating macro-prudential policy on a sub-set of credit providers, 
ignoring the shadow banks, suffers the same problems as micro-prudential policy operating on a 
sub-set of banks. It risks not only being ineffective, but also providing incentives for risk to migrate 
to the unregulated sector. This underlines the importance of policing the regulatory boundary and 
moving this boundary if credit provision risks crossing the border in the course of setting macroprudential policy. 
Ninth, especially at the outset, uncertainties about the role and efficacy of macro-prudential policy 
will be considerable. Simplicity and humility will be needed. Simplicity to prevent confusion 
about the objectives and transmission channels for macro-prudential policy, given the importance of 
signalling. Humility to reduce the chances of banks over-relying on public policy signals about 
credit provision at a time when these are sure to be noisy (see Morris and Shin (2002)). As with the 
business cycle and monetary policy, macro-prudential policy cannot be expected to eliminate the 
credit cycle. This is neither feasible nor desirable. And simple rules, augmented by judgement, 
33 
offer the best chance of ensuring robust decision-making at the start of a new macro-prudential 
policy regime (Taylor and Williams (2010)). 
Tenth, the state of macro-prudential policy today has many similarities with the state of monetary 
policy just after the second world war. Data is incomplete, theory patchy, policy experience 
negligible. Monetary policy then was conducted by trial and error. The same will be true of macroprudential policy now. Mistakes will be made. But as experience with the other arms of 
macroeconomic policy has taught us, the biggest mistake would be not to try.

Global imbalances are colossal. At their high-water mark in 2006, imbalances of the G7 economies totalled 
around 5% of GDP. Like Colossus, global imbalances straddle many of today’s most important global public 
policy issues. These include the architecture of the international monetary system, the future of the 
international trading system and the design of the international financial system. These are high stakes.
This paper assembles some facts from the past on global imbalances. More speculatively, it also assesses 
some of the factors shaping the course of imbalances in the future. Those medium-term forces suggest that 
imbalances may get worse before they get better. If so, this global financial fault-line could cause further 
tremors to the international monetary, trading and financial system in the period ahead, the like of which we 
are currently experiencing.
Global Imbalances – Past
Global imbalances are a natural by-product of free trade in goods, services and capital. In others words, 
capital flows are a necessary ingredient of trade and capital liberalisation. In that sense, they are good 
cholesterol. But capital flows may also demonstrate an imbalance between demand and output in an 
economy, which must eventually correct if debt and wealth stocks are not to become unsustainable. Sharp 
corrections in rates of domestic absorption and/or capital flows could then result, with attendant output costs. 
In that sense, global imbalances may also be bad cholesterol.1
So which are capital flows today? Historical evidence is illuminating. Chart 1 plots (the absolute value of) 
current account balances in thirteen countries, as a % of GDP, since 1880. Other than in wartime, global 
imbalances are at their highest in well over a century. They have surpassed levels which prevailed during 
the classical Gold Standard and are more than twice levels during the Bretton Woods period (Chart 2). For 
some individual countries, these capital flows look larger still, both absolutely and relative to historical norms 
(Chart 3).
It is not just the size of these global flows that is unusual. So too is their direction. Contrary to theory, capital 
is flowing from developing countries with a low capital stock, towards developed countries with a high capital 
stock (Lucas (1990)). In other words, capital is flowing “uphill”, away from countries where the marginal 
product of capital should be high and towards countries where it should be low. This makes the pattern of 
global capital flows doubly perplexing.
 
1
 Blanchard and Milesi-Ferretti (2009) discuss the causes of “good” and “bad” imbalances.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
3
3
Chart 1: Current account balances Chart 2: Average current account balances
0
1
2
3
4
5
6
1870 1887 1904 1921 1938 1955 1972 1989 2006
Per cent of GDP
0.0
0.5
1.0
1.5
2.0
2.5
Gold 
Standard 
1870-1913
Interwar 
1919-1939
Bretton 
Woods 
1944-1971
Post BW 
1972-2009
Per cent of GDP
Chart 3: Current account balances 
for individual countries
Chart 4: Gross and net capital flows
2 0
1 5
10
5
0
5
10
1 5
2 0
8 0 82 8 4 8 6 88 9 0 9 2 94 9 6 9 8 00 0 2 0 4 06 0 8
Total current account surpluses (a)
Total current account deficits (b)
Gross capital outflows (c) 
Gross capital inflows (d) 
BIS bank gross outflows
BIS bank gross inflows
Per cent of world GDP
-
+
Sources: IMF WEO, Taylor (2002) and Bank calculations.
(a) 5-year moving average. Data prior to 1980 are from Taylor 
(2002). Data post 1980 are from the IMF WEO.
Sources: Bank for International Settlements, IMF WEO and 
Bank calculations.
(a) Sum of global current account surpluses.
(b) Sum of global current account deficits.
(c) Sum of global net purchases of foreign assets by 
residents.
(d) Sum of global net purchases of domestic assets by 
foreigners.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
4
4
Perplexing need not mean bad. Current account imbalances are no more than the arithmetic difference 
between gross capital inflows and outflows. As these gross flows increase as a result of financial 
liberalisation, it is natural to expect the difference between them to become more volatile (Obstfeld (2010)). 
As Chart 4 shows, the growth of gross capital flows in the years preceding the crisis was dramatic. At the 
start of the 1980s, gross capital flows were around 5% of world GDP. By 2007, they had risen to 7 times 
that. 
This growth in capital flows and imbalances has been mirrored in the path of the external balance sheets of 
the major economies. These, too, have adjusted rapidly. In 1985, the US was a net external creditor. By 
2009, it had become a net external debtor to the tune of around 20% of GDP. In 1999, China was a net 
external debtor. By 2008, it had become a net external creditor of around one third of GDP. 
An interesting perspective on the drivers of imbalances is provided by looking at the gross saving and 
investment behaviour of surplus and deficit countries over this period. Chart 5 looks at these patterns. It 
suggests a couple of striking patterns.
First, the correlation between saving and investment rates appears to have weakened significantly over the 
past two decades, for both surplus and deficit countries. This is consistent with increasing capital mobility. 
Historically, there has been a high correlation between national saving and investment rates – the FeldsteinHorioka (1980) puzzle. Chart 6 plots the Felstein-Horioka correlation coefficient alongside an index of 
financial liberalisation from Obstfeld and Taylor (2004). 
Between 1930 and 1980, the Feldstein-Horioka coefficient was close to one – a puzzlingly high comovement between saving and investment. But that pattern has shifted significantly over the past 30 years. 
Saving and investment correlations are no longer puzzlingly strong. Indeed, just ahead of crisis, 
saving/investment correlations had fallen to around zero. They are presently at their lowest levels in almost 
a century. On the face of it, this might point toward a rather benign interpretation of rising global imbalances: 
they may simply be the mirror image of rising capital liberalisation. This would be a good cholesterol story. 
Even then, however, there may be risks associated with intermediating large global flows of funds. Gross 
flows can matter. As experience during 2008 illustrated (Chart 4), sharp reversals of these gross flows can 
disrupt the functioning of financial systems, in particular banking systems reliant on overseas funding. Large 
inflows may also cause indigestion problems for small domestic capital markets. The strong upward 
pressures on asset prices in a number of emerging market economies over the past few months are 
testament to that.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
5
5
Chart 5: Saving and investment Chart 6: Capital mobility and FeldsteinHorioka coefficient
0
5
1 0
1 5
20
25
3 0
3 5
1980 1984 1988 1992 1996 2000 2004 2008
Surplus country saving
Deficit country saving
Surplus country investment
Deficit country investment
Per cent of GDP
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 -0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1880 1900 1920 1940 1960 1980 2000
Feldstein-Horioka coefficient (RHS) (a)
Capital mobility index (LHS) (b)
High
Low
Index
Sources: IMF WEO and Bank calculations.
(a) Surplus and deficit countries defined based on their 
average current account balances between 2003 and 2007.
Sources: Taylor (2002), IMF WEO, Obstfeld and Taylor 
(2004) and Bank calculations.
(a) The Feldstein-Horioka coefficient is the correlation 
coefficient between savings and investment for 15 countries 
(the sample varies slightly over the period).
(b) Obstfeld and Taylor’s capital mobility index is 
judgemental and takes values between 0 and 1.
As Chart 5 also shows, most of the rise in global flows over the past decade is accounted for by changes in 
the saving rates of deficit and surplus countries, rather than in their investment rates. In other words, 
differential savings behaviour has been the prime mover behind rising global imbalances. It is unsurprising, 
then, that savings behaviour lies at the heart of the current debate among policymakers and academics on 
the causes of imbalances. 
This, too, need not be bad news. For example, differential savings may simply be mirroring fundamentals, 
reflecting differences in rates of time preference among consumers or differential rates of technological 
progress among companies. Equally, it is possible to conceive of explanations for differential saving which 
are less well-rooted in fundamentals, such as lax monetary policy in the West or mercantilist trade policy in 
the East.2
 These latter explanations take us closer to a bad cholesterol story for global imbalances. 
This is where much of the current debate on global imbalances is centred. Are they the result of surplus 
countries saving excessively – a “savings glut” explanation (Bernanke (2005))? Or is it instead the deficit 
 
2
 This can lead to a global co-ordination (or adding-up) problem, as countries seek individually to each run a current account surplus, 
so depressing global demand (King (2010)).
All speeches are available online at www.bankofengland.co.uk/publication/speeches
6
6
countries saving too little – a “savings shortage” explanation (Taylor (2007)). Put differently, is the problem 
impatience in the West or excessive patience in the East? 
Global Imbalances – Present
To begin to answer those questions, consider recent saving behaviour in two archetypical surplus and deficit 
countries – China and the US. Chart 7 looks at the respective contributions of China and the US to global 
savings since 1980. In 1980, the US contributed around a quarter of global saving. Today, it contributes 
around a tenth. China has been the mirror image. Having contributed less than 5% in 1980, today China is 
the single largest source of global saving, contributing around one fifth.
Chart 7: China and US share of world saving
0
5
1 0
1 5
2 0
2 5
3 0
8 0 8 3 8 6 8 9 9 2 9 5 9 8 0 1 0 4 0 7
China U S
Per
cent
 Source: IMF WEO and Bank calculations.
It has been argued by some that these trends could simply reflect different cultural attitudes towards saving 
among US and Chinese citizens. In the US, it is often said that there is a culture of spending rather than 
saving – neurologically, the impatience gene may be dominant (Haldane (2010)). In China, by contrast, the 
culture may be biased towards saving rather than spending, with the patience gene dominant. In other 
words, differences in agents’ rate of time preference, or degree of patience, might account for savings 
imbalances (Buiter (1981)).
But a recent survey of time preferences in 45 countries suggests cultural differences are not the full story 
(Wang, Rieger and Hens (2010)). Asked to choose between a payoff this month and a larger payoff next 
month, 68% of US students choose to wait; they exhibit patience. Facing the same trade-off, 62% of 
Chinese students made the same choice. These differences are small.3
 They are also the “wrong way 
around” to explain imbalances. 
 
3
 Among the 45 countries, Germany ranked as the most patient (almost 90% of students waited), Nigeria the least patient (less than 
10%).
All speeches are available online at www.bankofengland.co.uk/publication/speeches
7
7
So if cultural factors are not the explanation, what is? Charts 8 and 9 plot national saving rates in China and 
the US, broken down on a sectoral basis. There are striking differences in saving behaviour across the 
government, household and corporate sectors in the two countries.
Around two-thirds of the rise in Chinese savings since the early 1990s derives from the corporate sector.4
 
This reflects two things. First, rapidly rising corporate profitability, against a backdrop of strong growth and 
rising productivity (Ma and Yi (2010)). Since 2000, profits among Chinese industrial companies have risen 
around 30% per year. 
Second, more significantly still, most of these profits have been retained within Chinese companies rather 
than distributed to shareholders (Qiao and Song (2009)). These differences in US/Chinese corporate 
savings behaviour show up dramatically in dividend payouts. The average dividend payout ratio among US 
corporations in 2009 was 40%, with less than a quarter of companies failing to pay a dividend. The average 
dividend payout ratio among listed Chinese corporations in 2009 was around 18%. Among these, more than 
half paid no dividend whatsoever.
Charts 10 and 11 consider the distribution of changes in dividend payout ratios among global and Chinese 
companies. Among global companies, payout ratios rarely fall. More than half of the time they increase. 
Impatient investors demand immediate dividend gratification, leading to large and rising distributions. This 
dividend ratchet is a puzzle in empirical finance (Black (1976)). If anything, it is one which is growing over 
time.
The payout pattern among Chinese quoted firms could not be more different. Increases and decreases in 
payout ratios are roughly evenly split. The high proportion of “unchanged” payout ratios simply reflects the 
zero payout policy of more than half of quoted Chinese companies. There is no distribution ratchet. Profits 
are largely ploughed back. In this respect, Chinese payout behaviour today is not dissimilar to that among 
US companies during its rapid growth phase in the 19th century (Chart 12).
 
4
 Using firm-level data, Bayoumi, Tong and Wei (2010) argue that Chinese listed firms’ corporate savings rates are neither especially 
high compared to listed firms in other countries and the gross savings rate for a typical listed Chinese firm has declined, albeit 
insignificantly, between 2002-2007.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
8
8
Chart 8: China gross national saving Chart 9: US gross national saving
0
1 0
20
3 0
4 0
50
6 0
1992 1995 1998 2001 2004 2007
Government Corporate
Household
Per cent of GDP
-1 0
-5
0
5
1 0
1 5
2 0
2 5
1992 1995 1998 2001 2004 2007
Government Corporate
Household Total
Per cent of GDP
Source: People’s Bank of China. Sources : Bureau of Economic Analysis and Bank calculations
(a) Total excludes statistical discrepancy
Chart 10: Dividend payouts: global companies Chart 11: Dividend payout ratios: Chinese 
companies
0
1 0
2 0
3 0
4 0
5 0
6 0
7 0
8 0
9 0
100
1981 1985 1989 1993 1997 2001 2005 2009
Per cent of firms Increase No Change Decrease
0
10
20
30
40
50
60
70
80
90
100
1997 1999 2001 2003 2005 2007 2009
Per cent of firms Increase No change Decrease
Source: Datastream and Bank calculations.
(a) The chart shows year on year changes in dividend payments 
at 215 of the largest firms on the FTSE, S&P500, Topix, CAC 
and DAX indexes.
Sources: Worldscope, Bank calculations.
(a) The chart shows year on year changes in dividend payout ratios 
for a rolling sample of listed Chinese firms. The sample covers 
firms in the Worldscope database as of 8 November, 2010.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
9
9
Chart 12: Dividend payout ratios: US firms in 
19th century
Chart 13: US household net worth and financial 
balance
0
1 0
2 0
3 0
4 0
5 0
6 0
7 0
8 0
9 0
100
1825 1830 1835 1840 1845 1850 1855 1860 1865
Per cent of firms Increase No Change Decrease
3.0
3.5
4.0
4.5
5.0
5.5
6.0
6.5
-6 7.0
-4
-2
0
2
4
6
8
10
1960 1970 1980 1990 2000
Financial balance (LHS)
Net worth (RHS; inverted)
Per cent of 
personal 
disposable 
income
Multiple of
Personal
disposable 
income
Source: Yale School of Management, reprinted in De Angelo, 
De Angelo and Skinner (2009).
(a) The chart shows year on year changes in dividend 
payments at a sample of 515 NYSE listed firms
Sources: Bureau of Economic Analysis; Federal Reserve and Bank 
calculations
(a) Includes non-profit organisations
There is more than one explanation for this behaviour among Chinese firms. A benign interpretation is that it 
reflects the high option value of Chinese companies retaining funds given future growth prospects, as with 
US firms in the 19th century. A less benign interpretation is that it reflects an absence of external sources of 
company finance and shortfalls in corporate governance, with insufficient discipline on companies by outside 
investors. In practice, both factors are likely to have been important over recent years. 
Turning to US and Chinese households, during this century household gross saving as a fraction of GDP has 
averaged less than 5% in the US. In China, it has averaged around 20%.
In China, the constancy in high household saving rates masks two striking, but broadly offsetting, factors: a 
10 percentage point fall in household’s income share; and a 10 percentage point rise in household’s saving 
propensity (Ma and Yi (2010)). Much of the rise in saving propensities appears to be precautionary. Facing 
“three mountains” in the future - education, pensions and health care – Chinese consumers have taken the
high (saving) road. 
In the US, the fall in the household financial balance correlates strongly with the path of household net worth 
(McKelvey (2010), Chart 13). This correlation emerged strongly during the 1980s, with the simultaneous rise 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
10
10
in net wealth and fall in saving. This correlation has remained strong as net worth has reversed recently due 
to falling house and equity prices. 
What lies behind these movements? Lax monetary policy has been suggested by some (Taylor (2007)). 
Financial liberalisation is a second plausible candidate. This had the effect of loosening constraints on the 
supply of housing credit and thereby boosting housing demand and prices in the US. It also enabled 
households to mobilise and withdraw the accrued equity in their property to finance spending. Estimates of 
housing equity withdrawal for spending rose from under 2% of personal disposable income in 2000 to around 
4% at the peak in 2005 (Greenspan and Kennedy (2007), Chart 14). 
Micro-level data support this story. They show that home equity withdrawal was driven primarily by 
households with low credit scores and high credit card utilisation rates, who are most likely to have been 
credit-constrained in the past (Mian and Sufi (2009)). As these constraints loosened, spending rose and 
saving fell. Liberalisation fed impatience. 
These patterns are also consistent with an inequality-based explanation for lower saving (Rajan (2010)). 
Rising inequality may generate an increased desire to “keep up with the Jones’s”. That, in turn, may have 
led to higher borrowing, and lower net saving, by the poor. 
And what applies within countries appears also to apply across them. Among advanced countries, there is a 
significant negative relationship between within-country measures of inequality and the current account 
position (Chart 15). Rising inequality, by lowering savings rates among the poor, appears to be deficitinducing. Rising imbalances may have important social, as well as economic, roots.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
11
11
Chart 14: US housing equity withdrawal Chart 15: Inequality and current account 
balances in advanced economies
-1
0
1
2
3
4
5
6
7
1991 1994 1997 2000 2003 2006
'Active housing equity withdrawal (a)'
Extracted equity used for spending
Per cent of 
personal 
disposable 
income y = -0.0049x + 0.3011
R² = 0.491
0.2
0.25
0.3
0.35
0.4
-10 0 10 20
Current account balance (% of GDP, 2005) Gini coefficient, mid 2000s
Sources: Bureau of Economic Analysis, Greenspan and Kennedy 
(2007), and Bank calculations
(a) Defined by Greenspan and Kennedy (2007) as the change in home 
equity mortgage debt plus gross cash out.
Sources: OECD, IMF WEO and Bank calculations.
Note: Chart includes advanced economies (IMF definition) 
excluding major oil exporters and reserve accumulators.
Global Imbalances – Future
So what will be the key forces driving global imbalances in the period ahead? Judging from the recent past, 
two factors have been important: 
Global financial integration;
Differential savings behaviour.
Consider in turn their likely evolution.
Global financial integration. In the period ahead, this will be shaped by a number of medium-term structural 
factors. One key such factor is cross-country GDP convergence. Over time, we would expect countries’ 
income levels to converge, as currently low income countries catch-up with their high income trading 
neighbours (Solow (1956)). This convergence process is a well-established empirical phenomenon (Barro 
(1991) and Mankiw, Romer and Weil (1992)). 
Accompanying this GDP convergence is likely to be catch-up in gross external balance sheet positions. As 
countries develop, so too does their integration into global capital markets, with higher levels of gross 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
12
12
external assets and liabilities. Cross-country evidence bears this out. There is a well-defined empirical 
relationship between a country’s GDP per head and its gross external balance sheet position (Chart 16). 
Chart 16: Per capita GDP and gross external 
balance sheets, 2007
Chart 17: Ratio of external balance sheet to 
GDP
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
8.00
4.0 6.0 8.0 10.0 12.0
Gross external balance sheet, multiple of GDP
Ln (per capita GDP)
-
0.50 
1.00 
1.50 
2.00 
2.50 
3.00 
3.50 
1980
1996
2012
2028
2044
China India Brazil
G7 Russia
Source: Updated and extended version of the External Wealth of 
Nations Mark II database developed by Lane and Milesi-Ferretti 
(2007), US Census Bureau and Bank calculations. 
(a) Includes all countries in the Lane and Milesi-Ferretti database, 
except Luxembourg, Bahrain, Hong Kong , Ireland, Kiribati and 
Timor-Leste.
(b) External balance sheet is defined as the average of external 
assets and liabilities.
Sources: Updated and extended version of the External 
Wealth of Nations Mark II database developed by Lane and 
Milesi-Ferretti (2007), IMF, US Census Bureau, Penn World 
Table and Bank calculations. 
(a) Measured as the average of external assets and external 
liabilities. 
Taken together, these two factors imply a pattern of steadily rising gross capital flows over time, with growth 
strongest among countries catching up with their high-income trading neighbours. It is possible to calibrate 
these integration trends and project them forward to simulate future external balance sheets positions among 
the major economies. 
These simulations take as initial conditions cross-country GDP per capita, projected forward using a 
calibrated rate of growth convergence drawn from previous empirical studies and adjusted for known future 
changes in population growth. This gives a set of cross-country projections for GDP per capita. These are 
then translated into gross external balance sheet positions using historical relationships with per capita GDP. 
Chart 17 plots some projections of (the average of) gross external assets and liabilities relative to GDP for 
the G7 countries, China, India, Brazil and Russia. These projections go out to 2050. Among the G7 
economies, external balance sheet ratios are broadly flat at around twice nominal GDP. This flatness is 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
13
13
driven in large part by assumption, as it is assumed the highest-income countries maintain a flat ratio of 
external assets to GDP. 
This is almost certainly too extreme an assumption. Although smaller than in the past, there remain strong 
home biases in the investment portfolios of developed countries’ institutional investors (Obstfeld and Rogoff 
(2000) and IMF (2010)). As liberalisation progresses, these biases would be expected to unwind, boosting 
gross external assets in relation to GDP in developed countries. For that reason, the calibration here can be 
thought of as a limiting case of the likely adjustments in the pattern of global capital flows.
Subject to that important caveat, the growth in the non-G7 economies’ contribution to global capital markets 
is still striking. Among the BRICs, external balance sheets as a fraction of GDP exceed G7 levels by around 
2035. By 2050, they are around 2.5 times nominal GDP. These patterns are even more striking when 
expressed as a share of global external assets. By 2050, over half of all G20 external assets are associated 
with the BRICs, up from around 9% currently (Chart 18). Among the non-G7, non-BRIC G20 countries, their 
share trebles from 5% to around 15%. 
Within this, some countries’ share of global assets sky-rocket. China’s share of global finance rises to 
around 30% by 2050, roughly that of the entire G7. India rises to almost 20%, from less than 0.5% currently 
(Chart 19). The US share falls from 28% to around 12%.
The flow counterpart to these external stock positions is no less dramatic (Chart 20). By 2020, gross capital 
flows of the non-G7 countries are projected to exceed those of the G7 economies. By 2030, non-G7 flows 
are more than three times G7 flows. And by 2040, they are almost four times G7 flows.
Even if these calibrations represent an extreme case, they are indicative of a striking power shift in the 
pattern of global financial flows. If this path were to be even broadly followed, it would have implications for 
the scale of global imbalances, which will tend to rise as gross capital flows outpace GDP growth. It would 
have implications for financial stability, as the scale of gross capital surges (fuelling bubbles) and reversals 
(fuelling crises) increases. And it may also have implications for the dollar’s reserve currency status.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
14
14
Chart 18: Share of G20 total external assets, by 
region 
Chart 19: Share of G20 total external assets, 
by country
0 %
20%
40%
60%
80%
100%
1980
1996
2012
2028
2044
Other G20 BRICS G7
0 %
5 %
10%
15%
20%
25%
30%
35%
40%
45%
1980 1996 2012 2028 2044
UK USA China
India Brazil Russia
France Germany
Sources: IMF, US Census Bureau, Penn World Table and Bank 
calculations.
Sources: IMF, US Census Bureau, Penn World Table and 
Bank calculations.
Chart 20: Ratio of non-G7 capital outflows to 
G7 capital outflows
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
2010 2020 2030 2040 2050
Source: Updated and extended version of the External Wealth 
of Nations Mark II database developed by Lane and MilesiFerretti (2007), IMF, US Census Bureau, Penn World Tables 
and Bank calculations.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
15
15
Differential savings behaviour. Consider next the possible future path of savings behaviour across the 
world’s major economies. There are likely to be many factors affecting cross-country saving rates in the 
period ahead. Predicting these is fraught with uncertainty. But one key medium-term determinant of saving 
is demographics (Wilson and Ahmed (2010)). And demographic trends are something about which there is 
little uncertainty in the period ahead.
Chart 21 looks at projected changes in the fraction of adults in the age range 40 to 59 in developed and 
emerging countries – the “prime savings” cohort. It suggests a sharp bifurcation in demographic trends in 
developed and emerging economies. Among advanced countries, an ageing population results in a decline 
in the middle-aged share, from a peak today of almost 30% to around 25% by 2050. For developing 
economies the reverse is true, with a continuing rise in the fraction of middle-aged individuals from 20% 
today to around 25% by 2050. 
Cross-country correlations suggest a relationship between this prime-saving cohort and saving behaviour. 
Chart 22 suggest that every 1 percentage point rise in the middle-aged population share raises the national 
savings rate by around 1.75 percentage points.5
 Taken together with emerging population trends, this 
suggests some potentially important life-cycle pressures on the global pattern of savings in the period ahead.
Chart 21: Share of population aged 40-59 Chart 22: Savings rate and population
0.15
0.17
0.19
0.21
0.23
0.25
0.27
0.29
0.31
2000
2005
2010
2015
2020
2025
2030
2035
2040
2045
2050
Less developed countries
More developed countries
World
y = 1.7569x - 0.288
R² = 0.2831
-
0.05 
0.10 
0.15 
0.20 
0.25 
0.30 
0.35 
0.40 
0.45 
0.50 
0.2 0.25 0.3 0.35 0.4
National saving rate, 2010
40-59 share
Sources: US Census Bureau and Bank calculations. Sources: IMF, UN and Bank calculations.
(a) National saving rates for advanced economies 
To bring these pressures to life, Chart 23 projects saving rates in a selection of countries out to 2050 based 
on population and GDP projections. This is plainly a gross over-simplification. These projections take no 
account of a whole host of other factors that may push in the other direction, including later retirement dates 
 
5
 See also the evidence in Wilson and Ahmed (2010).
All speeches are available online at www.bankofengland.co.uk/publication/speeches
16
16
which may boost saving in countries with an ageing population, and falls in saving rates in emerging 
countries as the social safety net is widened. 
But based on demographic and convergence trends alone, saving rates in the BRICS, already high, are 
projected to continue their ascent. For example, in India they lead to an increase in saving of around 10 
percentage points of GDP, raising the saving ratio to 50% by 2050, as demographics push a larger fraction 
of the population into the high savings cohort. Among developed countries, meanwhile, these trends are 
reversed with further demography-induced falls in savings rates.
Chart 23: Saving rate by country(a) Chart 24: Share of total G20 savings(a)
savings
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1980
1996
2012
2028
2044
USA UK China
Germany Brazil India
0
5
1 0
1 5
2 0
2 5
3 0
3 5
4 0
4 5
1980
1996
2012
2028
2044
US China
Per cent
Sources: IMF, US Census Bureau, UN, Penn World Table 
and Bank calculations.
(a) National saving rate. 
Sources: IMF, US Census Bureau, UN, Penn World Table 
and Bank calculations.
(a) Russia is not included prior to 1995 due to data 
availability. 
Given the rising income share of the BRIC countries, these patterns are even more dramatic when we 
consider country shares of future global savings. China rises to represent around 40% of global savings by 
2050. Over the same period, the US share of global savings continues its descent, stabilising at around 5%.
On the face of it, these partial equilibrium projections of future savings rates suggest that medium-term 
pressures on global imbalances could intensify, with saving rates rising among surplus countries and falling 
in deficit countries. To bring this intuition to life, assume for simplicity’s sake that investment rates as a 
fraction of GDP remain fixed at 2010 levels across countries, but that saving rates evolve as in Chart 23. 
The result would be a trebling of global imbalances from current elevated levels over the next twenty years.
Of course, these projections only look at the marginal impact on imbalances of medium-term demographic 
and GDP trends. They take no account of a host of other factors that may defuse pressures on imbalances, 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
17
17
including adjustments in real exchange rates, lower investment ratios as the marginal product of capital 
reduces in emerging markets and falls in saving rates in emerging markets as the social safety net is 
widened and deepened. These offsetting factors will need to be very significant, however, if they are to 
counterbalance medium-term upward pressures on global imbalances from demographics and convergence.
Conclusion
So where does this leave us? With a potentially dramatic change in the future international financial 
landscape. With a potentially decisive shift in the pattern of global capital flows. With potential pressures for 
a further widening in global imbalances. And, if so, with a likely intensification of pressures on international 
monetary, financial and trading systems. The impetus to reform these global systems is strong today. It may 
be stronger still tomorrow.

Global imbalances are colossal. At their high-water mark in 2006, imbalances of the G7 economies totalled 
around 5% of GDP. Like Colossus, global imbalances straddle many of today’s most important global public 
policy issues. These include the architecture of the international monetary system, the future of the 
international trading system and the design of the international financial system. These are high stakes.
This paper assembles some facts from the past on global imbalances. More speculatively, it also assesses 
some of the factors shaping the course of imbalances in the future. Those medium-term forces suggest that 
imbalances may get worse before they get better. If so, this global financial fault-line could cause further 
tremors to the international monetary, trading and financial system in the period ahead, the like of which we 
are currently experiencing.
Global Imbalances – Past
Global imbalances are a natural by-product of free trade in goods, services and capital. In others words, 
capital flows are a necessary ingredient of trade and capital liberalisation. In that sense, they are good 
cholesterol. But capital flows may also demonstrate an imbalance between demand and output in an 
economy, which must eventually correct if debt and wealth stocks are not to become unsustainable. Sharp 
corrections in rates of domestic absorption and/or capital flows could then result, with attendant output costs. 
In that sense, global imbalances may also be bad cholesterol.1
So which are capital flows today? Historical evidence is illuminating. Chart 1 plots (the absolute value of) 
current account balances in thirteen countries, as a % of GDP, since 1880. Other than in wartime, global 
imbalances are at their highest in well over a century. They have surpassed levels which prevailed during 
the classical Gold Standard and are more than twice levels during the Bretton Woods period (Chart 2). For 
some individual countries, these capital flows look larger still, both absolutely and relative to historical norms 
(Chart 3).
It is not just the size of these global flows that is unusual. So too is their direction. Contrary to theory, capital 
is flowing from developing countries with a low capital stock, towards developed countries with a high capital 
stock (Lucas (1990)). In other words, capital is flowing “uphill”, away from countries where the marginal 
product of capital should be high and towards countries where it should be low. This makes the pattern of 
global capital flows doubly perplexing.
 
1
 Blanchard and Milesi-Ferretti (2009) discuss the causes of “good” and “bad” imbalances.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
3
3
Chart 1: Current account balances Chart 2: Average current account balances
0
1
2
3
4
5
6
1870 1887 1904 1921 1938 1955 1972 1989 2006
Per cent of GDP
0.0
0.5
1.0
1.5
2.0
2.5
Gold 
Standard 
1870-1913
Interwar 
1919-1939
Bretton 
Woods 
1944-1971
Post BW 
1972-2009
Per cent of GDP
Chart 3: Current account balances 
for individual countries
Chart 4: Gross and net capital flows
2 0
1 5
10
5
0
5
10
1 5
2 0
8 0 82 8 4 8 6 88 9 0 9 2 94 9 6 9 8 00 0 2 0 4 06 0 8
Total current account surpluses (a)
Total current account deficits (b)
Gross capital outflows (c) 
Gross capital inflows (d) 
BIS bank gross outflows
BIS bank gross inflows
Per cent of world GDP
-
+
Sources: IMF WEO, Taylor (2002) and Bank calculations.
(a) 5-year moving average. Data prior to 1980 are from Taylor 
(2002). Data post 1980 are from the IMF WEO.
Sources: Bank for International Settlements, IMF WEO and 
Bank calculations.
(a) Sum of global current account surpluses.
(b) Sum of global current account deficits.
(c) Sum of global net purchases of foreign assets by 
residents.
(d) Sum of global net purchases of domestic assets by 
foreigners.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
4
4
Perplexing need not mean bad. Current account imbalances are no more than the arithmetic difference 
between gross capital inflows and outflows. As these gross flows increase as a result of financial 
liberalisation, it is natural to expect the difference between them to become more volatile (Obstfeld (2010)). 
As Chart 4 shows, the growth of gross capital flows in the years preceding the crisis was dramatic. At the 
start of the 1980s, gross capital flows were around 5% of world GDP. By 2007, they had risen to 7 times 
that. 
This growth in capital flows and imbalances has been mirrored in the path of the external balance sheets of 
the major economies. These, too, have adjusted rapidly. In 1985, the US was a net external creditor. By 
2009, it had become a net external debtor to the tune of around 20% of GDP. In 1999, China was a net 
external debtor. By 2008, it had become a net external creditor of around one third of GDP. 
An interesting perspective on the drivers of imbalances is provided by looking at the gross saving and 
investment behaviour of surplus and deficit countries over this period. Chart 5 looks at these patterns. It 
suggests a couple of striking patterns.
First, the correlation between saving and investment rates appears to have weakened significantly over the 
past two decades, for both surplus and deficit countries. This is consistent with increasing capital mobility. 
Historically, there has been a high correlation between national saving and investment rates – the FeldsteinHorioka (1980) puzzle. Chart 6 plots the Felstein-Horioka correlation coefficient alongside an index of 
financial liberalisation from Obstfeld and Taylor (2004). 
Between 1930 and 1980, the Feldstein-Horioka coefficient was close to one – a puzzlingly high comovement between saving and investment. But that pattern has shifted significantly over the past 30 years. 
Saving and investment correlations are no longer puzzlingly strong. Indeed, just ahead of crisis, 
saving/investment correlations had fallen to around zero. They are presently at their lowest levels in almost 
a century. On the face of it, this might point toward a rather benign interpretation of rising global imbalances: 
they may simply be the mirror image of rising capital liberalisation. This would be a good cholesterol story. 
Even then, however, there may be risks associated with intermediating large global flows of funds. Gross 
flows can matter. As experience during 2008 illustrated (Chart 4), sharp reversals of these gross flows can 
disrupt the functioning of financial systems, in particular banking systems reliant on overseas funding. Large 
inflows may also cause indigestion problems for small domestic capital markets. The strong upward 
pressures on asset prices in a number of emerging market economies over the past few months are 
testament to that.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
5
5
Chart 5: Saving and investment Chart 6: Capital mobility and FeldsteinHorioka coefficient
0
5
1 0
1 5
20
25
3 0
3 5
1980 1984 1988 1992 1996 2000 2004 2008
Surplus country saving
Deficit country saving
Surplus country investment
Deficit country investment
Per cent of GDP
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 -0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1880 1900 1920 1940 1960 1980 2000
Feldstein-Horioka coefficient (RHS) (a)
Capital mobility index (LHS) (b)
High
Low
Index
Sources: IMF WEO and Bank calculations.
(a) Surplus and deficit countries defined based on their 
average current account balances between 2003 and 2007.
Sources: Taylor (2002), IMF WEO, Obstfeld and Taylor 
(2004) and Bank calculations.
(a) The Feldstein-Horioka coefficient is the correlation 
coefficient between savings and investment for 15 countries 
(the sample varies slightly over the period).
(b) Obstfeld and Taylor’s capital mobility index is 
judgemental and takes values between 0 and 1.
As Chart 5 also shows, most of the rise in global flows over the past decade is accounted for by changes in 
the saving rates of deficit and surplus countries, rather than in their investment rates. In other words, 
differential savings behaviour has been the prime mover behind rising global imbalances. It is unsurprising, 
then, that savings behaviour lies at the heart of the current debate among policymakers and academics on 
the causes of imbalances. 
This, too, need not be bad news. For example, differential savings may simply be mirroring fundamentals, 
reflecting differences in rates of time preference among consumers or differential rates of technological 
progress among companies. Equally, it is possible to conceive of explanations for differential saving which 
are less well-rooted in fundamentals, such as lax monetary policy in the West or mercantilist trade policy in 
the East.2
 These latter explanations take us closer to a bad cholesterol story for global imbalances. 
This is where much of the current debate on global imbalances is centred. Are they the result of surplus 
countries saving excessively – a “savings glut” explanation (Bernanke (2005))? Or is it instead the deficit 
 
2
 This can lead to a global co-ordination (or adding-up) problem, as countries seek individually to each run a current account surplus, 
so depressing global demand (King (2010)).
All speeches are available online at www.bankofengland.co.uk/publication/speeches
6
6
countries saving too little – a “savings shortage” explanation (Taylor (2007)). Put differently, is the problem 
impatience in the West or excessive patience in the East? 
Global Imbalances – Present
To begin to answer those questions, consider recent saving behaviour in two archetypical surplus and deficit 
countries – China and the US. Chart 7 looks at the respective contributions of China and the US to global 
savings since 1980. In 1980, the US contributed around a quarter of global saving. Today, it contributes 
around a tenth. China has been the mirror image. Having contributed less than 5% in 1980, today China is 
the single largest source of global saving, contributing around one fifth.
Chart 7: China and US share of world saving
0
5
1 0
1 5
2 0
2 5
3 0
8 0 8 3 8 6 8 9 9 2 9 5 9 8 0 1 0 4 0 7
China U S
Per
cent
 Source: IMF WEO and Bank calculations.
It has been argued by some that these trends could simply reflect different cultural attitudes towards saving 
among US and Chinese citizens. In the US, it is often said that there is a culture of spending rather than 
saving – neurologically, the impatience gene may be dominant (Haldane (2010)). In China, by contrast, the 
culture may be biased towards saving rather than spending, with the patience gene dominant. In other 
words, differences in agents’ rate of time preference, or degree of patience, might account for savings 
imbalances (Buiter (1981)).
But a recent survey of time preferences in 45 countries suggests cultural differences are not the full story 
(Wang, Rieger and Hens (2010)). Asked to choose between a payoff this month and a larger payoff next 
month, 68% of US students choose to wait; they exhibit patience. Facing the same trade-off, 62% of 
Chinese students made the same choice. These differences are small.3
 They are also the “wrong way 
around” to explain imbalances. 
 
3
 Among the 45 countries, Germany ranked as the most patient (almost 90% of students waited), Nigeria the least patient (less than 
10%).
All speeches are available online at www.bankofengland.co.uk/publication/speeches
7
7
So if cultural factors are not the explanation, what is? Charts 8 and 9 plot national saving rates in China and 
the US, broken down on a sectoral basis. There are striking differences in saving behaviour across the 
government, household and corporate sectors in the two countries.
Around two-thirds of the rise in Chinese savings since the early 1990s derives from the corporate sector.4
 
This reflects two things. First, rapidly rising corporate profitability, against a backdrop of strong growth and 
rising productivity (Ma and Yi (2010)). Since 2000, profits among Chinese industrial companies have risen 
around 30% per year. 
Second, more significantly still, most of these profits have been retained within Chinese companies rather 
than distributed to shareholders (Qiao and Song (2009)). These differences in US/Chinese corporate 
savings behaviour show up dramatically in dividend payouts. The average dividend payout ratio among US 
corporations in 2009 was 40%, with less than a quarter of companies failing to pay a dividend. The average 
dividend payout ratio among listed Chinese corporations in 2009 was around 18%. Among these, more than 
half paid no dividend whatsoever.
Charts 10 and 11 consider the distribution of changes in dividend payout ratios among global and Chinese 
companies. Among global companies, payout ratios rarely fall. More than half of the time they increase. 
Impatient investors demand immediate dividend gratification, leading to large and rising distributions. This 
dividend ratchet is a puzzle in empirical finance (Black (1976)). If anything, it is one which is growing over 
time.
The payout pattern among Chinese quoted firms could not be more different. Increases and decreases in 
payout ratios are roughly evenly split. The high proportion of “unchanged” payout ratios simply reflects the 
zero payout policy of more than half of quoted Chinese companies. There is no distribution ratchet. Profits 
are largely ploughed back. In this respect, Chinese payout behaviour today is not dissimilar to that among 
US companies during its rapid growth phase in the 19th century (Chart 12).
 
4
 Using firm-level data, Bayoumi, Tong and Wei (2010) argue that Chinese listed firms’ corporate savings rates are neither especially 
high compared to listed firms in other countries and the gross savings rate for a typical listed Chinese firm has declined, albeit 
insignificantly, between 2002-2007.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
8
8
Chart 8: China gross national saving Chart 9: US gross national saving
0
1 0
20
3 0
4 0
50
6 0
1992 1995 1998 2001 2004 2007
Government Corporate
Household
Per cent of GDP
-1 0
-5
0
5
1 0
1 5
2 0
2 5
1992 1995 1998 2001 2004 2007
Government Corporate
Household Total
Per cent of GDP
Source: People’s Bank of China. Sources : Bureau of Economic Analysis and Bank calculations
(a) Total excludes statistical discrepancy
Chart 10: Dividend payouts: global companies Chart 11: Dividend payout ratios: Chinese 
companies
0
1 0
2 0
3 0
4 0
5 0
6 0
7 0
8 0
9 0
100
1981 1985 1989 1993 1997 2001 2005 2009
Per cent of firms Increase No Change Decrease
0
10
20
30
40
50
60
70
80
90
100
1997 1999 2001 2003 2005 2007 2009
Per cent of firms Increase No change Decrease
Source: Datastream and Bank calculations.
(a) The chart shows year on year changes in dividend payments 
at 215 of the largest firms on the FTSE, S&P500, Topix, CAC 
and DAX indexes.
Sources: Worldscope, Bank calculations.
(a) The chart shows year on year changes in dividend payout ratios 
for a rolling sample of listed Chinese firms. The sample covers 
firms in the Worldscope database as of 8 November, 2010.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
9
9
Chart 12: Dividend payout ratios: US firms in 
19th century
Chart 13: US household net worth and financial 
balance
0
1 0
2 0
3 0
4 0
5 0
6 0
7 0
8 0
9 0
100
1825 1830 1835 1840 1845 1850 1855 1860 1865
Per cent of firms Increase No Change Decrease
3.0
3.5
4.0
4.5
5.0
5.5
6.0
6.5
-6 7.0
-4
-2
0
2
4
6
8
10
1960 1970 1980 1990 2000
Financial balance (LHS)
Net worth (RHS; inverted)
Per cent of 
personal 
disposable 
income
Multiple of
Personal
disposable 
income
Source: Yale School of Management, reprinted in De Angelo, 
De Angelo and Skinner (2009).
(a) The chart shows year on year changes in dividend 
payments at a sample of 515 NYSE listed firms
Sources: Bureau of Economic Analysis; Federal Reserve and Bank 
calculations
(a) Includes non-profit organisations
There is more than one explanation for this behaviour among Chinese firms. A benign interpretation is that it 
reflects the high option value of Chinese companies retaining funds given future growth prospects, as with 
US firms in the 19th century. A less benign interpretation is that it reflects an absence of external sources of 
company finance and shortfalls in corporate governance, with insufficient discipline on companies by outside 
investors. In practice, both factors are likely to have been important over recent years. 
Turning to US and Chinese households, during this century household gross saving as a fraction of GDP has 
averaged less than 5% in the US. In China, it has averaged around 20%.
In China, the constancy in high household saving rates masks two striking, but broadly offsetting, factors: a 
10 percentage point fall in household’s income share; and a 10 percentage point rise in household’s saving 
propensity (Ma and Yi (2010)). Much of the rise in saving propensities appears to be precautionary. Facing 
“three mountains” in the future - education, pensions and health care – Chinese consumers have taken the
high (saving) road. 
In the US, the fall in the household financial balance correlates strongly with the path of household net worth 
(McKelvey (2010), Chart 13). This correlation emerged strongly during the 1980s, with the simultaneous rise 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
10
10
in net wealth and fall in saving. This correlation has remained strong as net worth has reversed recently due 
to falling house and equity prices. 
What lies behind these movements? Lax monetary policy has been suggested by some (Taylor (2007)). 
Financial liberalisation is a second plausible candidate. This had the effect of loosening constraints on the 
supply of housing credit and thereby boosting housing demand and prices in the US. It also enabled 
households to mobilise and withdraw the accrued equity in their property to finance spending. Estimates of 
housing equity withdrawal for spending rose from under 2% of personal disposable income in 2000 to around 
4% at the peak in 2005 (Greenspan and Kennedy (2007), Chart 14). 
Micro-level data support this story. They show that home equity withdrawal was driven primarily by 
households with low credit scores and high credit card utilisation rates, who are most likely to have been 
credit-constrained in the past (Mian and Sufi (2009)). As these constraints loosened, spending rose and 
saving fell. Liberalisation fed impatience. 
These patterns are also consistent with an inequality-based explanation for lower saving (Rajan (2010)). 
Rising inequality may generate an increased desire to “keep up with the Jones’s”. That, in turn, may have 
led to higher borrowing, and lower net saving, by the poor. 
And what applies within countries appears also to apply across them. Among advanced countries, there is a 
significant negative relationship between within-country measures of inequality and the current account 
position (Chart 15). Rising inequality, by lowering savings rates among the poor, appears to be deficitinducing. Rising imbalances may have important social, as well as economic, roots.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
11
11
Chart 14: US housing equity withdrawal Chart 15: Inequality and current account 
balances in advanced economies
-1
0
1
2
3
4
5
6
7
1991 1994 1997 2000 2003 2006
'Active housing equity withdrawal (a)'
Extracted equity used for spending
Per cent of 
personal 
disposable 
income y = -0.0049x + 0.3011
R² = 0.491
0.2
0.25
0.3
0.35
0.4
-10 0 10 20
Current account balance (% of GDP, 2005) Gini coefficient, mid 2000s
Sources: Bureau of Economic Analysis, Greenspan and Kennedy 
(2007), and Bank calculations
(a) Defined by Greenspan and Kennedy (2007) as the change in home 
equity mortgage debt plus gross cash out.
Sources: OECD, IMF WEO and Bank calculations.
Note: Chart includes advanced economies (IMF definition) 
excluding major oil exporters and reserve accumulators.
Global Imbalances – Future
So what will be the key forces driving global imbalances in the period ahead? Judging from the recent past, 
two factors have been important: 
Global financial integration;
Differential savings behaviour.
Consider in turn their likely evolution.
Global financial integration. In the period ahead, this will be shaped by a number of medium-term structural 
factors. One key such factor is cross-country GDP convergence. Over time, we would expect countries’ 
income levels to converge, as currently low income countries catch-up with their high income trading 
neighbours (Solow (1956)). This convergence process is a well-established empirical phenomenon (Barro 
(1991) and Mankiw, Romer and Weil (1992)). 
Accompanying this GDP convergence is likely to be catch-up in gross external balance sheet positions. As 
countries develop, so too does their integration into global capital markets, with higher levels of gross 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
12
12
external assets and liabilities. Cross-country evidence bears this out. There is a well-defined empirical 
relationship between a country’s GDP per head and its gross external balance sheet position (Chart 16). 
Chart 16: Per capita GDP and gross external 
balance sheets, 2007
Chart 17: Ratio of external balance sheet to 
GDP
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
8.00
4.0 6.0 8.0 10.0 12.0
Gross external balance sheet, multiple of GDP
Ln (per capita GDP)
-
0.50 
1.00 
1.50 
2.00 
2.50 
3.00 
3.50 
1980
1996
2012
2028
2044
China India Brazil
G7 Russia
Source: Updated and extended version of the External Wealth of 
Nations Mark II database developed by Lane and Milesi-Ferretti 
(2007), US Census Bureau and Bank calculations. 
(a) Includes all countries in the Lane and Milesi-Ferretti database, 
except Luxembourg, Bahrain, Hong Kong , Ireland, Kiribati and 
Timor-Leste.
(b) External balance sheet is defined as the average of external 
assets and liabilities.
Sources: Updated and extended version of the External 
Wealth of Nations Mark II database developed by Lane and 
Milesi-Ferretti (2007), IMF, US Census Bureau, Penn World 
Table and Bank calculations. 
(a) Measured as the average of external assets and external 
liabilities. 
Taken together, these two factors imply a pattern of steadily rising gross capital flows over time, with growth 
strongest among countries catching up with their high-income trading neighbours. It is possible to calibrate 
these integration trends and project them forward to simulate future external balance sheets positions among 
the major economies. 
These simulations take as initial conditions cross-country GDP per capita, projected forward using a 
calibrated rate of growth convergence drawn from previous empirical studies and adjusted for known future 
changes in population growth. This gives a set of cross-country projections for GDP per capita. These are 
then translated into gross external balance sheet positions using historical relationships with per capita GDP. 
Chart 17 plots some projections of (the average of) gross external assets and liabilities relative to GDP for 
the G7 countries, China, India, Brazil and Russia. These projections go out to 2050. Among the G7 
economies, external balance sheet ratios are broadly flat at around twice nominal GDP. This flatness is 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
13
13
driven in large part by assumption, as it is assumed the highest-income countries maintain a flat ratio of 
external assets to GDP. 
This is almost certainly too extreme an assumption. Although smaller than in the past, there remain strong 
home biases in the investment portfolios of developed countries’ institutional investors (Obstfeld and Rogoff 
(2000) and IMF (2010)). As liberalisation progresses, these biases would be expected to unwind, boosting 
gross external assets in relation to GDP in developed countries. For that reason, the calibration here can be 
thought of as a limiting case of the likely adjustments in the pattern of global capital flows.
Subject to that important caveat, the growth in the non-G7 economies’ contribution to global capital markets 
is still striking. Among the BRICs, external balance sheets as a fraction of GDP exceed G7 levels by around 
2035. By 2050, they are around 2.5 times nominal GDP. These patterns are even more striking when 
expressed as a share of global external assets. By 2050, over half of all G20 external assets are associated 
with the BRICs, up from around 9% currently (Chart 18). Among the non-G7, non-BRIC G20 countries, their 
share trebles from 5% to around 15%. 
Within this, some countries’ share of global assets sky-rocket. China’s share of global finance rises to 
around 30% by 2050, roughly that of the entire G7. India rises to almost 20%, from less than 0.5% currently 
(Chart 19). The US share falls from 28% to around 12%.
The flow counterpart to these external stock positions is no less dramatic (Chart 20). By 2020, gross capital 
flows of the non-G7 countries are projected to exceed those of the G7 economies. By 2030, non-G7 flows 
are more than three times G7 flows. And by 2040, they are almost four times G7 flows.
Even if these calibrations represent an extreme case, they are indicative of a striking power shift in the 
pattern of global financial flows. If this path were to be even broadly followed, it would have implications for 
the scale of global imbalances, which will tend to rise as gross capital flows outpace GDP growth. It would 
have implications for financial stability, as the scale of gross capital surges (fuelling bubbles) and reversals 
(fuelling crises) increases. And it may also have implications for the dollar’s reserve currency status.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
14
14
Chart 18: Share of G20 total external assets, by 
region 
Chart 19: Share of G20 total external assets, 
by country
0 %
20%
40%
60%
80%
100%
1980
1996
2012
2028
2044
Other G20 BRICS G7
0 %
5 %
10%
15%
20%
25%
30%
35%
40%
45%
1980 1996 2012 2028 2044
UK USA China
India Brazil Russia
France Germany
Sources: IMF, US Census Bureau, Penn World Table and Bank 
calculations.
Sources: IMF, US Census Bureau, Penn World Table and 
Bank calculations.
Chart 20: Ratio of non-G7 capital outflows to 
G7 capital outflows
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
2010 2020 2030 2040 2050
Source: Updated and extended version of the External Wealth 
of Nations Mark II database developed by Lane and MilesiFerretti (2007), IMF, US Census Bureau, Penn World Tables 
and Bank calculations.
All speeches are available online at www.bankofengland.co.uk/publication/speeches
15
15
Differential savings behaviour. Consider next the possible future path of savings behaviour across the 
world’s major economies. There are likely to be many factors affecting cross-country saving rates in the 
period ahead. Predicting these is fraught with uncertainty. But one key medium-term determinant of saving 
is demographics (Wilson and Ahmed (2010)). And demographic trends are something about which there is 
little uncertainty in the period ahead.
Chart 21 looks at projected changes in the fraction of adults in the age range 40 to 59 in developed and 
emerging countries – the “prime savings” cohort. It suggests a sharp bifurcation in demographic trends in 
developed and emerging economies. Among advanced countries, an ageing population results in a decline 
in the middle-aged share, from a peak today of almost 30% to around 25% by 2050. For developing 
economies the reverse is true, with a continuing rise in the fraction of middle-aged individuals from 20% 
today to around 25% by 2050. 
Cross-country correlations suggest a relationship between this prime-saving cohort and saving behaviour. 
Chart 22 suggest that every 1 percentage point rise in the middle-aged population share raises the national 
savings rate by around 1.75 percentage points.5
 Taken together with emerging population trends, this 
suggests some potentially important life-cycle pressures on the global pattern of savings in the period ahead.
Chart 21: Share of population aged 40-59 Chart 22: Savings rate and population
0.15
0.17
0.19
0.21
0.23
0.25
0.27
0.29
0.31
2000
2005
2010
2015
2020
2025
2030
2035
2040
2045
2050
Less developed countries
More developed countries
World
y = 1.7569x - 0.288
R² = 0.2831
-
0.05 
0.10 
0.15 
0.20 
0.25 
0.30 
0.35 
0.40 
0.45 
0.50 
0.2 0.25 0.3 0.35 0.4
National saving rate, 2010
40-59 share
Sources: US Census Bureau and Bank calculations. Sources: IMF, UN and Bank calculations.
(a) National saving rates for advanced economies 
To bring these pressures to life, Chart 23 projects saving rates in a selection of countries out to 2050 based 
on population and GDP projections. This is plainly a gross over-simplification. These projections take no 
account of a whole host of other factors that may push in the other direction, including later retirement dates 
 
5
 See also the evidence in Wilson and Ahmed (2010).
All speeches are available online at www.bankofengland.co.uk/publication/speeches
16
16
which may boost saving in countries with an ageing population, and falls in saving rates in emerging 
countries as the social safety net is widened. 
But based on demographic and convergence trends alone, saving rates in the BRICS, already high, are 
projected to continue their ascent. For example, in India they lead to an increase in saving of around 10 
percentage points of GDP, raising the saving ratio to 50% by 2050, as demographics push a larger fraction 
of the population into the high savings cohort. Among developed countries, meanwhile, these trends are 
reversed with further demography-induced falls in savings rates.
Chart 23: Saving rate by country(a) Chart 24: Share of total G20 savings(a)
savings
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1980
1996
2012
2028
2044
USA UK China
Germany Brazil India
0
5
1 0
1 5
2 0
2 5
3 0
3 5
4 0
4 5
1980
1996
2012
2028
2044
US China
Per cent
Sources: IMF, US Census Bureau, UN, Penn World Table 
and Bank calculations.
(a) National saving rate. 
Sources: IMF, US Census Bureau, UN, Penn World Table 
and Bank calculations.
(a) Russia is not included prior to 1995 due to data 
availability. 
Given the rising income share of the BRIC countries, these patterns are even more dramatic when we 
consider country shares of future global savings. China rises to represent around 40% of global savings by 
2050. Over the same period, the US share of global savings continues its descent, stabilising at around 5%.
On the face of it, these partial equilibrium projections of future savings rates suggest that medium-term 
pressures on global imbalances could intensify, with saving rates rising among surplus countries and falling 
in deficit countries. To bring this intuition to life, assume for simplicity’s sake that investment rates as a 
fraction of GDP remain fixed at 2010 levels across countries, but that saving rates evolve as in Chart 23. 
The result would be a trebling of global imbalances from current elevated levels over the next twenty years.
Of course, these projections only look at the marginal impact on imbalances of medium-term demographic 
and GDP trends. They take no account of a host of other factors that may defuse pressures on imbalances, 
All speeches are available online at www.bankofengland.co.uk/publication/speeches
17
17
including adjustments in real exchange rates, lower investment ratios as the marginal product of capital 
reduces in emerging markets and falls in saving rates in emerging markets as the social safety net is 
widened and deepened. These offsetting factors will need to be very significant, however, if they are to 
counterbalance medium-term upward pressures on global imbalances from demographics and convergence.
Conclusion
So where does this leave us? With a potentially dramatic change in the future international financial 
landscape. With a potentially decisive shift in the pattern of global capital flows. With potential pressures for 
a further widening in global imbalances. And, if so, with a likely intensification of pressures on international 
monetary, financial and trading systems. The impetus to reform these global systems is strong today. It may 
be stronger still tomorrow.

The financial crisis of the past three years has, on any measure, been extremely costly. As in 
past financial crises, public sector debt seems set to double relative to national income in a 
number of countries (Reinhart and Rogoff (2009)). And measures of foregone output, now 
and in the future, put the net present value cost of the crisis at anywhere between one and five 
times annual world GDP (Haldane (2010)). Either way, the scars from the current crisis seem 
likely to be felt for a generation. 
It is against this backdrop that an intense debate is underway internationally about reform of 
finance (Goodhart (2010)). Many of the key planks of that debate are covered in other 
chapters in this volume. Some of these reform measures are extensions or elaborations of 
existing regulatory initiatives – for example, higher buffers of higher quality capital and 
liquidity. Others propose a reorientation of existing regulatory apparatus – for example, 
through counter-cyclical adjustments in prudential policy (Bank of England (2009b), Large 
(2010)). Others still suggest a root-and-branch restructuring of finance – for example, by 
limiting the size and/or scope of banking (Kay (2009), Kotlikoff (2010)). 
In evaluating these reform proposals, it is clearly important that the on-going benefits of 
finance are properly weighed alongside the costs of crisis. Doing so requires an 
understanding and measurement of the contribution made by the financial sector to economic 
well-being. This is important both for making sense of the past (during which time the role 
of finance has grown) and for shaping the future (during which it is possible the role of 
finance may shrink). 
While simple in principle, this measurement exercise is far from straightforward in practice. 
Recent experience makes clear the extent of the problem. In September 2008, the collapse of 
Lehman Brothers precipitated a chain reaction in financial markets. This brought the 
financial system, and many of the world’s largest institutions, close to the point of collapse. 
During the fourth quarter of 2008, equity prices of the major global banks fell by around 50% on 
average, a loss of market value of around $640 billion. As a consequence, world GDP and world 
trade are estimated to have fallen at an annualised rate of about 6% and 25% respectively in 
2008Q4. Banking contributed to a Great Recession on a scale last seen at the time of the 
Great Depression. 
3
Yet the official statistics on the contribution of the financial sector paint a rather different 
picture. According to the National Accounts, the nominal gross value-added (GVA) of the 
financial sector in the UK grew at the fastest pace on record in 2008Q4. As a share of wholeeconomy output, the direct contribution of the UK financial sector rose to 9% in the last 
quarter of 2008. Financial corporations’ gross operating surplus (GVA less compensation 
for employees and other taxes on production) increased by £5.0bn to £20bn, also the largest 
quarterly increase on record. At a time when people believed banks were contributing the 
least to the economy since the 1930s, the National Accounts indicated the financial sector 
was contributing the most since the mid-1980s. How do we begin to square this circle? 
That is the purpose of this chapter. It is planned as follows. In Section 2, we consider 
conventional measures of financial sector value added and how these have evolved over time. 
In Section 3, we consider a growth accounting breakdown of the factor inputs which have 
driven growth – quantities of labour and capital and the returns to these factors. This 
suggests banking has undergone, at least arithmetically, a “productivity miracle” over the past 
few decades. Section 4 explores in greater detail some of the quantitative drivers of high 
aggregate returns to banking, while Section 5 explores some of banks’ business activities. 
Risk illusion, rather than a productivity miracle, appears to have driven high returns to 
finance. The recent history of banking appears to be as much mirage as miracle. Section 6 
concludes with some policy implications. 
2. Measuring Financial Sector Output
(a) Historical Trends in GVA 
The standard way of measuring the contribution of a sector to output in the economy is GVA. 
This is defined as the value of gross output that a sector or industry produces less the value of 
intermediate consumption (that is, goods and services used in the process of production). 
GVA only measures the sector’s direct contribution to the economy. The indirect 
contribution of finance - for example, on productivity growth through the provision of funds 
for start-up businesses and new investment projects - may also be important. But looking at 
historical trends in value added is a useful starting point. 
Chart 1 plots an index of real GVA of the financial intermediation sector in the UK from the 
middle of the 19th century, alongside an index of whole-economy output. Both series are in 
4
constant prices and indexed to 1975=100. Table 1 breaks down the growth rates of finance 
and whole economy output into three sub-samples – pre-First World War, from the First 
World War to the early 1970s, and thence to date. The historical trends in GVA for the 
financial sector are striking. 
Over the past 160 years, growth in financial intermediation has outstripped whole economy 
growth by over 2 percentage points per year. Or put differently, growth in financial sector 
value added has been more than double that of the economy as a whole since 1850. This is 
unsurprising in some respects. It reflects a trend towards financial deepening which is 
evident across most developed and developing economies over the past century. This 
structural trend in finance has been shown to have contributed positively to growth in the 
whole-economy (Wadhwani (2010)). 
The sub-sample evidence suggests, however, that this has not been a straight line trend. The 
pre-First World War period marked a period of very rapid financial deepening, with the 
emergence of joint stock banks to service the needs of a rapidly growing non-financial 
economy. Finance grew at almost four times the pace of the real economy during this rapidgrowth period (Table 1). 
The period which followed, from the First World War right through until the start of the 
1970s, reversed this trend. The growth in finance fell somewhat short of that in the rest of the 
economy. This in part reflected the effects of tight quantitative constraints on, and 
government regulation of, the financial sector. 
The period from the early 1970s up until 2007 marked another watershed. Financial 
liberalisation took hold in successive waves. Since then, finance has comfortably outpaced 
growth in the non-financial economy, by around 1.5 percentage points per year. If anything, 
this trend accelerated from the early 1980s onwards. Measured real value added of the 
financial intermediation sector more than trebled between 1980 and 2008, while whole 
economy output doubled over the same period. 
In 2007, financial intermediation accounted for more than 8% of total GVA, compared with 
5% in 1970. The gross operating surpluses of financial intermediaries show an even more 
dramatic trend. Between 1948 and 1978, intermediation accounted on average for around 
1.5% of whole economy profits. By 2008, that ratio had risen tenfold to about 15% (Chart 2). 
5
Internationally, a broadly similar pattern is evident. In the US, following a major decline 
during the Great Depression, the value added of the financial sector has risen steadily since 
the end of the Second World War. As a fraction of whole economy GVA, it has quadrupled 
over the period, from about 2% of total GDP in the 1950s to about 8% today (Chart 3). 
Similar trends are evident in Europe and Asia. According to data from the Banker, the largest 
1000 banks in the world reported aggregate pre-tax profits of almost $800 billion in fiscal 
year 2007/08 (Chart 4), almost 150% higher than in 2000/01. This equates to annualised 
returns to banking of almost 15%. 
Some of these trends in the value added and profits of the financial sector, and in particular 
their explosive growth recently, are also discernible in the market valuations of financial 
firms relative to non-financial firms. Total returns to holders of major banks’ equity in the 
UK, US and euro area rose a cumulative 150% between 2002 and 2007 (Chart 5). This 
comfortably exceeded the returns to the non-financial economy and even to some of the more 
risk-seeking parts of the financial sector, such as hedge funds. 
To illustrate this rather starkly, consider a hedged bet placed back in 1900, which involved 
going long by £100 in financial sector equities and short in non-financial equities by the same 
amount. Chart 6 shows cumulative returns to following this hedged strategy. From 1900 up 
until the end of the 1970s, this bet yielded pretty much nothing, with financial and nonfinancial returns rising and falling roughly in lockstep. But from then until 2007, cumulative 
returns to finance took off and exploded in a bubble-like fashion. Only latterly, with the 
onset of the crisis, has that bubble burst and returned to earth. 
(b) Measuring GVA in the Financial Sector 
To begin to understand these trends, it is important first to assess how financial sector valueadded is currently measured and the problems this poses when gauging the sector’s 
contribution to the broader economy. 
Most sectors charge explicitly for the products or services they provide and are charged 
explicitly for the inputs they purchase. This allows the value-added of each sector to be 
measured more or less directly. For example, gross output of a second-hand car dealer can be 
calculated as the cash value of all cars sold. The value added of that dealer would then be 
6
estimated by subtracting its intermediate consumption (the value of cars bought) from gross 
output. 
This is also the case for some of the services provided by the financial sector.1
 For example, 
investment banks charge explicit fees when they advise clients on a merger or acquisition. 
Fees or commissions are also levied on underwriting the issuance of securities and for the 
market-making activities undertaken for clients. But such direct charges account for only 
part of the financial system’s total revenues. Finance – and commercial banking in particular 
– relies heavily on interest flows as a means of payment for the services they provide. Banks 
charge an interest rate margin to capture these intermediation services. 
To measure the value of financial services embedded in interest rate margins, the concept of 
FISIM – Financial Intermediation Services Indirectly Measured – has been developed 
internationally. The concept itself was introduced in the 1993 update of the United Nations 
System of National Accounts (SNA). The SNA recognises that financial intermediaries 
provide services to consumers, businesses, governments and the rest of the world for which 
explicit charges are not made. In associated guidelines, a number of such services are 
identified including: 
 Taking, managing and transferring deposits; 
 Providing flexible payment mechanisms such as debit cards; 
 Making loans or other investments; and 
 Offering financial advice or other business services. 
FISIM is estimated for loans and deposits only. The calculation is based on the difference 
between the effective rates of interest (payable and receivable) and a ‘reference’ rate of 
interest, multiplied by the stock of outstanding balances. According to SNA guidelines, ‘this 
reference rate represents the pure cost of borrowing funds – that is, a rate from which the risk 
premium has been eliminated to the greatest extent possible, and that does not include any 
intermediation services.’2
 For example, a £1,000 loan with a 9% interest receivable and a 4% 
reference rate gives current price FISIM on the loan = £1,000 x (9% – 4%) = £50. And for a 
                                                           
1
 For further details refer to, for example, Akritidis L (2007).
2
 1993 System of National Accounts, paragraph 6.128: http://unstats.un.org/unsd/sna1993/toctop2.asp. 
7
£1,000 deposit with a 3% interest payable and a 4% reference rate, this gives current price 
FISIM on the deposit = £1,000 x (4% – 3%) = £10. Overall, estimated current price FISIM 
accounts for a significant share of gross output of the banking sector (Chart 7). 
Estimating a real measure of FISIM is fraught with both conceptual and computational 
difficulties. In the earlier example of the second-hand car dealer, statisticians can use the 
number of cars sold as an indicator of the volume of gross output. But the conceptual 
equivalent for financial intermediation is not clear. Would two loans of £50 each to the same 
customer represent a higher level of activity than one loan of £100? Methods for measuring 
FISIM at constant prices are based on conventions. In the UK, real FISIM is calculated by 
applying the base-year interest margins to an appropriate volume indicator of loans and 
deposits. The latter is estimated by deflating the corresponding stocks of loans and deposits 
using the GDP deflator. This method means that any volatility in the current price measure 
of FISIM caused by changes in interest margins does not feed into the real measure.
(c) Refining the Measurement of FISIM 
While the introduction of FISIM into the national accounts was an important step forward, it 
is not difficult to construct scenarios where the contribution of the financial sector to the 
economy could be mis-measured under this approach. A key issue is the extent to which 
bearing risk should be measured as a productive service provided by the banking system. 
(i) Adjusting FISIM for Risk 
Under current FISIM guidelines, which use risk-free policy rates to measure the reference 
rate, banks’ compensation for bearing risk constitutes part of their measured nominal output. 
This can lead to some surprising outcomes. For example, assume there is an economy-wide 
increase in the expected level of defaults on loans or in liquidity risk, as occurred in October 
2008. Banks will rationally respond by increasing interest rates to cover the rise in expected 
losses. FISIM will score this increased compensation for expected losses on lending as a rise 
in output. In other words, at times when risk is rising, the contribution of the financial sector 
to the real economy may be overestimated. This goes some way towards explaining the 
2008Q4 National Accounts paradox of a rapidly rising financial sector contribution to 
nominal GDP. 
8
Of course, the financial sector does bear the risk of other agents in the economy. Banks take 
on maturity mismatch or liquidity risk on behalf of households and companies. And banks 
also make risky loans funded by debt, which exposes them to default or solvency risk. But it 
is not clear that bearing risk is, in itself, a productive activity. Any household or corporate 
investing in a risky debt security also bears credit and liquidity risk. The act of investing 
capital in a risky asset is a fundamental feature of capital markets and is not specific to the 
activities of banks. Conceptually, therefore, it is not clear that risk-based income flows 
should represent bank output. 
The productive activity provided by an effectively functioning banking system might be 
better thought of as measuring and pricing credit and liquidity risk. For example, banks 
screen borrowers’ creditworthiness when extending loans, thereby acting as delegated 
monitor. And they manage liquidity risk through their treasury operations, thereby acting as 
delegated treasurer. These risk-pricing services are remunerated implicitly through the 
interest rates banks charge to their customers. 
Stripping out the compensation for bearing risk to better reflect the service component of the 
financial sector could be achieved in different ways. One possibility would be to adjust 
FISIM using provisions as an indicator of expected losses. A broader adjustment for risk, as 
has been suggested by several commentators, would be to move away from the risk-free rate 
as the reference rate within FISIM.3
 For example, a paper prepared for the OECD Working 
Party on National Accounts (Mink (2008)) suggested that the FISIM calculation should use 
reference rates that match the maturity and credit risk of loans and deposits. This would also 
eliminate an inconsistency within the current National Accounts framework. Measured 
financial intermediation output increases if a bank bears the risk of lending to a company. 
But gross output is unchanged if a household holds a bond issued by the same company and 
thus bears the same risk. 
To see how such a mechanism would work, consider the following simple example. A bank 
lends £100 to a corporate borrower at 7% per annum for one-year. The risk-free rate is 5%. 
The bank correctly assesses the credit risk of the corporate to be A-rated. The market spread 
for A-rated credits at a maturity of one-year is 1% over the risk-free rate. Current FISIM 
would estimate bank output as £2 (Table 2). Risk-adjusted FISIM, though, would estimate 
                                                           
3
 Wang et al (2004), Wang (2003), Mink (2008), Colangelo and Inklaar (2010). 
9
banks’ output as £1. 
An adjustment of FISIM along these lines could potentially be material. According to 
simulations on the impact of such an approach for the Euro-area countries, aggregate riskadjusted FISIM would stand at about 60% of current aggregate FISIM for the Euro-area 
countries over the period 2003-7 (Mink (2008)). 
(ii) Measuring Risk 
Adjusting FISIM for risk would better capture the contribution of the financial sector to the 
economy. The fundamental problem is, however, that risk itself is unobservable ex-ante. 
The methodology described above measures risk in a relative way; it effectively assumes that 
if banks deviate from prevailing market rates, this is to compensate for the services they 
provide to borrowers and depositors. But at no point is there an assessment of the ability of 
the financial system to price risk correctly in an absolute sense. This might not be the 
objective of statisticians when measuring output. But it is essential when gauging the 
contribution of finance to economic well-being. 
To see this more clearly, consider an alternative example (Table 3). A bank lends £100 to a 
corporate borrower. But the bank incorrectly assesses the credit risk of the corporate to be Arated, when the true credit risk is BB-rated. Assume for simplicity that the corporate, 
knowing that its credit risk is greater than A, is prepared to pay a spread higher than that on 
an A-rated credit risk (say 2%). The market spreads for A-rated and BB-rated credits are 1% 
and 2% respectively. “Measured” risk-adjusted FISIM is still an improvement on current 
FISIM. But the value of bank output is still overstated relative to “true” risk-adjusted FISIM. 
This would be equivalent to second-car hand dealers consistently selling lemons. But a 
dodgy car-seller would be quickly found out. Mechanical risk is observable. Dealers that 
persistently mis-price cars would be driven from the market. Buyers might instead then 
choose to meet online. 
A banking system that does not accurately assess and price risk is not adding much value to 
the economy. Buyers and sellers of risk could meet instead in capital markets – as they have, 
to some extent, following the crisis. But unlike the condition of a car, risk is unobservable. 
So mis-pricing of risk, and mis-measurement of the services banks provide to the real 
10
economy, may persist. This echoes events in the run-up to crisis when market prices 
systematically under-priced risk for a number of years. Using the market price of risk would 
have led statisticians systematically to overstate the potential contribution of the financial 
sector over this period. 
Attempting to adjust the measurement of bank output for risk by changing the reference rate 
in FISIM is an improvement on current practices. But it would still fall short of assessing 
whether the financial sector is pricing risk correctly and hence assessing the true value of the 
services banks provides to the wider economy. Unless the price of risk can be evaluated, it 
seems unlikely the contribution of the financial sector to the economy can be measured with 
accuracy.
3. Decomposing the Contribution of the Financial Sector – the Productivity “Miracle” 
To that end, an alternative way of looking at the contribution of the financial sector is through 
inputs to the production process. This might shed more light on the sources of the rapid 
growth in finance. Was this expansion accompanied by a rising share of resources employed 
by finance relative to the rest of the economy? Or did it instead reflect unusually high returns 
to these factors of production? This section considers these questions in turn. 
(a) Growth accounting decomposition 
The basic growth accounting framework breaks down the sources of economic growth into 
the contributions from increases in the inputs to production, capital and labour. This amounts 
to relating growth in GDP to growth in labour input and in various capital services (from 
buildings, vehicles, computers and other resources). When these factors have all been 
accounted for, the remainder is often attributed to technical change – the so-called Solow 
residual (Solow (1957)). 
The growth accounting framework assumes an underlying aggregate production function. In 
its most basic form, the aggregate production function can be written as: 
Q  f (K, L,t)
11
where Q is output, K and L represent capital and labour units and t appears in f to allow for 
technical change. 
Assuming constant returns to scale, perfect competition (so that factors of production are paid 
their marginal products) and Hicks-neutral technical change (so that shifts in the production 
function do not affect marginal rates of substitution between inputs), output growth can be 
expressed as a weighted sum of the growth rates of inputs and an additional term that 
captures shifts over time in the production technology. The weights for the input growth 
rates are the respective shares in total input payments – the labour and capital shares. More 
specifically: 
L
L
K
K
A
A
Q
Q
K L
   
  
where A(t) is a multiplicative factor in the production function capturing technical change. 
 K , L represent respectively the capital and labour shares of income. 
Charts 8 and 9 look at the proportion of labour and physical capital employed by the financial 
intermediation sector in the UK relative to the whole economy over the past forty years. 
They follow a not dissimilar path, with both labour and capital inputs rising as a share of the 
whole economy for much of the period. The proportion of labour employed by finance rises 
by around 50% between 1977 and 1990, while the proportion of capital almost trebles from 
4% to 12% over the same period. Financial liberalisation over the period drew factors of 
production into finance, both labour and capital, on a fairly dramatic scale. 
Perhaps the most striking development, however, is what happens next. These trends have 
not persisted during this century. If anything, the labour and capital shares of the financial 
sector have been on a gently declining path over this period. Growth in both labour and 
capital employed in the financial sector has been modest and has been lower than in the 
economy as a whole. Since this fall in factor input shares coincides with a period when 
measured value-added of the financial sector was rising sharply, this suggests something 
dramatic must have been happening to productivity in finance – the Solow residual. 
The measured residual, in a growth accounting sense, reflects improvements in the total 
factor productivity (TFP) of the inputs. A growth accounting decomposition suggests that 
measured TFP growth in the financial sector averaged about 2.2% per year between 1995 and 
2007 (Chart 10). This comfortably exceeds TFP growth at the whole-economy level, 
12
estimated at an average of about 0.5-1.0% over the same period. In other words, on the face 
of it at least, there is evidence of the financial sector having undergone something of a 
“productivity miracle” during this century. This pattern has not been specific to the UK. 
Measured TFP growth in the financial sector exceeded that of the whole economy across 
many developed countries between 1995-2007, a trend that accelerated in the ‘bubble’ years 
of 2003-2007 (Chart 11). 
(b) Returns to factors of production 
TFP in a growth framework is no more than an accounting residual. It provides no 
explanation of the measured productivity “miracle” in finance. A related question is whether 
the observed productivity miracle was reflected in returns to the factors of production in 
finance. Chart 12 decomposes total GVA of financial corporations into income flowing to 
labour (defined to include employees only) and income flowing to capital. Broadly speaking, 
the rise in GVA is equally split between the returns to labour (employee compensation) and 
to capital (gross operating surplus). The miracle has been reflected in the returns to both 
labour and capital, if not in the quantities of these factors employed. 
For labour, these high returns are evident both in cross-section and time-series data. Chart 13 
shows average weekly earnings across a range of sectors in the UK in 2007. Financial 
intermediation is at the top of the table, with weekly average earnings roughly double those 
of the whole-economy median. This differential widened during this century, broadly 
mirroring the accumulation of leverage within the financial sector (Chart 14). 
The time-series evidence is in some respects even more dramatic. Philippon and Reshef 
(2009) have undertaken a careful study of “excess” wages in the US financial industry since 
the start of the previous century, relative to a benchmark wage. Chart 15 plots their measure 
of excess wages. This shows a dramatic spike upwards which commenced in the early 1980s, 
but which exploded from the 1990s onwards. The only equivalent wage spike was in the runup to the Great Crash in 1929. Philippon and Reshef attribute both of these wage spikes to 
financial deregulation. 
This picture is broadly mirrored when turning from returns to labour to returns to capital. In 
the 1950s gross profitability of the financial sector relative to capital employed was broadly 
in line with the rest of the economy (Chart 16). But since then, and in particular over the past 
13
decade, returns to capital have far outpaced those at an economy-wide level. 
Chart 17 plots UK banks’ return on equity capital (ROE) since 1920 (Alessandri and Haldane 
(2009)). Although conceptually a different measure of returns to capital, the broad message 
is the same. Trends in ROE are clearly divided into two periods. In the period up until 
around 1970, ROE in banking was around 7% with a low variance. In other words, returns to 
finance broadly mimicked those in the economy as whole, in line with the gamble payoffs in 
Chart 6. But the 1970s mark a regime shift, with the ROE in banking roughly trebling to over 
20%, again in line with gamble payoffs. Excess returns accumulated to capital as well as 
labour. 
These returns were by no means unique to UK banks. Chart 18 plots ROEs for major 
internationally active banks in the US and Europe during this century. Two features are 
striking. First, the level of ROEs was consistently at or above 20% and on a rising trend up 
until the crisis. This is roughly double ROEs in the non-financial sector over the period. 
Second, the degree of cross-country similarity in these ROE profiles is striking. This, too, is 
no coincidence. During much of this period, banks internationally were engaged in a highly 
competitive ROE race. Therein lies part of the explanation for these high returns to labour 
and capital in banking. 
4. Explaining Aggregate Returns in Banking – Excess Returns and Risk Illusion
How do we explain these high, but temporary, excess returns to finance which appear to have 
driven the growing contribution of the financial sector to aggregate economic activity? In 
this section we discuss potential balance sheet strategies which may have contributed to these 
rents. Essentially, high returns to finance may have been driven by banks assuming higher 
risk. Banks’ profits, like their contribution to GDP, may have been flattered by the mismeasurement of risk. 
The crisis has subsequently exposed the extent of this increased risk-taking by banks. In 
particular, three (often related) balance sheet strategies for boosting risks and returns to 
banking were dominant in the run-up to crisis: 
 increased leverage, on and off-balance sheet; 
 increased share of assets held at fair value; and 
14
 writing deep out-of-the-money options. 
What each of these strategies had in common was that they generated a rise in balance sheet 
risk, as well as return. As importantly, this increase in risk was to some extent hidden by the 
opacity of accounting disclosures or the complexity of the products involved. This resulted 
in a divergence between reported and risk-adjusted returns. In other words, while reported 
ROEs rose, risk-adjusted ROEs did not (Haldane (2009)). 
 
To some extent, these strategies and their implications were captured to a degree in 
performance measures. For example, the rise in reported average ROEs of banks over the 
past few decades occurred alongside a rise in its variability. At the same time as average 
ROEs in banking were trebling, so too was their standard deviation (Chart 17). In that sense, 
the banking “productivity miracle” may have been, at least in part, a mirage – a simple, if 
dramatic, case of risk illusion by banks, investors and regulators. 
(a) Increased leverage 
Banks’ balance sheets have grown dramatically in relation to underlying economic activity 
over the past century. Charts 19 and 20 plot this ratio for the UK and the US over the past 
130 years. For the US, there has been a secular rise in banks’ assets from around 20% to over 
100% of GDP. For the UK, a century of flat-lining at around 50% of GDP was broken in the 
early 1970s, since when banks’ assets in relation to national income have risen tenfold to over 
500% of GDP. 
This century has seen an intensification of this growth. According to data compiled by the 
Banker, the balance sheets of the world’s largest 1000 banks increased by around 150% 
between 2001 and 2009 (Chart 21). In cross-section terms, the scale of assets in the banking 
system now dwarfs that in other sectors. Looking at the size of the largest firm’s assets in 
relation to GDP across a spectrum of industries, finance is by far the largest (Chart 22). 
The extent of balance sheet growth was, if anything, understated by banks’ reported assets. 
Accounting and regulatory policies permitted banks to place certain exposures off-balance 
sheet, including special purpose vehicles and contingent credit commitments. Even 
disclosures of on-balance sheet positions on derivatives disguised some information about 
banks’ contingent exposures. 
15
This rapid expansion of the balance sheet of the banking system was not accompanied by a 
commensurate increase in its equity base. Over the same 130 year period, the capital ratios of 
banks in the US and UK fell from around 15-25% at the start of the 20th century to around 5% 
at its end (Chart 23). In other words, on this metric measures of balance sheet leverage rose 
from around 4-times equity capital in the early part of the previous century to around 20 
times capital at the end. 
If anything, the pressure to raise leverage increased further moving into this century. 
Measures of gearing rose sharply between 2000 and 2008 among the major global banks, 
other than US commercial banks which were subject to a leverage ratio constraint (Chart 24). 
Once adjustments are made to on- and off-balance sheet assets and capital to give a more 
comprehensive cross-country picture, levels of gearing are even more striking. Among the 
major global banks in the world, levels of leverage were on average more than 50 times 
equity at the peak of the boom (Chart 25). 
For a given return on assets (RoA), higher leverage mechanically boosts a banks’ ROE. The 
decision by many banks to increase leverage appears to have been driven, at least in part, by a 
desire to maintain ROE relative to competitors, even as RoA fell. For example, as Chart 26 
illustrates, virtually all of the increase in the ROE of the major UK banks during this century 
appears to have been the result of higher leverage. Banks’ return on assets – a more precise 
measure of their productivity – was flat or even falling over this period. 
Between 1997 and 2008, as UK banks increased leverage, they managed to maintain broadly 
constant capital ratios by, on average, seeking out assets with lower risk weights (Chart 27). 
A similar pattern was evident among a number of the Continental European major global 
banks (Chart 28). It is possible to further decompose ROE to provide additional insight into 
how banks increased reported returns as follows: 
RoE = Total assets X Tier 1 capital x Net income x RWAs 
Tier 1 capital Common equity RWAs Total assets (1.1) 
RoE = Financial leverage X Common equity margin x RoRWAs x Unit-risk 
16
Banks can boost ROE by acting on any of the terms on the right-hand side of equation (1.1): 
increasing assets relative to capital (financial leverage), holding a larger proportion of capital4
other than as common equity (common equity margin), or assuming a greater degree of risk 
per unit of assets (return on risk-weighted assets, RoRWA) – leveraging assets, leveraging
capital structure or leveraging regulation. 
Table 4 shows two of the elements of this breakdown for the major global banks – leverage 
and unit risk. For most banks, the story is one of a significant increase in assets relative to 
capital, with little movement into higher risk assets (unit risk makes a negative contribution 
for most banks). Those banks with highest leverage, however, are also the ones which have 
subsequently reported the largest write-downs. That suggests banks may also have invested 
in riskier assets, which regulatory risk-weights had failed to capture. 
Table 5 looks at the third component, the common equity margin, of some of the same global 
banks. Among at least some of these banks, this margin makes a significant contribution to 
ROE growth, as banks moved into hybrid Tier 1 capital instruments at the expense of core 
equity. As such hybrid instruments have shown themselves largely unable to absorb losses 
during the crisis, this boost to ROE is also likely to have been an act of risk illusion. 
Taken together, this evidence suggests that much of the “productivity miracle” of high ROEs 
in banking appear to have been the result not of productivity gains on the underlying asset 
pool, but rather a simple leveraging up of the underlying equity in the business. 
(b) Larger trading books 
A second strategy pursued by a number of banks in the run-up to crisis was to increase their 
assets held at fair value, principally through their trading books, relative to their banking 
books of underlying loans. Among the major global banks, the share of loans to customers in 
total assets fell from around 35% in 2000 to 29% by 2007 (Chart 29). Over the same period, 
trading book asset shares almost doubled from 20% to almost 40%. These large trading 
books were associated with high leverage among the world’s largest banks (Chart 30). 
                                                           
4
 The term “Tier 1 capital” refers to the component of banks’ regulatory capital comprising common equity and 
capital instruments close to common equity (“hybrid Tier 1 capital”), as defined by rules set out by regulators. 
For a discussion of the composition of UK banks’ regulatory capital see Bank of England (2009a). 
17
What explains this shift in portfolio shares? Regulatory arbitrage appears to have been a 
significant factor. Trading book assets tended to attract risk weights appropriate for dealing 
with market but not credit risk. This meant it was capital-efficient for banks to bundle loans 
into tradable structured credit products for onward sale. Indeed, by securitising assets in this 
way, it was hypothetically possible for two banks to swap their underlying claims but for both 
firms to claim capital relief. The system as a whole would then be left holding less capital, 
even though its underlying exposures were identical. When the crisis came, tellingly losses 
on structured products were substantial (Chart 31). 
A further amplifying factor is that trading books are marked-to-market and any gains or 
losses taken through to the profit and loss account. So holding a large trading book is a very 
good strategy when underlying asset prices in the economy are rising rapidly. This was 
precisely the set of the circumstances facing banks in the run-up to crisis, with asset prices 
driven higher by a search for yield among investors. In effect, this rising tide of asset price 
rises was booked as marked-to-market profits by banks holding assets in their trading book. 
Everyone, it appeared, was a winner. 
But because these gains were driven by a mis-pricing of risk in the economy at large, trading 
book profits were in fact largely illusory. Once asset prices went into reverse during 2008 as 
risk was re-priced, trading book losses quickly materialised. Write-downs on structured 
products totalled $210 billion among the major global banks in 2008 alone. 
(c) Writing deep out-of-the-money options 
A third strategy, which boosted returns by silently assuming risk, arises from offering tail risk 
insurance. Banks can in a variety of ways assume tail risk on particular instruments – for 
example, by investing in high-default loan portfolios, the senior tranches of structured 
products or writing insurance through credit default swap (CDS) contracts. In each of these 
cases, the investor earns an above-normal yield or premium from assuming the risk. For as 
long as the risk does not materialise, returns can look riskless – a case of apparent “alpha”. 
Until, that is, tail risk manifests itself, at which point losses can be very large. 
There are many examples of banks pursuing essentially these strategies in the run-up to crisis. 
For example, investing in senior tranches of sub-prime loan securitisations is, in effect, 
18
equivalent to writing deep-out-of-the-money options, with high returns except in those tail 
states of the world when borrowers default en masse. It is unsurprising that issuance of assetbacked securities, including sub-prime RMBS (residential mortgage-backed securities), grew 
dramatically during the course of this century, easily outpacing Moore’s Law (the benchmark 
for the growth in computing power since the invention of the transistor) (Chart 32).5
 
Tranched structured products, such as CDOs (collateralised debt obligations) and CLOs 
(collateralised loan obligations), generate a similar payoff profile for investors to sub-prime 
loans, yielding a positive return in stable states of the world – apparent alpha – and a large 
negative return in adverse states. Volumes outstanding of CDOs and CLOs also grew at a 
rate in excess of Moore’s Law for much of this century. The resulting systematic mis-pricing 
of, in particular, the super-senior tranches of these securities was a significant source of 
losses to banks during the crisis, with ratings downgrades large and frequent (Chart 33). 
A similar risk-taking strategy was the writing of explicit insurance contracts against such tail 
risks, for example through CDS. These too grew very rapidly ahead of crisis (Chart 34). 
Again, the writers of these insurance contracts gathered a steady source of premium income 
during the good times – apparently “excess returns”. But this was typically more than offset 
by losses once bad states materialised. This, famously, was the strategy pursued by some of 
the monoline insurers and by AIG. For example, AIG’s capital market business, which 
included its ill-fated financial products division, reported total operating income of $2.3 
billion in the run-up to crisis from 2003 to 2006, but reported operating losses of around $40 
billion in 2008 alone. 
What all of these strategies had in common was that they involved banks assuming risk in the 
hunt for yield – risk that was often disguised because it was parked in the tail of the return 
distribution. Excess returns – from leverage, trading books and out-of-the-money options – 
were built on an inability to measure and price risk. The productivity miracle was in fact a 
risk illusion. In that respect, mis-measurement of the contribution of banking in the National 
Accounts and the mis-measurement of returns to banking in their own accounts have a 
common underlying cause. 
                                                           
5
 Moore’s Law refers to the observation by Intel co-founder Gordon Moore in 1965 that transistor density on 
integrated circuits had doubled every year since the integrated circuit was invented and the prediction that this 
would continue. 
19
5. Explaining Disaggregated Returns to Banking 
A distinct, but complementary, explanation of high returns to banking is that they reflect 
structural features of the financial sector. For example, measures of market concentration are 
often used as a proxy for the degree of market power producers have over consumers. It is 
telling that measures of the concentration of the banking sector have increased dramatically 
over the course of the past decade, coincident with the rise in banking returns. Chart 35 plots 
the share of total bank assets of the largest three banks in the US since the 1930s. Having 
flat-lined up until the 1990s, the top 3 share has since roughly tripled. A similar trend is 
evident in the UK (where the share of the top 3 banks currently stands at above 50%) and 
globally (where the share of the top 3 has doubled over the past 10 years). 
At the same time, it is well known that market concentration need not signal a lack of 
competitiveness or efficiency within an industry or sector (Wood and Kabiri (2010)). Highly 
competitive industries can be concentrated and highly decentralised industries uncompetitive. 
A better arbiter of market power may be measures of market contestability, in particular the 
potential for barriers to entry to and exit from the market. Entry and exit rates from banking 
have, historically, tended to be very modest by comparison with the non-financial sector and 
other parts of the financial sector, such as hedge funds. 
For banks operating in many markets and offering a range of services, aggregate returns may 
offer a misleading guide to the degree of market contestability. Looking separately at the 
different activities financial firms undertake provides a potentially clearer indication of the 
drivers of performance and the structural factors determining them. In this respect, JP 
Morgan Chase provides an interesting case study. 
JP Morgan Chase is a large universal bank offering a full package of banking services to 
customers, retail and wholesale. Its published accounts also provide a fairly detailed 
decomposition of the returns to these different activities. Chart 36 looks at the returns on 
equity at JP Morgan Chase, broken down by business line and over time. These estimates are 
based on the firm’s economic capital model. So provided this model adequately captures 
risk, these estimates ought to risk-adjust returns across the different business lines, allocating 
greater amounts of capital to riskier activities. 
(a) “Low risk/low return” business activities 
20
Consider first some of the activities generally perceived to be low-risk/low return – asset 
management and treasury and securities services and retail financial services. All of these 
seemingly low risk activities appear to deliver above-average returns on equity, ranging from 
a high of around 50% on treasury and asset management services to around 20%+ on retail 
financial services. 
One potential explanation of these high returns is that the risk associated with these activities, 
and hence the capital allocated to them, may be under-estimated by banks’ models. Another 
is that the demand for these services is highly price inelastic – for example, because of 
information imperfections on the part of end-users of these services. Anecdotally, there is 
certainly evidence of a high degree of stickiness in the demand for retail financial services. 
Statistically, an adult is more likely to leave their spouse than their bank. 
In a UK context, there have been a number of studies by the authorities on the degree of 
competition within retail financial services, including by the Competition Commission (2005) 
and the Office of Fair Trading (OFT) (2008). The OFT market study found a very low rate of 
switching of personal current accounts between banks – fewer than 6% per year. By itself, 
however, this low switching rate does not necessarily imply a market failure. For example, it 
could be the result of a reputational equilibrium in which money gravitates to banks whose 
brand name is recognised and respected. 
A more obvious market friction in the UK retail financial services market derives from “free 
in credit” banking. In effect, all retail payment services are charged at a zero up-front fee, 
except large-value payment transfers through CHAPS6
 (which are typically charged at around 
£25). This charging schedule is not well aligned with marginal costs. It encourages 
bundling of payment services and the charging of latent or hidden fees on other transactions 
services – for example, overdraft fees. Explicit charging for retail financial services would 
increase transparency and reduce the scope for distortions in the use of these services. 
High returns on treasury management services also present something of a puzzle. These 
include transactions, information and custodial services to clients. None of these activities 
                                                           
6
 CHAPS is the same-day electronic funds transfer system, operated by the bank-owned CHAPS Clearing 
Company, that is used for high-value/wholesale payments but also for other time-critical lower value payments 
(such as house purchase). 
21
are especially expertise-intensive and the market for these services ought in principle to be 
contestable internationally. 
(b) “High risk/high return” business activities 
The higher risk activities associated with finance, such as commercial and investment 
banking, do not on the face of it appear to yield as high returns on equity. Nonetheless these 
returns, at around 20%, are above levels in the non-financial sector. 
Investment banking activities are, in risk terms, a mixed bag. They comprise fairly low-risk 
activities, such as (merger and acquisition) M&A advisory work, with higher-risk activities 
such as securities underwriting and proprietary trading. To complicate matters, banks’ annual 
accounts data do not differentiate simply between these activities – for example, between 
market-making and proprietary trading activities in fixed income, currency and commodities 
(FICC) and equities. Chart 37 provides a revenue breakdown of US investment banks’ 
activities. 
The lack of a breakdown between client and proprietary sources of revenues is problematic 
when making sense of investment banking activities, both in the run-up to and during the 
crisis. In the run-up to crisis, FICC and equity-related activity contributed significantly to 
revenues, partly on the back of proprietary trading in assets whose prices were rising rapidly. 
Some of these gains then dissolved when asset prices, in particular for FICC, went into 
reverse during 2008. 
The story of 2009/10 is of a strong recovery in FICC and equity revenues. The source of this 
revenue recovery is, however, different to the boom. Instead of proprietary risk-taking, 
increased revenues appear instead to have been driven by market-making activities on behalf 
of clients. These were boosted by a bulge in client activity and wider bid-ask spreads, against 
a backdrop of lower levels of competition (Chart 38). It is an open question whether these 
returns to market-making will persist. 
In some respects, returns to M&A and advisory activities represent even more of a puzzle. 
For a start, it is well known that most M&A activity is value-destroying (for example, Palia 
(1995)). Advisory fees of 0.5-1.5% are typically taken, even though these activities are 
essentially risk-less. And in total under-writing fees are often around 3-4% in Europe and 
22
higher still in the US, having risen during the course of the crisis. The level and persistence 
of these fees is also something of a puzzle. 
One potential explanation is that high fees on underwriting and advisory activities are 
sustained as a reputational equilibrium. In effect, clients are willing to pay a premium to 
have bonds or equity underwritten by a recognised name, as this is a signal of quality to endinvestors. A similar phenomenon might explain the “2 and 20” fee structure of hedge funds. 
The OFT has recently announced an investigation into underwriting fees in the UK market. 
Another part of the puzzle was banks’ approach to managing risk across these business lines. 
For example, treasury functions are designed to help a firm as a whole manage its balance 
sheet, with internal transfer pricing for liquidity services to business lines. By acting in that 
way, the risk-taking incentives of each business unit can be aligned with the business as a 
whole, thereby complementing firms’ internal risk management. 
In practice, during the run-up to crisis, treasury functions were often run as a profit centre. 
That would tend to encourage two sets of risk-taking behaviour. First, it may have 
encouraged banks to take risks in balance sheet management – for example, by seeking out 
cheaper sources of capital (for example, hybrids over pure equity) or liquidity (shorter-term 
unsecured borrowing over long-term secured funding). Second, it may have led to the 
systematic under-pricing of liquidity services to banks’ business unit, fuelling excessive 
growth and/or risk-taking. Tackling these risks would require banks’ treasury operations to 
cease being profit centres and to execute effective internal transfer pricing. 
6. Conclusion 
The financial sector has undergone an astonishing roller-coaster in the course of a decade. 
The ascent to heaven and subsequent descent to hell has been every bit as dramatic as in the 
1930s. In seeking to smooth next time’s ride, prophylactic public policy has a key role to 
play. Of the many initiatives that are underway, this paper has highlighted three which may 
warrant further attention in the period head: 
 First, given its ability to both invigorate and incapacitate large parts of the non-financial 
economy, there is a strong case for seeking improved means of measuring the true valueadded by the financial sector. As it is rudimentary to its activities, finding a more 
23
sophisticated approach to measuring risk, as well as return, within the financial sector 
would seem to be a priority. The conflation of the two can lead to an overstatement of 
banks’ contribution to the economy and an understatement of the true risk facing banks 
and the economy at large. Better aggregate statistics and bank-specific performance 
measures could help better to distinguish miracles and mirages. This might include 
developing more sophisticated risk-adjustments to FISIM and a greater focus on banks’ 
return on assets rather than equity by investors and managers. 
 Second, because banks are in the risk business it should be no surprise that the run-up to 
crisis was hallmarked by imaginative ways of manufacturing this commodity, with a view 
to boosting returns to labour and capital. Risk illusion is no accident; it is there by 
design. It is in bank managers’ interest to make mirages seem like miracles. Regulatory 
measures are being put in place to block off last time’s risk strategies, including through 
re-calibrated leverage and capital ratios. But risk migrates to where regulation is weakest, 
so there are natural limits to what regulatory strategies can reasonably achieve. At the 
height of a boom, both regulators and the regulated are prone to believe in miracles. That 
is why the debate about potential structural reform of finance is important - to lessen the 
burden on regulation and reverse its descent into ever-greater intrusiveness and 
complexity. At the same time, regulators need also to be mindful of risk migrating 
outside the perimeter of regulation, where it will almost certainly not be measured. 
 Third, finance is anything but monolithic. But understanding of these different business 
lines is complicated by the absence of reliable data on many of these activities. There are 
several open questions about the some of these activities, not least those for which returns 
appear to be high. This includes questions about the risks they embody and about the 
competitive structure of the markets in which they are traded. These are issues for both 
prudential regulators and the competition authorities, working in tandem. If experience 
after the Great Depression is any guide, it seems likely that these structural issues will 
take centre-stage in the period ahead. 

The car industry is a pollutant. Exhaust fumes are a noxious by-product. Motoring benefits those 
producing and consuming car travel services – the private benefits of motoring. But it also 
endangers innocent bystanders within the wider community – the social costs of exhaust 
pollution. 
Public policy has increasingly recognised the risks from car pollution. Historically, they have 
been tackled through a combination of taxation and, at times, prohibition. During this century, 
restrictions have been placed on poisonous emissions from cars - in others words, prohibition. 
This is recognition of the social costs of exhaust pollution. Initially, car producers were in uproar. 
The banking industry is also a pollutant. Systemic risk is a noxious by-product. Banking benefits 
those producing and consuming financial services – the private benefits for bank employees, 
depositors, borrowers and investors. But it also risks endangering innocent bystanders within the 
wider economy – the social costs to the general public from banking crises. 
Public policy has long-recognised the costs of systemic risk. They have been tackled through a 
combination of regulation and, at times, prohibition. Recently, a debate has begun on direct 
restrictions on some banking activities - in other words, prohibition. This is recognition of the 
social costs of systemic risk. Bankers are in uproar. 
This paper examines the costs of banking pollution and the role of regulation and restrictions in 
tackling it. In light of the crisis, this is the $100 billion question. The last time such a debate was 
had in earnest followed the Great Depression. Evidence from then, from past crises and from 
other industries helps define the contours of today’s debate. This debate is still in its infancy. 
While it would be premature to be reaching policy conclusions, it is not too early to begin sifting 
the evidence. What does it suggest? 
Counting the Systemic Cost 
One important dimension of the debate concerns the social costs of systemic risk. Determining 
the scale of these social costs provides a measure of the task ahead. It helps calibrate the 
intervention necessary to tackle systemic risk, whether through regulation or restrictions. So how 
big a pollutant is banking? 
3 
There is a large literature measuring the costs of past financial crises.1
 This is typically done by 
evaluating either the fiscal or the foregone output costs of crisis. On either measure, the costs of 
past financial crises appear to be large and long-lived, often in excess of 10% of pre-crisis GDP. 
What about the present crisis? 
The narrowest fiscal interpretation of the cost of crisis would be given by the wealth transfer from 
the government to the banks as a result of the bailout. Plainly, there is a large degree of 
uncertainty about the eventual loss governments may face. But in the US, this is currently 
estimated to be around $100 billion, or less than 1% of US GDP. For US taxpayers, these losses 
are (almost exactly) a $100 billion question. In the UK, the direct cost may be less than £20 
billion, or little more than 1% of GDP. 
Assuming a systemic crisis occurs every 20 years, recouping these costs from banks would not 
place an unbearable strain on their finances. The tax charge on US banks would be less than $5 
billion per year, on UK banks less than £1 billion per year.2
 Total pre-tax profits earned by US 
and UK banks in 2009 alone were around $60 billion and £23 billion respectively. 
But these direct fiscal costs are almost certainly an underestimate of the damage to the wider 
economy which has resulted from the crisis – the true social costs of crisis. World output in 2009 
is expected to have been around 6.5% lower than its counterfactual path in the absence of crisis. 
In the UK, the equivalent output loss is around 10%. In money terms, that translates into output 
losses of $4 trillion and £140 billion respectively. 
Moreover, some of these GDP losses are expected to persist. Evidence from past crises suggests 
that crisis-induced output losses are permanent, or at least persistent, in their impact on the level 
of output if not its growth rate.3
 If GDP losses are permanent, the present value cost of crisis will 
exceed significantly today’s cost. 
By way of illustration, Table 1 looks at the present value of output losses for the world and the 
UK assuming different fractions of the 2009 loss are permanent - 100%, 50% and 25%. It also 
_____________________________________________________________________________ 
1
 For example, Reinhart and Rogoff (2009). 
2
 The levy on US banks announced by the US government in January takes the $100 billion loss and recoups it over 
10 years rather than 20. 
3
 IMF (2009). 
4 
assumes, somewhat arbitrarily, that future GDP is discounted at a rate of 5% per year and that 
trend GDP growth is 3%.4
 Present value losses are shown as a fraction of output in 2009. 
As Table 1 shows, these losses are multiples of the static costs, lying anywhere between one and 
five times annual GDP. Put in money terms, that is an output loss equivalent to between $60 
trillion and $200 trillion for the world economy and between £1.8 trillion and £7.4 trillion for the 
UK. As Nobel-prize winning physicist Richard Feynman observed, to call these numbers 
“astronomical” would be to do astronomy a disservice: there are only hundreds of billions of 
stars in the galaxy. “Economical” might be a better description. 
It is clear that banks would not have deep enough pockets to foot this bill. Assuming that a crisis 
occurs every 20 years, the systemic levy needed to recoup these crisis costs would be in excess of 
$1.5 trillion per year. The total market capitalisation of the largest global banks is currently only 
around $1.2 trillion. Fully internalising the output costs of financial crises would risk putting 
banks on the same trajectory as the dinosaurs, with the levy playing the role of the meteorite. 
It could plausibly be argued that these output costs are a significant over-statement of the damage 
inflicted on the wider economy by the banks. Others are certainly not blameless for the crisis. 
For every reckless lender there is likely to be a feckless borrower. If a systemic tax is to be 
levied, a more precise measure may be needed of banks’ distinctive contribution to systemic risk. 
One such measure is provided by the (often implicit) fiscal subsidy provided to banks by the state 
to safeguard stability. Those implicit subsidies are easier to describe than measure. But one 
particularly simple proxy is provided by the rating agencies, a number of whom provide both 
“support” and “standalone” credit ratings for the banks. The difference in these ratings 
encompasses the agencies’ judgement of the expected government support to banks. 
Table 2 looks at this average ratings difference for a sample of banks and building societies in the 
UK, and among a sample of global banks, between 2007 and 2009. Two features are striking. 
First, standalone ratings are materially below support ratings, by between 1.5 and 4 notches over 
the sample for UK and global banks. In other words, rating agencies explicitly factor in material 
government support to banks. 
_____________________________________________________________________________ 
4
 The results are plainly sensitive to the choice of discount rate and trend growth rate. Other things equal, the higher 
the discount rate and the lower the trend growth rate, the smaller the losses. 
5 
Second, this ratings difference has increased over the sample, averaging over one notch in 2007 
but over three notches by 2009. In other words, actions by government during the crisis have 
increased the value of government support to the banks. This should come as no surprise, given 
the scale of intervention. Indeed, there is evidence of an up-only escalator of state support to 
banks dating back over the past century.5
Table 3 takes the same data and divides the sample of UK banks and building societies into 
“large” and “small” institutions. Unsurprisingly, the average rating difference is consistently 
higher for large than for small banks. The average ratings difference for large banks is up to 5 
notches, for small banks up to 3 notches. This is pretty tangible evidence of a second recurring 
phenomenon in the financial system – the “too big to fail” problem. 
It is possible to go one step further and translate these average ratings differences into a monetary 
measure of the implied fiscal subsidy to banks. This is done by mapping from ratings to the 
yields paid on banks’ bonds;6
 and by then scaling the yield difference by the value of each banks’ 
ratings-sensitive liabilities.7
 The resulting money amount is an estimate of the reduction in banks’ 
funding costs which arises from the perceived government subsidy. 
Table 4 shows the estimated value of that subsidy for the same sample of UK and global banks, 
again between 2007 and 2009. For UK banks, the average annual subsidy for the top five banks 
over these years was over £50 billion - roughly equal to UK banks’ annual profits prior to the 
crisis. At the height of the crisis, the subsidy was larger still. For the sample of global banks, the 
average annual subsidy for the top five banks was just less than $60 billion per year. These are 
not small sums. 
Table 4 also splits UK banks and building societies into “Big 5”, “medium” and “small” buckets. 
As might be expected, the large banks account for over 90% of the total implied subsidy. On 
these metrics, the too-big-to-fail problem results in a real and on-going cost to the taxpayer and a 
real and on-going windfall for the banks. If it were ever possible to mint a coin big enough, these 
would be the two sides of it. 
These results are no more than illustrative – for example, they make no allowance for subsidies 
arising on retail deposits. Nonetheless, studies using different methods have found similarly-
_____________________________________________________________________________ 
5
 Haldane (2009a). 
6
 Using the end-year yield on the financial corporates bond index across the ratings spectrum. 
7
 For example, banks’ retail deposits are excluded but unsecured wholesale borrowing is included. 
6 
sized subsidies. For example, Baker and McArthur ask whether there is a difference in funding 
costs for US banks either side of the $100 billion asset threshold – another $100 billion question.8
 
They find a significant wedge in costs, which has widened during the crisis. They calculate an 
annual subsidy for the 18 largest US banks of over $34 billion per year. Applying the same 
method in the UK would give an annual subsidy for the five largest banks of around £30 billion. 
This evidence can provide only a rough guide to systemic scale and cost. But the qualitative 
picture it paints is clear and consistent. First, measures of the costs of crisis, or the implicit 
subsidy from the state, suggest banking pollution is a real and large social problem. Second, 
those entities perceived to be “too big to fail” appear to account for the lion’s share of this risk 
pollution. The public policy question, then, is how best to tackle these twin evils. 
Taxation and Prohibition 
To date, the public policy response has largely focussed on the role of prudential regulation in 
tackling these problems. Higher buffers of capital and liquid assets are being discussed to address 
the first problem. And add-ons to these capital and liquidity buffers for institutions posing the 
greatest systemic risk are being discussed to address the second.9
 In essence, this is a taxation 
solution to the systemic risk pollution problem.10
There is a second approach. On 21 January 2010, US President Barack Obama proposed placing 
formal restrictions on the business activities and scale of US banks. Others have made 
complementary proposals for structural reform of banking.11 Typically, these involve separation 
of bank activities, either across business lines or geographies. In essence, this is the prohibition
solution to the systemic pollution problem. 
This sets the scene for a great debate. It is not a new one. The taxation versus prohibition 
question crops up repeatedly in public choice economics. For centuries it has been central to the 
international trade debate on the use of quotas versus subsidies. During this century, it has 
become central to the debate on appropriate policies to curtail carbon emissions.12 
_____________________________________________________________________________ 
8
 Baker and McArthur (2009). 
9
 Basel Committee on Banking Supervision (2009). 
10 For example, Brunnermeier et al (2009), NYU Stern School of Business (2009). 11 For example, Kay (2009), Kotlikoff (2010). 
12 Stern (2006). 
7 
In making these choices, economists have often drawn on Martin Weitzman’s classic public 
goods framework from the early 1970s.13 Under this framework, the optimal amount of pollution 
control is found by equating the marginal social benefits of pollution-control and the marginal 
private costs of this control. With no uncertainty about either costs or benefits, a policymaker 
would be indifferent between taxation and restrictions when striking this cost/benefit balance. 
In the real world, there is considerable uncertainty about both costs and benefits. Weitzman’s 
framework tells us how to choose between pollution-control instruments in this setting. If the 
marginal social benefits foregone of the wrong choice are large, relative to the private costs 
incurred, then quantitative restrictions are optimal. Why? Because fixing quantities to achieve 
pollution control, while letting prices vary, does not have large private costs. When the marginal 
social benefit curve is steeper than the marginal private cost curve, restrictions dominate. 
The results flip when the marginal cost/benefit trade-offs are reversed. If the private costs of the 
wrong choice are high, relative to the social benefits foregone, fixing these costs through taxation 
is likely to deliver the better welfare outcome. When the marginal social benefit curve is flatter 
than the marginal private cost curve, taxation dominates. So the choice of taxation versus 
prohibition in controlling pollution is ultimately an empirical issue. 
To illustrate the framework, consider the path of financial regulation in the US over the past 
century. The US announcements in January are in many respects redolent of US financial 
reforms enacted during the late 1920s and early 1930s. Then, restrictions were imposed on both 
bank size and scope, in the form of the McFadden (1927) and Glass-Steagall (1933) Acts. The 
history of both, viewed through Weitzman’s lens, is illuminating for today’s debate. 
The McFadden Act (1927) in the US gave nationally-chartered banks broadly the same branching 
rights as state banks within the state. But it also confirmed the effective prohibition on national 
banks opening new branches across state lines that had previously been implicit in the US 
National Banking Act (1864). It covered a wide range of banking functions, including deposittaking and brokerage. 
The motivation behind the Act appears to have been in part political, reflecting lobbying by small 
unit banks under threat from larger competitors. But it also had an economic dimension, as a 
_____________________________________________________________________________ 
13 Weitzman (1974). 
8 
check on the dangers of “excessive concentration of financial power”.14 The same too-big-to-fail 
arguments are of course heard today, though the concerns then were competition rather than 
crisis-related ones. Weitzman’s marginal social benefit curve was perceived to be steep, made so 
by state-level competition concerns. 
McFadden appeared to be fairly effective in limiting the size of US banks from the 1930s right 
through to the mid-1970s. Over this period, the average asset size of US banks in relation to 
nominal GDP was roughly flat (Chart 1). As recently as the early 1980s, it was still at around its 
level at the time of the Great Depression. 
The 1980s marked a watershed, with interstate branching restrictions progressively lifted. States 
began to open their borders to out-of-state Bank Holding Companies (BHCs). The 1982 Garn-St 
Germain Act allowed any BHC to acquire failed banks and thrifts, regardless of the state law. 
Finally, the Riegle-Neal Act of 1994, which took effect in 1997, largely lifted restrictions on 
interstate branching for both domestic BHCs and foreign banks. 
The rationale for this change of heart was a mirror-image of the 1920s. Large banks convinced 
politicians of the high private costs of restrictions, which inhibited the efficiency of their offering 
to the public. In Weitzman’s framework, private costs trumped social benefits. The effects of the 
removal of interstate restrictions were dramatic. The average size of US banks, relative to GDP, 
has risen roughly threefold over the past 20 years (Chart 1). Too-big-to-fail was reborn in a new 
guise. 
The US Banking Act (1933) was co-sponsored by Senator Carter Glass and Representative Henry 
Steagall – hence “Glass-Steagall”. It prevented commercial banks from conducting most types of 
securities business, including principal trading, underwriting and securities lending. It also 
banned investment banks from taking deposits. The key functions of commercial and investment 
banking were effectively prised apart. 
The Act was motivated by stability concerns in the light of the Great Depression. The stock 
market boom of the 1920s had been fuelled by cheap credit from the banks. The stock market 
crash of 1929 brought that, and a great many US banks, to a shuddering halt. Among many 
banks, net losses on securities were as great as losses on loans. These losses transmitted to the 
real economy through a collapse in lending, whose stock halved between 1929 and 1933. 
_____________________________________________________________________________ 
14 Chapman and Westerfield (1942). 
9 
Against this economic backdrop, and amid heated banker-bashing, it is easy to see how the social 
benefits of segregation were perceived as far outweighing the private costs at the time. Kennedy 
(1973) describes how “Stock dealings which had made bankers rich and respected in the era of 
affluence now glared as scarlet sins in the age of depression. Disillusionment with speculators 
and securities merchants carried over from investment bankers to commercial bankers; the two 
were often the same, and an embittered public did not care to make fine distinctions”. Glass and 
Steagall made just such a distinction. They underpinned it with legislation, signed by President 
Roosevelt in June 1933. 
As with McFadden, Glass-Steagall appears to have been effective from the 1930s right up until 
the latter part of the 1980s. Measures of concentration in the US banking system remained 
broadly flat between the 1930s and the late 1980s (Chart 2). But competitive pressures were 
building from the late 1970s onwards. Strains on US commercial banks intensified from 
alternative lending vehicles (such as mutual funds and commercial paper markets) and from 
overseas banks. The private costs of restrictions were rising. 
Legislators responded. After 1988, securities affiliates within BHCs were permitted, though were 
still subject to strict limits. In 1999, the Gramm-Leach-Bliley Act revoked the restrictions of 
Glass-Steagall, allowing co-mingling of investment and commercial banking. This came as a 
specific response to the perceived high private costs of restrictions relative to the perceived social 
benefits – again, in a reversal of the Weitzman calculus from the early 1930s. 
As with size, the effects of liberalisation on banking concentration were immediate and dramatic. 
The share of the top three largest US banks in total assets rose fourfold, from 10% to 40% 
between 1990 and 2007 (Chart 2). A similar trend is discernible internationally: the share of the 
top five largest global banks in the assets of the largest 1000 banks has risen from around 8% in 
1998 to double that in 2009. 
This degree of concentration, combined with the large size of the banking industry relative to 
GDP, has produced a pattern which is not mirrored in other industries. The largest banking firms 
are far larger, and have grown far faster, than the largest firms in other industries (Chart 3). With 
the repeal of the McFadden and Glass-Steagall Acts, the too-big-to-fail problem has not just 
returned but flourished. 
10 
In the light of the Great Recession, and the large apparent costs of too-big-to-fail, does 
Weitzman’s cost-benefit calculus suggest there is a case for winding back the clock to the reforms 
of the Great Depression? Determining that requires an assessment of the benefits and costs of 
restrictions. 
The Benefits of Prohibition 
The potential benefits of restricting activity in any complex adaptive system, whether financial or 
non-financial, can roughly be grouped under three headings: modularity, robustness and 
incentives. Each has a potentially important bearing on systemic resilience and hence on the 
social benefits of restrictions. 
(a) Modularity 
In 1973, Nobel-prizing winning economist Robert Merton showed that the value of a portfolio of 
options is at least as great as the value of an option on the portfolio.15 On the face of it, this seems 
to fly in the face of modern portfolio theory, of which Merton himself was of course one of the 
key architects. Whatever happened to the benefits of portfolio diversification? 
The answer can be found in an unlikely source – Al’Qaeda. Although the precise organisational 
form of Al’Qaeda is not known with certainty, two structural characteristics are clear. First, it 
operates not as a centralised, integrated organisation but rather as a highly decentralised and loose 
network of small terrorist cells. Second, as events have shown, Al’Qaeda has exhibited 
considerable systemic resilience in the face of repeated and on-going attempts to bring about its 
collapse. 
These two characteristics are closely connected. A series of decentralised cells, loosely bonded, 
make infiltration of the entire Al’Qaeda network extremely unlikely. If any one cell is 
incapacitated, the likelihood of this undermining the operations of other cells is severely reduced. 
That, of course, is precisely why Al’Qaeda has chosen this organisational form. Al’Qaeda is a 
prime example of modularity and its effects in strengthening systemic resilience. 
There are many examples from other industries where modularity in organisational structure has 
been deployed to enhance systemic resilience. Computer manufacture is one. During the late 
_____________________________________________________________________________ 
15 Merton (1973). 
11 
1960s, computers were highly integrated systems. Gradually, they evolved into the quintessential 
modular system of today, with distinct modules (CPU, hard disk, keyboard) which were 
replaceable if they failed without endangering the functioning of the system as a whole. This 
improved resilience and reliability. 
In the computing industry, modularity appears to have had an influence on industry structure. 
Since the 1970s, the computer hardware industry has moved from a highly concentrated structure 
to a much more fragmented one. In 1969, IBM had a market share of over 70%. By this century, 
the market share of the largest hardware firm was around a third of that. Modularity has meant 
the computer industry has become less prone to “too-big-to-fail” problems. 
Other examples of modularity in organisational structures include: 
• The management of forest fires, which typically involves the introduction of firebreaks to 
control the spread of fire;16
• The management of utility services, such as water, gas and electricity, where the network 
often has built-in latencies and restrictions to avoid overload and contagion; 
• The management of infectious diseases which these days often involves placing 
restrictions on travel, either within a country (as in the case of foot-and-mouth disease in 
the UK) or outside of it (as in the case of H5N1);17
• The control of computer viruses across the world wide web, which is typically achieved 
by constructing firewalls which restrict access to local domains; 
• Attempts on the world domino toppling record, which involve arranging the dominos in 
discrete blocks to minimise the risk of premature cascades. 
These are all examples where modular structures have been introduced to strengthen system 
resilience. In all of these cases, policy intervention was required to affect this change in structure. 
The case for doing so was particularly strong when the risk of viral spread was acute. In some 
cases, intervention followed specific instances of systemic collapse. 
The North American electricity outage in August 2003 affected 55 million people in the US and 
Canada. It had numerous adverse knock-on effects, including to the sewage system, telephone 
and transport network and fuel supplies. A number of people are believed to have died as a 
_____________________________________________________________________________ 
16 Carlson and Doyle (1999). 
17 Kelling et al (2003). 
12 
consequence. This event led to a rethinking of the configuration of the North American 
electricity grid, with built-in latencies and stricter controls on power circulation. 
In the mid-1980s, an attempt on the world domino-toppling record – at that time, 8000 dominos - 
had to be abandoned when the pen from one of the TV film crew caused the majority of the 
dominos to cascade prematurely. Twenty years later a sparrow disturbed an attempt on the world 
domino-toppling record. Although the sparrow toppled 23,000 dominos, 750 built-in gaps 
averted systemic disaster and a new world record of over 4 million dominos was still set. No-one 
died, except the poor sparrow which (poetically if controversially) was shot by bow and arrow. 
So to banking. It has many of the same basic ingredients as other network industries, in particular 
the potential for viral spread and periodic systemic collapse. For financial firms holding asset 
portfolios, however, there is an additional dimension. This can be seen in the relationship 
between diversification on the one hand and diversity on the other.18 The two have quite different 
implications for resilience. 
In principle, size and scope increase the diversification benefits. Larger portfolios ought to make 
banks less prone to idiosyncratic risk to their asset portfolio. In the limit, banks can completely 
eradicate idiosyncratic risk by holding the market portfolio. The “only” risk they would face is 
aggregate or systematic risk. 
But if all banks are fully diversified and hold the market portfolio, that means they are all, in 
effect, holding the same portfolio. All are subject to the same systematic risk factors. In other 
words, the system as a whole lacks diversity. Other things equal, it is then prone to generalised, 
systemic collapse. Homogeneity breeds fragility. In Merton’s framework, the option to default 
selectively through modular holdings, rather than comprehensively through the market portfolio, 
has value to investors. 
The precise balance between diversification and diversity depends on banks’ balance sheet 
configuration. What does this suggest? Charts 4 and 5 plot the income variability of a set of 24 
global banks against their asset size and a measure of the diversity of their business model.19 
There is no strong relationship between either size or diversity and income volatility. If anything 
_____________________________________________________________________________ 
18 Beale et al (2009). 19 A Herfindahl-Hirschman index of revenue concentration is constructed to measure diversification, with a measure 
of zero meaning that the HHI = 1, i.e. revenue is concentrated solely on one activity. Revenue concentration is 
calculated across three buckets for the last pre crisis year (2006) - Retail and commercial banking; Corporate and 
investment banking; Asset and wealth management. 
13 
the relationship is positively sloped, with size and diversity increasing income variability, not 
smoothing it. 
Charts 6 and 7 look at banks’ experience during the crisis. Size and diversity are plotted against 
banks’ write-downs (per unit of assets). Again, if anything, these relationships are positively 
sloped, with larger, more diversified banks suffering proportionally greater losses. This is 
consistent with evidence from econometric studies of banking conglomerates which has found 
that larger banks, if anything, exhibit greater risk due to higher volatility assets and activities.20 
This evidence is no more than illustrative. But it suggests that, in the arm wrestle between 
diversification and diversity, the latter appears to have held the upper hand. Bigger and broader 
banking does not obviously appear to have been better, at least in a risk sense. In banking, as on 
many things, Merton may have had it right. 
(b) Robustness 
The Merton result holds in a world in which investors form judgements based on knowledge of 
the distribution of risk. But in complex dynamic systems, the distribution of risk may be lumpy 
and non-linear, subject to tipping points and discontinuities.21 Faced with this, the distribution of 
outcomes for the financial system as a whole may well be incalculable. The financial system may 
operate in an environment of uncertainty, in the Knightian sense, as distinct from risk. 
There is a literature on how best to regulate systems in the face of such Knightian uncertainty.22 
It suggests some guideposts for regulation of financial systems. First, keep it simple. Complex 
control of a complex system is a recipe for confusion at best, catastrophe at worst. Complex 
control adds, not subtracts, from the Knightian uncertainty problem. The US constitution is four 
pages long. The recently-tabled Dodd Bill on US financial sector reform is 1,336 pages long. 
Which do you imagine will have the more lasting impact on behaviour. 
Second, faced with uncertainty, the best approach is often to choose a strategy which avoids the 
extreme tails of the distribution. Technically, economists call this a “minimax” strategy – 
minimising the likelihood of the worst outcome. Paranoia can sometimes be an optimal strategy. 
This is a principle which engineers took to heart a generation ago. It is especially evident in the 
_____________________________________________________________________________ 
20 De Nicolo (2000). 
21 Haldane (2009b). 
22 See, for example, Aikman et al (2010). 
14 
aeronautical industry where air and space disasters acted as beacons for minimax redesign of 
aircraft and spaceships. 
Third, simple, loss-minimising strategies are often best achieved through what economists call 
“mechanism design” and what non-economists call “structural reform”. In essence, this means 
acting on the underlying organisational form of the system, rather than through the participants 
operating within it. In the words of economist John Kay, it is about regulating structure not 
behaviour.23 
Taken together, these three features define a “robust” regulatory regime – robust to uncertainties 
from within and outside the system. Using these robustness criteria, it is possible to assess 
whether restrictions might be preferable to taxation in tackling banking pollution. To illustrate 
this, contrast the regulatory experience of Glass-Steagall (a restrictions approach) and Basel II (a 
taxation approach). 
Glass-Steagall was simple in its objectives and execution. The Act itself was only 17 pages long. 
Its aims were shaped by an extreme tail event (the Great Depression) and were explicitly minimax 
(to avoid a repetition). It sought to achieve this by acting directly on the structure of the financial 
system, quarantining commercial bank and brokering activities through red-line regulation. In 
other words, Glass-Steagall satisfied all three robustness criteria. And so it proved, lasting well 
over half a century without a significant systemic event in the US. 
The contrast with Basel II is striking. This was anything but simple, comprising many thousands 
of pages and taking 15 years to deliver. It was calibrated largely to data drawn from the Great 
Moderation, a period characterised by an absence of tail events - more minimin than minimax. 
Basel II was underpinned by a complex menu of capital risk weights. This was fine-line, not redline, regulation. In short, Basel II satisfied few of the robustness criteria. And so it proved, 
overwhelmed by the recent crisis scarcely after it had been introduced. 
(c) Incentives 
Tail risk within some systems is determined by God – in economist-speak, it is exogenous. 
Natural disasters, like earthquakes and floods, are examples of such tail risk. Although 
_____________________________________________________________________________ 
23 Kay (2009). 
15 
exogenous, even these events have been shown to occur more frequently than a normal 
distribution would imply.24 God’s distribution has fat tails. 
Tail risk within financial systems is not determined by God but by man; it is not exogenous but 
endogenous. This has important implications for regulatory control. Finance theory tells us that 
risk brings return. So there are natural incentives within the financial system to generate tail risk 
and to avoid regulatory control. In the run-up to this crisis, examples of such risk-hunting and 
regulatory arbitrage were legion. They included escalating leverage, increased trading portfolios 
and the design of tail-heavy financial instruments.25
The endogeneity of tail risk in banking poses a dilemma for regulation. Putting uncertainties to 
one side, assume the policymaker could calibrate perfectly tail risk in the system today and the 
capital necessary to insure against it. In an echo of the 1979 Madness song, banks would then 
have incentives to position themselves “One Step Beyond” the regulatory buffer to harvest the 
higher returns that come from assuming tail risk. They do so safe in the knowledge that the state 
will assume some of this risk if it materialises. Tail risk would expand to exhaust available 
resources. Countless crises have testified to this dynamic. 
This dynamic means it is hazardous to believe there is a magic number for regulatory ratios 
sufficient to insure against tail risk in all states of the world. Because tail risk is created not 
endowed, calibrating a capital ratio for all seasons is likely to be, quite literally, pointless – 
whatever today’s optimal regulatory point, risk incentives mean that tomorrow’s is sure to be 
different. 
In response, some economists have proposed corner solutions to the systemic risk problem – in 
effect, radical structural redesign. Starting with Irving Fisher in the 1930s, some have proposed 
narrow banks with a 100% liquid asset ratio to protect the liquidity services banks provide. 26 
Others have proposed mutual fund banks with a 100% equity ratio to safeguard banks’ solvency.27 
These limiting solutions are proof to risk incentives. The one guaranteed safe hiding place for the 
risk-fearing policymaker is the corner. 
One criticism of these proposals is that they might raise materially the cost of capital to banks and 
hence to the real economy. For example, 100% capital ratios could cause the economy-wide cost 
_____________________________________________________________________________ 
24 Korup and Clague (2009). 
25 Haldane (2009a) discusses some of these strategies in greater detail and the payoffs they generate. 
26 Kay (op.cit.) 
27 Kotlikoff (2010). 
16 
of capital to sky-rocket given the premium charged for equity over debt finance. This same 
argument is frequently heard in debates about more modest rises in banks’ capital ratios. But 
there are good counter-arguments that need also to be weighed. 
By lowering risk, higher levels of equity ought to lower banks’ cost of debt finance. Indeed, in a 
frictionless world Modigliani and Miller famously showed that this effect would fully offset the 
higher cost of equity, thereby leaving the total cost of capital for banks unchanged.28 In other 
words, the cost of capital for a bank may be unaffected by its capital structure, at least when 
distortions in the economy are small. Even when they large, some offset in debt costs is likely. 
It is possible to go one step further and argue that higher bank capital ratios could potentially 
lower banks’ cost of capital. The size of the premium demanded by holders of equity is a longstanding puzzle in finance – the equity premium puzzle.29 Robert Barro has suggested this puzzle 
can be explained by fears of extreme tail events.30 And what historically has been the single 
biggest cause of those tail events? Banking crises. Boosting banks’ capital would lessen the 
incidence of crises. If this lowered the equity premium, as Barro suggests, the cost of capital in 
the economy could actually fall. 
The Costs of Prohibition 
Turning to the other side of the equation, what does existing evidence tell us about the costs to 
banks of restrictions, whether on the scale or scope of their activities? In Weitzman’s framework, 
how significant are the private costs of restrictions? Fortunately, there is a reasonably rich 
empirical literature on economies of scale and scope in banking. 
(a) Economies of Scale 
On economies of scale, the literature tends either to look at the cross-sectional efficiency of banks 
of different sizes, or the time-series efficiency of banks either side of a merger. As it turns out, 
both roads reach the same destination. Economies of scale appear to operate among banks with 
assets less, perhaps much less, than $100 billion. But above that threshold there is evidence, if 
anything, of diseconomies of scale. The Weitzman marginal private cost curve is U-shaped.31 
_____________________________________________________________________________ 
28 Modigliani and Miller (1958). See also Miles (2009). 
29 Mehra and Prescott (1985). 
30 Barro (2006). 
31 Santomero and Eckles (2000). 
17 
Experience in the US following the McFadden Act suggests size in banking can bring benefits. 
Over 9,000 US banks failed during the Great Depression, the majority of which were unit banks. 
Friedman and Schwarz (1963) blame the absence of bank branching for the high failure rate 
among US banks. The costs of limited branching were also felt well after the Great Depression in 
higher-cost provision of banking services, in particular for larger companies using lending 
syndicates to finance large-scale investment.32
US experience after McFadden chimes with cross-country evidence drawn, in particular, from 
developing countries. For example, using a dataset of 107 countries, Barth et al (2004) assess the 
effects of restrictions on the efficiency and stability of the financial system. They find evidence 
that restrictions are damaging to both, in particular barriers to foreign bank entry. 
Do those arguments resonate within advanced country banking systems today? Two 
comprehensive studies in the mid-1990s found that economies of scale in banking are exhausted 
at relatively modest levels of assets, perhaps between $5-10 billion.33 A more recent 2004 survey 
of studies in both the US and Europe finds evidence of a similar asset threshold.34 Even once 
allowance is made for subsequent balance sheet inflation, this evidence implies that economies of 
scale in banking may cease at double-digit dollar billions of assets. 
Evidence from banking mergers offers little more encouragement. There is no strong evidence of 
increased bank efficiency after a merger or acquisition.35 And there is little to suggest crossactivity mergers create economic value.36 That rather chimes with recent crisis experience. Of 
the bank mergers and acquisitions which have taken place recently, the majority have resulted in 
the merged firm under-performing the market in the subsequent period. Of course, all 
econometric studies have their limitations so these results do not close the case. Nonetheless, the 
uniformity of the evidence is striking. 
(b) Economies of Scope 
Turning from economies of scale to economies of scope, the picture painted is little different. 
Evidence from US bank holding companies suggests that diversification gains from multiple 
business lines may be more than counter-balanced by heightened exposures to volatile income-
_____________________________________________________________________________ 
32 For example, Calomiris and Hubbard (1995). 
33 Saunders (1996), Berger and Mester (1997). 
34 Amel et al (2004). 35 For example, Berger and Humphrey (1997) based on a survey of over 100 studies. 
36 For example, De Long (2001). 
18 
generating activities, such as trading.37 This mirrors the evidence from Charts 4 and 5 and from 
the Great Depression. Internationally, a recent study of over 800 banks in 43 countries found a 
conglomerate “discount” in their equity prices.38 In other words, the market assigned a lower 
value to the conglomerate than the sum of its parts, echoing Merton’s 1973 insight. This is 
evidence of diseconomies of scope in banking. 
On the face of it, these findings are a puzzle. The most likely cause was articulated by Austin 
Robinson back in the 1930s – “Man’s mind and man’s memory is essentially a limited 
factor…Every increase in size beyond a point must involve a lengthening of the chain of 
authority…at some point the increasing costs of co-ordination must exceed the declining 
economies”.39 Oliver Williamson’s “span of control” theory of organisations made rigorous this 
intuition thirty years later. 
The essence of these arguments is that limits on the optimal size and scope of firms may be as 
much neurological as technological. Numbers of synapses may matter more than numbers of 
servers. The history of military units provides a good illustration. In Roman times, the optimal 
size of a military unit was 100 – hence the Roman centurion. This was the maximum number of 
men a general felt able to know well enough to lead and control. The constraint was neurological. 
Two millennia have passed. Extraordinary advances have been made in military 
telecommunications technology. And the optimal size of the military unit in the US army today? 
Just under 100 people.40 The number of relationships humans are felt able to maintain is believed 
to lie below 150 - so-called Dunbar’s Law.41 For most of us, it is single-digits. That number has 
been roughly the same since the dawn of time, despite the extra-ordinary recent advance of 
technology and social networks. As Nicholas Christakis has observed, Facebook “friends” are not 
really your friends. 
With hindsight, this crisis has provided many examples of failures rooted in an exaggerated sense 
of knowledge and control. Risks and counterparty relationships outstripped banks’ ability to 
manage them. Servers outpaced synapses. Large banks grew to comprise several thousand 
distinct legal entities. When Lehman Brothers failed, it had almost one million open derivatives 
_____________________________________________________________________________ 
37 Stiroh and Rumble (2006). 
38 Laeven and Levine (2007); see also Schmid and Walter (2009) for recent US evidence. 
39 Robinson (1934). 
40 Christakis and Fowler (2009). 
41 Dunbar (1993). 
19 
contracts – the financial equivalent of Facebook friends. Whatever the technology budget, it is 
questionable whether any man’s mind or memory could cope with such complexity. 
To sum up, the maximum efficient scale of banking could be relatively modest. Perhaps it lies 
below $100 billion. Experience suggests there is at least a possibility of diseconomies of scale 
lying in wait beyond that point. Conglomerate banking, while good on paper, appears to be more 
mixed in practice. If these are not inconvenient truths, they are at least sobering conjectures. 
They also sit awkwardly with the current configuration of banking. 
In 2008, 145 banks globally had assets above $100 billion, most of them universal banks 
combining multiple business activities. Together, these institutions account for 85% of the assets 
of the world’s top 1000 banks ranked by Tier 1 capital. If these institutions could be resolved 
easily, so that the systemic consequences of their failure were limited, efficiency considerations 
could perhaps be set to one side. Or, put in Weitzman’s terms, the social benefits of cutting banks 
down to size would be low. 
But crisis experience has demonstrated that the apparatus does not currently exist to resolve safely 
these institutions. There are no examples during this crisis of financial institutions beyond $100 
billion being resolved without serious systemic spillovers.42 Instead, those in trouble have been 
bailed-out. The same 145 institutions account for over 90% of the support offered by 
governments during the course of the crisis. 
In the light of the crisis, and in the language of Weitzman, the marginal social benefits of 
restrictions could be greater than the marginal private costs. The maximum efficient scale of 
banking may lie below the maximum resolvable scale. A large part of the effort of the 
international community over the past few years has been directed at increasing the maximum 
resolvable scale of banks – for example, through improved resolution regimes and living wills.43 
If successful, that effort would shift the balance of the Weitzman cost/benefit calculus in the 
direction of bigger banks; it could help achieve the modularity, robustness and better aligned 
incentives which restrictions otherwise deliver. 
But if this effort is unsuccessful, past evidence and present experience pose a big question about 
existing banking structures. Against that backdrop, it is understandable that restrictions on scale 
_____________________________________________________________________________ 
42 Washington Mutual, with assets of around $300bn, was resolved by FDIC, but was perceived by many to have 
caused systemic spillovers. 
43 For example, Tucker (2010). 
20 
and activity are part of today’s debate about solutions to the systemic pollution problem. $100 
billion may not just be the question; it may also be part of the answer. 
Conclusion 
We are at the start of a great debate on the future structure of finance, not the end. Some fear that 
momentum for radical financial reform will be lost. But financial crises leave a scar. This time’s 
sovereign scar should act as a lasting reminder of the criticality of reform. Today’s crisis has 
stretched some state’s sinews to the limit. Both literally and metaphorically, global finance 
cannot afford another. 
The history of banking is that risk expands to exhaust available resources. Tail risk is bigger in 
banking because it is created, not endowed. For that reason, it is possible that no amount of 
capital or liquidity may ever be quite enough. Profit incentives may place risk one step beyond 
regulation. That means banking reform may need to look beyond regulation to the underlying 
structure of finance if we are not to risk another sparrow toppling the dominos. 
Today’s financial structure is dense and complex, like a tropical rainforest. Like the rainforests, 
when it works well it is a source of richness. Yet it is, as events have shown, at the same time 
fragile. Simpler financial eco-systems offer the promise of greater robustness, at some cost in 
richness. In the light of a costly financial crisis, both eco-systems should be explored in seeking 
answers to the $100 billion question. 

Recently, the global financial system has experienced its own dramatic thunderstorm. 
Financial markets have been thick with fog and filthy air. At times, lightening strikes 
have threatened seizure in some financial markets and institutions. Two years on, the 
rumbles of thunder are still discernable. 
The debate on the causes and consequences of this perfect storm will swirl around for 
many years to come. At the centre of this storm is, on the face of it, a rather basic 
question: how should the instruments that make up the financial system be valued? 
So basic a question ought not to be a matter of life and death. But for a great many 
financial institutions during this crisis, it would have been precisely that. 
The fundamental concept on which this debate hinges is fair value. Like beauty, its 
meaning lies in the eyes of the beholder. For some, “fair is foul” – the application of 
fair value principles risks exposing financial firms to the vagaries of markets, fog, 
filthy air and all. For others, ignoring the signals from financial market is itself foul 
and risks creating a financial landscape that is foggy and anything but fair. 
The fair value debate is generating electricity in the usually static-free professions of 
accountancy and regulation. Bankers fulminate at the mere mention. Among Heads 
of State in some of the biggest countries in the world, accounting standards for 
derivatives have generated levels of fear and consternation usually reserved for nonfinancial weapons of mass destruction. 
Against that backdrop, this paper attempts to shed a little light on this heated debate. 
3
Three Phases of Fair Value 
So what lies at the heart of this debate? It is well-captured by Preston Delano, US 
Comptroller of the Currency: 
“…the soundness of the banking system depends upon the soundness of the 
country’s business and industrial enterprises, and should not be measured by 
the precarious yardstick of current market quotations which often reflect 
speculative and not true appraisals of intrinsic worth”.1
Delano was US Comptroller of the Currency in 1938. This provides a clue to the fact 
that the fair value debate is not a new one. To understand this debate, its origins and 
undulations, it is worth starting at the very beginning. 
Although book-keeping has far earlier antecedents, modern accountancy is believed to 
have begun in the Italian cities of Genoa, Venice and Florence in the 14th century. It 
is no coincidence that modern banking emerged at precisely the same time in 
precisely the same cities. Banks emerged to service rapidly expanding commercial 
companies. And double-entry book-keeping became an essential means of recording 
and tracking who owed what to whom, oiling the wheels of finance. 
It is no coincidence, too, that the first-known description of accountancy was provided 
by an Italian, Luca Pacioli, in the late 15th century.2
 Pacioli was not your typical 
accountant. A wandering Franciscan monk, tutor and mathematician, he was friend 
and sometime collaborator of Leonardo de Vinci. Although comfortably the less 
famous of the two, Pacioli is still known today as the father of modern accounting. 
From those beginnings, double-entry began to spread north within Europe during the 
Middle Ages: to Germany in the 15th century, Spain and England in the 16th century 
and Scotland in the 17th century. By the late 18th century, Goethe had called double-
 
1
 Revision in Bank Examination Procedure and in the Investment Securities Regulation of the 
Comptroller of the Currency, Federal Reserve Bulletin, July, 1938, pages 563-564. 
2
 Pacioli (1494). 
4
entry “among the finest inventions of the human mind”.3
 Some people are easily 
impressed. Despite that, the progress of double-entry was surprisingly slow. At the 
start of the 19th century, there were only eleven Londoners who listed their occupation 
as “accomptants”. Imagine. 
The 19th century marked a turning point. In the UK, joint stock companies began to 
spring up. The Bankruptcy Act of 1831 gave accountants a role in winding-up 
enterprises and the Companies Acts of 1844 and 1862 established a legal requirement 
for companies to register and file accounts. By the end of the century, audit practices 
were becoming established. The accountant’s role was to provide a true and fair view 
of a company’s assets and income, as protection for the state (to whom it paid taxes) 
and investors (to whom it paid dividends). 
It was these concerns that led to the gradual emergence during the second half of the 
19th century of fair-value based accounting conventions in the US. From the late 19th
century, banks’ securities were carried at market values and their fixed assets at 
“appraised values”. In other words, by the early 20th century fair value principles 
were widely applied to companies in general and to banks in particular. In many 
respects, this period may have been the high-water mark for fair value principles. 
In the US, this first wave of the fair value debate ended in 1938.4
 The backdrop was 
inauspicious. The first phase of the Great Depression, between 1929 and 1933, saw 
the failure of a large number of US banks. Between 1933 and 1937, the US economy 
recovered somewhat. But by 1938 there were fears of a double dip. At the Fed’s 
prompting, Franklin D Roosevelt called a convention comprising the US Treasury, the 
Federal Reserve Board, the Comptroller of the Currency and the Federal Deposit 
Insurance Corporation (FDIC). Its purpose was to determine what should be done 
with prudential standards to safeguard recovery. 
This was no ordinary regulatory convention. Marriner S Eccles, Chairman of the 
Federal Reserve, called it “guerrilla warfare”. In one corner were the regulators, the 
Comptroller of the Currency and FDIC. Scarred by their regulatory experience, and 
 
3
 Goethe (1796). 4
 Simonsen and Hempel (1993) provide a fascinating account of this episode.
5
fearing further bank failures, the Comptroller and FDIC pushed for high prudential 
standards, including preservation of fair values for banks’ assets. In the other corner 
was the Fed. Scarred by their monetary policy experience, and fearing a further 
collapse in lending, the Fed argued for laxer prudential standards and the 
abandonment of fair values. Battle commenced. 
The tussle lasted two months, often played out in public through the New York Times. 
In the end, the Fed prevailed. On 26 June 1938, Franklin D Roosevelt announced 
(without so much as a hint of irony) the Uniform Agreement on Bank Supervisory 
Procedures. Banks’ investment grade assets were to be valued not at market values 
but at amortised cost. And banks’ sub-investment grade assets were to be valued at a 
long-run average of market prices. In the teeth of crisis, and in the interests of 
macroeconomic stability, the first phase of fair value had ended. 
This pattern was to be repeated half a century later – the second wave of fair value. 
Historic cost accounting remained in the ascendancy in the US from the 1940s right 
though to the early 1970s. But from the mid-1970s onwards, accounting standardsetters began to embrace fair value measurement, first in the context of banks’ 
portfolios of equities and other marketable securities.5
 By the late 1980s, there was 
widespread recognition that traditional accounting approaches were obscuring the real 
value of securities and derivatives. 
US experience during the Savings and Loan crisis in the mid-1980s provided further 
impetus. Forbearance, including about the valuation of assets and liabilities, was 
widely believed to have been a cause of the build-up of problems among the thrifts.6
 
In 1989, Congress passed the Financial Institutions Recovery, Reform and 
Enforcement Act (FIRREA), tightening valuation standards among banks and 
bringing them closer to fair values. In the same year, the International Accounting 
Standards Committee (IASC) commenced a project to assess the measurement and 
disclosure of financial instruments. These too were to suffer a set-back. 
 
5
 United States Securities and Exchange Commission (2008). 
6
 FDIC (1997). 
6
By 1990, recession had taken hold in the US, with lending contracting sharply. As in 
1938, the US economy was suffering “financial headwinds”. As in 1938, the Fed 
were quick to call for a relaxation of prudential and valuation standards to head-off 
pressures on banks.7
 And as in 1938, the upshot was a concerted move by the thenPresident, George Bush, relaxing examination and valuation standards.8
 For the 
second time, fair value had been returned to its box. 
And so to the present day – the third phase. By 2008, the ranks of “accomptants” had 
swelled, with numbers of recognised accountants in the UK totalling over 275,000. 
Yet the issues today have loud echoes of 1938. Through the 1990s, the main 
international accounting standard-setters extended the boundaries of fair value. In the 
US, this was given impetus by the Federal Deposit Insurance Corporation 
Improvement Act (FDICIA) in 1991. Widespread use of mark to market was a key 
ingredient of the prompt corrective action approach embodied in FDICIA. 
From 1992, it became a requirement among US companies to disclose the fair value 
of all financial instruments in the notes to their accounts. Towards the end of the 
1990s, this move was formalised with financial instruments (derivatives, equity and 
debt) being included explicitly in the accounts at fair value. In the US, this followed 
adoption of Statement of Financial Accounting Standard 133 (SFAS 133) in June 
1998. Elsewhere, it followed adoption of the IASC’s International Accounting 
Standard 39 (IAS 39) in January 2001. 
The banking crisis of the past two years has brought that evolution to a halt. As 
pressures on banks’ balance sheets have intensified, subdued lending growth has 
raised concerns that recovery may be retarded. A debate has begun internationally on 
rolling back fair value to arrest this downward trajectory. Once again, central bank 
governors, politicians, regulators and countries have been prominent in their criticism 
of fair value. Some fear that fair value is poised to enter the third dip on its 
rollercoaster journey. 
 
7
 In a letter to Richard Breeden, Chairman of the Securities and Exchange Commission, which refers 
back to the 1930s experience, then-Chairman Alan Greenspan notes “that market value accounting 
raises a substantial number of significant issues that need to be resolved before considering the 
implementation of such an approach in whole or in part for banking organizations”. Alan Greenspan, 
“Letter to Hon. Richard C. Breeden,” Federal Reserve, November 1, 1990. 8
 Simonsen and Hempel (op. cit.). 
7
Fair Values and Market Prices
What have been the underlying forces leading fair value to be at first lauded, then 
questioned and periodically abandoned? At the heart of this is the vexed question of 
whether market prices are a true and fair assessment of value. 
In theory, market prices ought to be a full and fair reflection of the present value of 
future cashflows on an asset. This is the fulcrum of the Efficient Markets Hypothesis 
(EMH). Market prices, if not perfect, are at least efficient aggregators of information 
– a one-stop shop for appraising value. This simplicity makes EMH powerful theory. 
But its real power is its widespread application in practice. EMH has not just 
monopolised the finance textbooks; it has also dominated the dealing rooms. 
If the EMH were to hold strictly, the fair value debate would be uncontentious. 
Marking of assets to market would be proper recognition of their economic value. In 
that financial utopia, the interests of accountants, investors and regulators would be 
perfectly aligned. Accountants would have a verifiable valuation yardstick; investors 
a true and fair view of their true worth; and regulators an objective means of 
evaluating solvency. Fair value would serve treble duty. 
In practice, the fair value debate is contentious and has been for at least a century. 
Through history, accountants, investors and regulators have not always sung in tune. 
Today, accountants are singing opera Pacioli-style, regulators are rapping at 300 
words a minute, while investors are left to whistle. In part this discord has been 
blamed on failures of EMH, “the precarious yardstick of current market quotations”. 
It should come as no surprise that fair value principles have faced their stiffest tests at 
times of crisis – the Great Depression during the previous century, the Great 
Recession during this. For it is at crisis time that EMH itself faces its stiffest test, 
perhaps none greater than recently. The heterodox British economist George Shackle 
observed: “Valuation is expectation and expectation is imagination”.9
 Imagination, 
and thus valuation, is apt to run wild at the peak of the boom and trough of the bust. 
 
9
 Shackle (1972) quoted in Bronk (2009). 
8
These episodes of over-active imagination, or deviations from EMH, can be grouped 
roughly three ways. Each has an important potential bearing on financial stability and 
on the fair value debate: 
• “Excess volatility”: Some of the earliest evidence against EMH focussed on 
the tendency of asset prices to fluctuate more than could be justified by 
movements in fundamentals – so-called excess volatility. While early 
evidence focussed on the behaviour of equity prices, the same tests have now 
been applied to a wide range of asset markets, including corporate bonds, 
asset-backed securities and exchange rates.10 There is overwhelming 
empirical evidence of excess volatility in asset prices. 
 
• “Medium-term misalignment”: Excess volatility, while inconvenient, need not 
by itself severely distort the functioning of capital markets. Asset prices’ 
signals might be noisy, but correct on average. But there is emerging evidence 
of asset prices becoming persistently misaligned from fundamentals in a 
variety of markets including equity, residential and commercial property and 
corporate bonds.11 
• “Apparent arbitrage”: A third aspect of the failure of EMH is evidence of 
seemingly pure arbitrage opportunities being sustained by market participants 
for lengthy periods. Unlike excess volatility and misalignment, these 
deviations from fundamentals represent riskless opportunities to make profits. 
They have been evident in past, and in particular in the present, crisis. 
Ultimately, the importance of these three features is an empirical question. Charts 1 
and 2 plot the long-run behaviour of the equity market, in the UK from the 1920s and 
in the US from the 1860s. These long sweeps of history are revealing about patterns 
of misalignment and excess volatility. In each case, some metric of fundamentals is 
 
10 Shiller (1981). 
11 Shiller (2005), Smithers (2009). 
9
needed. A model-based measure of fundamentals is used, based on long-run average 
values of dividend growth discounted at a long-run average real interest rate.12
For the US and UK, Charts 1 and 2 present persuasive evidence of both excess 
volatility and misalignment. On average over the sample, equity prices in the UK and 
US are around twice as volatile as fundamentals. The average absolute deviation of 
UK and US equity prices from fundamentals has been over 20% and over 30% 
respectively. If anything, there is evidence of misalignments having increased. 
Average absolute misalignments have averaged almost 30% and 70% in the UK and 
US since 1980. 
 
These deviations from EMH are no less striking moving from financial to real assets. 
Since 1930, real property prices have been more than twice as volatile as typical 
measures of fundamentals. And real property prices have, in different countries, at 
times deviated significantly from measures of fundamentals over the same period.13
EMH predicts essentially a zero correlation in prices across time, as they follow the 
random walk of the homeward-bound drunk. This evidence paints a picture of excess 
volatility (in the short-run) and slow mean-reversion (in the long-run). In other 
words, there is both positive (at short horizons) and negative (at longer horizons) 
serial correlation in market prices. This leaves EMH run-over in both directions. 
With a steely nerve and deep pockets, investors could make profits from exploiting 
these trends. But as Keynes remarked, the market can often remain irrational for 
longer than even a strong-willed investor can remain solvent. In other words, these 
are risky bets. The crisis has also revealed, however, examples of bets which were, 
on the face of it, essentially risk-less deviations from EMH or “apparent arbitrage”. 
Chart 3 considers the price of two on the face of it identical portfolios - an index of 
CDS contracts and an individually-constructed portfolio of the same CDS contracts. 
On average, they ought to trade as one and the same. But for around a year from 
 
12 Alternative metrics for fundamentals, such as cyclically adjusted price-earnings ratios or q, yield a 
broadly similar conclusion (Shiller (2005), Smithers (2009)). 
13 Shiller (2005). 
10
October 2008 onwards, the spreads on these two portfolios differed by as much as 60 
basis points. Even once transactions costs are taken into account, there were 
persistent and significant riskless profits on the table. Chart 4 looks at the difference 
between two, again on the face of it, identical money market bets – forward rate 
agreement spreads and forward rates implied by the LIBOR spreads, both of the same 
maturity. Over the exact same period, these differed by as much as 250 basis points. 
So why were the bets not placed and the arbitrage opportunities exploited? First, 
money is needed to place even a riskless bet. That was the scarcest of commodities 
after the failure of Lehman Brothers in October 2008. Second, placing a bet also 
requires a trustworthy bookmaker. They too were thin on the ground in the midst of 
crisis. In their absence, arbitrage may be more “apparent” than real. Market prices 
are likely to deviate from fundamentals due to liquidity and counterparty premia. 
By way of illustration, Chart 5 provides a decomposition of the yield on subinvestment grade corporate securities in the UK. By mid-2007 at the peak of the 
boom, the liquidity premium on these assets had pretty much been eliminated. By the 
end of 2008, this liquidity premium had risen by almost 2500 basis points. As capital 
markets moved from flood to drought, market prices turned from rich to poor. 
Fair Value and Financial Stability 
Against this backdrop, what are the potential financial stability implications of using 
market prices as a valuation yardstick? In roughly chronological order, three main 
arguments have been used in defence of marking to market. Broadly, these mirror the 
three historic phases of fair value: 
• “Protecting shareholders” 
During the 19th and early 20th centuries, as joint stock companies sprang up, the key 
purpose of company accounts was to protect shareholders’ interests. Like truth in the 
face of the war, in the face of crisis the financial accounts appear to have been the first 
casualty. During the 19th century, the published accounts of Spanish banks became 
less frequent during episodes of crisis. Mussolini’s Italian government of 1931 went 
11
one step further, suspending publication of accounts by the banks to forestall panic.14 
It is unlikely Goethe and Mussolini would have agreed on the merits of double-entry. 
While less extreme, there is compelling evidence of British banks having massaged 
balance sheets from the late 19th century right up until the early 1970s, especially 
during crisis. Typically, this involved the systematic under-valuation of assets to 
allow hidden reserves to be carried on the balance sheet. The experience of UK banks 
in 1952 was typical. As the prices of government securities fell sharply, the basis for 
valuation by banks was shifted from “at or below market value” to “at or under cost”. 
This mirrored the Roosevelt and Bush forbearance announcements of 1938 and 1990. 
The motives for hidden reserves among UK banks were purportedly prudential, as 
protection against the “excessive dividend expectations” of shareholders and as a 
cushion against losses in crisis.15 Although the Companies Act of 1947 prohibited the 
use of hidden reserves, the banks were exempt from its provisions. But the writing 
was on the wall. Non-disclosure by banks came under repeated fire during the 1960s. 
Sensing the inevitable, British banks “voluntarily” decided to pursue full disclosure in 
1969, in the interests of shareholder transparency and protection, though there is 
evidence of hidden reserves persisting right up to the 1980s. 
Those considerations remain relevant today. Shareholders in global banks have lost 
40% of their net worth since the start of the crisis. The implied volatility of global 
banks’ equities is around 50% higher than at the start of the crisis. At its peak, it was 
seven times higher. Confidence in banks’ balance sheets has been shaken to the core. 
It is unlikely to be restored by a return to murky valuation and hidden reserves. 
• “Gambling for Resurrection” 
One special case of shareholder protection arises when management increase their 
risk-taking incentives as the probability of failure rises. Such incentives are in-built in 
a world of limited liability. But the ability to engage in such gambling for redemption 
depends importantly on the degree of information asymmetry between the 
 
14 James (1992). 
15 Billings and Capie (2009). 
12
shareholders and the manager. The lower the transparency of the accounts, the 
greater the incentives and ability of management to bet the ranch. 
This type of behaviour is if anything more likely among banks, given the intrinsically 
greater opacity of their assets. And examples are legion. For example, in the run-up 
to the Savings and Loan crisis in the US, many thrifts financed long-term fixed rate 
assets with variable rate deposits, thereby running significant interest rate risk. This 
was a big gamble. But because it was disguised in the accounts, neither regulators nor 
the thrifts themselves felt obliged to manage this risk. As interest rates rose, the 
gamble failed causing many thrifts to collapse. In response, the Office of Thrift 
Supervision required fair values to be reported from the early 1990s. 
• “Timely Risk Management” 
Perhaps the most recent of the arguments used to support fair values arises from its 
role as a risk management device. Market prices, while noisy, offer timely signals. 
They are likely to prompt early recognition and management of emerging risks and 
mistakes, by both regulators and the regulated. Sunlight is an effective disinfectant. 
In this regard, it is telling that more widespread marking to market accompanied 
regulatory efforts to improve prompt corrective action measures – for example, in the 
US through FDICIA. Among market participants, the use of fair values and fleet-offoot risk management techniques is widely felt to have contributed to the relative 
success of some firms during the course of the crisis. Lloyd Blankfein, CEO of 
Goldman Sachs, certainly appears to think so.16
For the prosecution, the main arguments also appear to be threefold. Essentially, 
these follow from the three commonly attributed failures of the EMH: 
 
16 “At Goldman Sachs, we calculate the fair value of our positions every day, because we would not 
know how to assess or manage risk if market prices were not reflected on our books. This approach 
provides an essential early warning system that is critical for risk managers and regulators”, Lloyd 
Blankfein, Financial Times, October 13 2009. 
13
• “Excess Volatility” 
If market prices exhibit greater volatility than warranted by fundamentals, this will be 
mirrored in the balance sheet footings and profits of entities marking their positions to 
market. This is far from a new phenomenon. Robert E Healy, the SEC’s first Chief 
Accountant back in the early 1930s, lamented that firms “can capitalise practically 
everything except the furnace ashes in the basement”.17
The impact of fair values on profits may have been even greater over recent decades. 
Banks’ profits have become significantly more volatile over the past few decades, 
with the standard deviation of banks’ return on equity trebling comparing the fortyyear periods either side of 1970. There is also evidence of banks’ equity prices 
having exhibited higher correlation as fair value principles have been extended.18
Consider a hypothetical experiment. Imagine banks in the UK had been required to 
mark their banking books to market over the period 1999 to 2008, in addition to their 
trading book. Market prices are used to proxy different categories of loan. For 
example, Residential Mortgage Backed Securities (RMBS) and covered bond prices 
are used to proxy mortgage loans. As with banks’ trading books, all gains and losses 
arising on the banking book are assumed to flow directly to profits. 
Chart 6 plots the path of UK banks’ profits, both actual and simulated under the mark 
to market assumption. Simulated profits are around eight times more volatile. 
Between 2001 and 2006, UK banks’ cumulative profits would have been around £100 
billion higher than recorded profits, as the expected future returns to risky projects 
were brought forward. This would have been the 21st century equivalent of 
capitalising the ashes in the blast furnace. 
For what goes around comes around. Hypothetical losses during 2008 would then 
have totalled in excess of £300 billion, as the risk from these projects was realised. 
The ashes in the furnace truly turned to dust. Had shareholders not already torn it out, 
this rollercoaster ride in profits would have been hair-raising. In this admittedly 
 
17 Quoted in Seligman (2003). 
18 Khan (2009). 
14
extreme case, it is questionable how much shareholder protection fair values would 
have delivered in practice. 
• “Fundamental Misalignment” 
A related but distinct issue arises when market prices deviate from their true values 
for a protracted period. Marking to market then runs the risk not just of unwarranted 
volatility but unjustifiable bankruptcy. Take a bank whose liabilities are perfectly 
maturity matched with its 10-year assets. So there is no necessity for the bank to 
liquidate its assets to make good its liabilities as they fall due. 
But the market price of the banks’ assets might well embody a premium for instant 
liquidity – a liquidity premium. As Chart 5, at times of stress, these premia are large 
and overshoot, lowering asset prices below economic value. In this situation, a 
marked to market balance sheet may give a misleading impression of banks’ worth. 
And if these distortions are large enough, fair values could even generate insolvency. 
During this crisis, the precipitate rise in liquidity premia and fall in asset prices may 
have called into question the viability of many banks had their assets been fair-valued. 
Consider again the banking book of UK banks on a mark to market basis. Chart 7 
shows the loss of value on this book, which would have peaked at over £400 billion 
during the early months of 2009. The total capital resources of UK banks at that time 
were around £280 billion. In other words, the UK banking system in aggregate would 
have been technically insolvent on a mark to market basis. The recent recovery in 
asset prices has been almost as remarkable as the preceding fall. It has meant UK 
banks were back in the black within a matter of months. 
• “Liquidity and Fire-sales” 
The act of marking to market may itself have a bearing on asset price dynamics. This 
arises because of its potential effects on banks’ behaviour. If swings in perceived 
solvency cause banks to sell assets, these fire-sales may themselves add to downward 
pressures on asset prices. Under mark to market, these pressures are felt by all 
15
institutions, not just the seller. In effect, fire-sales by one firm have negative 
externalities for all others. And as other banks adjust their own balance sheets in 
response, there is a risk the downward dynamic is perpetuated. 
By acting in this way, marking to market has the potential to serve as an amplifier of 
stress in the financial system.19 Other things equal, it could result in sharper and more 
severe asset price falls than in the past, accompanied by greater institutional stress. It 
could exaggerate excess volatility and misalignment. Marking to market may not just 
be a casualty of the failure of EMH; it may also be a cause. 
Hyun Shin has likened the destabilising dynamics of mark to market to the unstable 
oscillations of London’s Millennium Bridge at the time it opened. In finance, these 
adverse dynamics have a much longer historical pedigree. In their classic monetary 
history of the United States, Friedman and Schwartz assigned mark to market a key 
role in propagating banking failure during the Great Depression.20 The evidence of 
such dynamics during this crisis is more mixed. Some studies have claimed this 
effect was limited to banks with large trading portfolios;21 others that it has been 
significant and wide-ranging across the financial sector.22
Perhaps the truth lies somewhere in between, with some market and institutions 
affected and others immune. Chart 8 plots commercial property values in the UK 
since 1920. There are five discernible boom and bust cycles in commercial property, 
signified by the dotted lines. Chart 9 looks at the cumulative falls in value during the 
bust. In four of the cases, the bust was similarly timed and sized. The exception is 
the bust of 2007-08, where the fall in value has been both greater and faster. It is 
plausible that fire-sales, aggravated in part by marking to market, may have 
contributed to this dynamic. 
 
19 For example, see Plantin, Sapra and Shin (2008) and Allen and Carletti (2007) for a theoretical 
exposition of these dynamics. 
20 “Under such circumstances, any runs on banks for whatever reason became to some extent selfjustifying, whatever the quality of assets held by banks. Banks had to dump their assets on the market, 
which inevitably caused a decline in the market value of those assets and hence of the remaining assets 
they held. The impairment in the market value of assets held by banks, specifically their bond 
portfolios, was the most important source of impairment of capital leading to bank suspensions, rather 
than the default of specific loans or specific bond issues.” (Friedman and Schwartz (1963)). 
21 For example, Laux and Leuz (2009). 
22 For example, Wallison (2008). 
16
The Fair Value Agenda 
So how do these considerations relate to the debate on international accounting 
standards? At present, these stand at a crossroads. In the US (through the Financial 
Accounting Standards Board (FASB)) and internationally (through the International 
Accounting Standards Board (IASB)), standard-setters are reviewing their treatment 
of financial instruments. Substantive decisions are planned during 2010. 
Within the IASB, a review of IAS39 is underway, with consultation on a new 
standard, IFRS9. The fair value debate is at the heart of the new proposals. One key 
dimension is valuation, where IFRS9 proposes a combination of amortised cost and 
fair values, with clear criteria to determine the suitability of assets for each category. 
In the US, FASB is expected to issue consultation proposals that would tend to 
reinforce the use of fair value among US banks. 
A second dimension is provisioning. The concern here is that the use of provisions 
based on incurred losses means that impairments are recognised too late, thereby 
contributing to pro-cyclicality of loan supply.23 In response, the IASB has issued a 
consultation paper proposing that provisions be set on an expected loss basis. These 
proposals are currently being explored by regulators internationally. 
Underlying both of these debates is a perceived tension between the needs of different 
stakeholders, in particular investors and regulators. So what broad principles might 
frame accounting standards if the demands of these stakeholders are to be met? Using 
the framework outlined earlier, these principles might include: 
• The importance of a common measuring rod 
 
The G20 have committed FASB and the IASB to convergence of international 
accounting standards by June 2011. This is an ambitious timetable, for it is not just a 
meeting of two minds. There are perhaps more than 30 different accounting standards 
 
23 For example, Turner (2010). 
17
operating worldwide. Many minds need to meet if a truly international standard is to 
emerge. 
It could be argued that differences in accounting standards do little harm. Like 
foreign languages, we may learn to live, perhaps even love, them. Attempts to 
compel a common language might risk creating the accountancy equivalent of 
Esperanto. Unfortunately, the analogy is inexact. Banks are, by their nature, 
international. So too are investors in, and regulators of, banks. If all parties speak 
different languages at the same time, the result is likely to be noise rather than signal. 
For banks, the noise to signal ratio has been particularly high during this crisis. 
Differences in accounting standards have contributed to this noise. In 2008, UK 
banks’ assets would have been £2 trillion, or around 30%, larger under European 
IFRS than under US GAAP standards. These differences make problematic 
international comparisons of such rudimentary concepts as bank leverage. This in 
turn hinders investors’ risk assessments and regulators’ supervisory assessments. 
• A failure of efficient markets is not of itself a failure of fair value 
It is commonly heard that the failure of EMH argues against fair value in favour of 
some alternative, such as amortised cost. The truth is more subtle than that. 
Deviation from EMH will cause both accounting measures to deliver distorted signals 
of value. Depending on the precise circumstances, either measure might deliver a 
more accurate measure of true economic value. 
To see this, consider four scenarios. Consider first a bank making a single loan. In 
the first period, amortised cost and fair valuations of this loan will be equal. Expected 
cashflows will in both cases be discounted at the prevailing market discount rate. To 
the extent EMH is violated – for example, because the market discount rate is too low 
– both accounting concepts will result in asset over-valuation. Both concepts will be 
equally imprudent. In other words, credit cycles that cause failures of EMH 
contaminate bank asset valuations irrespective of the accounting convention. 
18
Consider next a bank with a portfolio of two loans, one initiated when assets prices 
were priced correctly, the other when they were over-valued. In this situation, 
amortised cost and fair values will value the asset portfolio differently. Because 
market prices are applied to the whole asset stock, fair value will tend to result in 
greater recorded overvaluation. In other words, marking to market is more 
susceptible to valuation cycles than amortised cost. 
Third, consider a situation where, having been over-valued, the market price of the 
second loan corrects back to equilibrium. Fair values now deliver the correct 
valuation of the entire asset portfolio. Amortised cost measures, meanwhile, will 
continue to give a misleadingly bullish account of the second loan’s valuation, since 
this will be discounted at the artificially-low discount rate used at initiation. In other 
words, in this set of circumstances the tables are turned, with fair values giving a 
more accurate and prudent measure of valuation. 
Finally, if instead of correcting to equilibrium, assume market prices over-correct – 
say, because of an overshoot in illiquidity premia of the type witnessed during crisis. 
It is then no longer clear which valuation metric is preferable. Both will be inaccurate 
to some degree but in opposite directions – the amortised cost measure suggesting 
valuations which are “too high”, while fair value will suggest valuations which are 
“too low”. The greater the initial misalignment in asset prices, and the smaller their 
subsequent overshoot, the greater the likelihood of fair values being preferred over 
amortised cost and vice-versa. Ultimately, however, this is an empirical question. 
In general terms, however, the point is clear: efficient markets are not necessary but 
may be sufficient to justify the use of fair value principles. 
• Better accounting for expected losses 
What is clear from these examples is that there is a potential trade-off in the use of 
amortised cost versus fair value measures when market prices deviate from EMH. 
Both might give misleading signals, but in opposite directions. Recognising the 
problems with either, is there a way of doing better than both? 
19
Perhaps the simplest way of doing so would be to use both valuation metrics. There 
have already been suggestions that “dual” accounts could be drawn up.24 The upside 
of this approach is that it would give regulators and investors more information on 
which to base assessments. It releases stakeholders from the need to pick a winner. 
The downside is that both valuations may be inaccurate, with a lack of clarity about 
which ought to be used, for example, to judge bank solvency. Or in the words of 
Macbeth’s three witches, “double, double, toil and trouble”. 
A more ambitious alternative would be to seek a more systemic and standardised 
valuation methodology in the first place, against which different approaches can be 
cross-checked. The key here would be to establish an objective measure of expected 
loss, less susceptible to the excess volatility of market prices but adept at picking up 
its timely signals. In the language of George Shackle, stricter valuation standards 
would help place some bounds on the expectations and imaginations of bankers. 
This is a role which neither accountants nor regulators are best placed to carry out. It 
would require a body with both expertise in valuation and objectivity. It would seek 
consistency and, as far as possible, accuracy in valuations across asset classes, 
institutions and countries. An International Valuation Standards Board already exists. 
But it standards have only been adopted in around half a dozen countries. Accounting 
and regulation already have fora to support consistency of standards. During this 
crisis, valuation practices have been every bit as important. Perhaps they too need 
international recognition. 
• Business models matter, especially for banks 
Accounting standards already reflect characteristics on the assets side of banks’ 
balance sheet. For example, IASB standards require consideration of the cashflow 
characteristics of assets (for example, specified cashflows of interest and principal) 
and the intentions of the holder (for example, to collect the contractual cashflows). 
But for banks the characteristics of their liabilities may be every bit as important as 
 
24 For example, FASB (2009). 
20
their assets. Indeed, at times of stress in funding markets, liability characteristics may 
be more important. 
Consider, for example, a 10-year loan with regular interest payments which is 
intended to be held to maturity by a bank. These characteristics would justify the 
bank carrying the asset at amortised cost. But imagine this loan is funded with 
overnight loans. Whatever the intention, this liability structure would require early 
liquidation of the asset if funding were to dry up. In other words, the ability of a bank 
to hold assets to maturity may be as important as the intention. 
The greater the maturity mismatch, the greater the likelihood of liability 
characteristics dominating asset intentions. In other words, the case for using fair 
values is greater when balance sheets are maturity mismatched. Or, put differently, 
precisely because market prices embody a liquidity premium, they could give a better 
view of the true asset position of a firm facing liquidity constraints. For institutions 
facing funding pressure, liquidity premia may be a legitimate measure of 
fundamentals. 
And which companies’ balance sheets are most subject to such maturity mismatch? 
Banks. It has been argued that banks ought to be protected from the vicissitudes of 
market prices. But, given their maturity transformation role, the case may actually be 
stronger for banks than for other types of both financial (such as insurance companies 
and pension funds) and non-financial firms. For some banks, this may be the 
accounting convention that best aligns the economic characteristics of both assets and 
liabilities. 
It is interesting that there was evidence of financial markets making their own switch 
in valuation convention during the course of the crisis. As funding maturities 
shortened, the probability of asset liquidation rose. It became rational, then, for 
investors to begin valuing even banking book assets at market prices, as in Chart 7. 
For a time, this process appeared to generate its own downward dynamic, with 
shortening maturities and falling asset prices eroding the impliedly mark-to-market 
solvency position of banks in a liquidity/solvency loop. 
21
Some have argued this downward dynamic itself justifies switching-off fair values. 
But the perils of doing so are clear. Persisting with an inappropriate valuation metric 
may give an inaccurate picture of banks’ true solvency position. It will also reduce 
banks’ incentives to adjust funding structures to guard against such a dynamic. It is 
precisely such risk management incentives that appear to explain the relative success 
of some firms, including Goldman Sachs, during this crisis. Therein may lie a lesson. 
Conclusion 
The fortunes of fair value have waxed and waned historically, particularly at crisis 
time. So it is no surprise that fair value is under attack today. We may be at yet 
another pivot point. 
With financial markets still thick with fog and filthy air, now would be an unfortunate 
time to starve balance sheets of the sunlight provided by fair values. Blocking out the 
sun or, worse still, claiming it revolves around the earth will not serve banks or 
regulators well in the longer run. Restoring traditional accounting principles sounds 
desirable, provided the (Italian) values we import are Pacioli rather than Mussolini. 
At the same time, it needs to be recognised that too much sunlight can scorch. That 
means applying appropriate filters to fair values, screening out their harmful rays. 
Rethinking valuation practices across firms, asset classes and countries, better to 
capture expected losses, is one important such filter. Recognising the liability as well 
as asset characteristics of institutions may be another. 
We need to ensure these changes do not erode fair value principles, as that would 
result in the baby being thrown out with the bathwater. Making fair values less foul 
ought to advantage both investors and regulators. It would protect fair value from 
lightening strikes when the next financial thunderstorm breaks.

Economists have not covered themselves in glory recently when it comes to 
forecasting. Tonight I want to put that right by making a prediction that is big, bold 
and frighteningly precise. Liverpool Football Club will finish this season third in 
the league. Manchester United will top the table followed by Arsenal. Having been 
top for most of last year, Chelsea will finish the season near the foot of the table. 
Newcastle United, meanwhile, will finish fourth. 
 
I know what you are thinking. Even by economists’ standards, some of those 
predictions sound implausible. Liverpool third? So let me explain. The season I 
have in mind is not the English football season; it is the financial reporting season. 
And the table to which I refer is not the Premiership points table; it is the league table 
of English football club debt1
. Rarely has securing a slot in the top four held less 
allure. Debt is the subject of my talk tonight. 
We are living through an extraordinary period for the economic and financial system. 
Events of recent years will be seen by financial historians as among the most 
significant in the past millennium. At the worst point of the crisis, savers and 
borrowers around the world came close to losing confidence in financial institutions. 
The resulting panic has had deep and long-lasting consequences for global activity. 
The statistics are striking. 
Between July 2007 and March 2009, the equity prices of global banks fell by 75%. 
That is a loss of market capitalisation of around $5 trillion. In the UK, banks’ equity 
prices fell by over 80%. Taken alongside falls in other asset prices, the loss of global 
wealth peaked at over $25 trillion, or almost 45% of global GDP. At that point, asset 
price falls in the UK and US were as large as during the Great Depression (Chart 1). 
Knock-on effects to the real economy were no less dramatic. Peak to trough, output 
in OECD economies has fallen by around 4%. In the UK, output is likely to have 
fallen by around 6%, peak to trough. The present value of those losses, carried 
forward across generations, would be a significant multiple of those falls. Today’s 
Great Recession is the most significant economic event since the Great Depression. 
 
1
 Deloitte (2009), “Safety in numbers: Annual Review of Football Finance”. 
2
That this Great Recession did not become a second Great Depression is in large part 
the result of policy actions. These have been unprecedented in speed and scale. 
Monetary policy around the world has been loosened dramatically. And direct 
support for the financial system has been equally swift and large, at around a quarter 
of global GDP. In the US and UK, it is nearer three-quarters of GDP (Chart 2). 
The Road to Recovery 
The good news is that these measures appear to be working. The recovery in global 
financial markets and the world economy over the past year has been almost as 
remarkable as the preceding fall. Certainly, it has been far earlier and far sharper than 
during the Great Depression (Charts 1 and 3). Having fallen off a cliff, the bounce in 
financial markets has been more startled bunny than dead cat. 
From their trough almost a year ago, global banks’ equity prices have risen by over 
130%, recovering more than half of their losses. For UK banks, the recovery has been 
even more dramatic, rising 140%. Since 1900, there has been only one 9-month 
period when the rally in UK financial sector equity prices has been greater (Chart 4). 
This rebound in asset prices has been broad as well as steep, spanning the maturity 
and risk spectrum. Yields on safe securities have fallen across all maturities. Policy 
rates in the UK fell to their lowest levels since the founding of the Bank of England. 
And long-term real interest rates in the US and UK, at around 1%, are around half 
their average level over the past decade. 
Moving up the risk spectrum, the price of both investment and sub-investment grade 
corporate bonds has risen since March last year, by 20% and 60% respectively. 
Global equity prices have risen by over 70% over the same period. The rise in UK 
equity prices is one of the largest in the 316-year history of the Bank of England 
(Chart 4). Residential and commercial property prices in the UK have turned. And 
global wealth losses on private sector securities, at around $6 trillion, have fallen by a 
factor of four over the past year alone. 
3
What explains this remarkable recovery in risky asset prices? Three factors seem to 
have been important, all of which can be traced to policy actions by the authorities: 
• First, the rate at which the future cashflows on risky assets are discounted has 
fallen due to lower short and long-term global real interest rates. Using a 
standard asset pricing model, this discount rate effect accounts for perhaps 
around one fifth of the rise in UK and euro-area equity prices and one third of 
the rise in US equity prices since March last year (Chart 5). 
• Second, as fears of a repeat of the Great Depression have abated, the premium 
that investors require to compensate for this risk – the risk premium – has 
fallen, boosting expected future cashflows on risky assets. This more than 
accounts for the rise in UK and US equity prices over the past year (Chart 5). 
It also accounts for a significant proportion of the rise in risky debt prices: a 
year ago, corporate debt prices were signalling larger losses on company 
securities than seen during the Great Depression (Chart 6). 
• Third, improved liquidity in financial markets has lowered decisively 
uncertainty about future market prices. This has lowered the compensation 
investors require for such risk – the liquidity premium – boosting asset prices. 
This accounts for around a half of the fall in spreads on sterling investment 
grade corporate bonds since March last year (Chart 7). 
Mirroring the recovery in asset prices, there is now convincing evidence of global 
output having turned in the second half of 2009. The IMF projects that the global 
economy will grow by 3.9% in 2010, having contracted by 0.8% in 2009. Surveys of 
manufacturing in the major economies are at levels last seen prior to the crisis. 
This combination of a stronger real economy and buoyant financial markets has 
generated a dramatic turnaround in fortunes of the banking system. Global banks' net 
income in 2009 is expected to be around $60 billion, compared to a loss of roughly 
that amount in 2008. Income from market-making in various financial products has 
been especially lucrative, given higher bid-ask spreads and client activity (Chart 8). 
4
This windfall gain has helped repair banks’ over-extended balance sheets. Global 
banks have boosted their Tier 1 capital ratios by almost 3 percentage points since the 
start of 2009. UK banks’ Tier 1 ratios have increased by around 3.4 percentage 
points. Liquidity ratios among global banks have also risen, with sterling liquid assets 
relative to total asset holdings more than trebling among UK banks. 
On the back of this positive news, the authorities in some countries have begun 
withdrawing extraordinary levels of support. More than half of the capital provided to 
the US banking system was repaid in 2009. The US authorities have also announced 
their intention to wind down several special liquidity facilities, as has the European 
Central Bank. This exit is far sooner than might have been expected six months ago. 
Stability, if not normality, is beginning to return. 
Debt Overhangs and Debt Hangovers 
If that were the end of the story, it would be a happy ending. But there are good 
reasons for believing this story has some way to run. For while the flow of news over 
the past year has been positive, some of the stock problems which were the root cause 
of the crisis remain intact. The lasting legacy of this crisis is too much debt held by 
too many sectors against too little capital. 
Economists have a special word for this type of problem – a debt overhang. Its 
economic effects are fairly well understood. Debt operates rather like a tax. Debt 
servicing costs, like a tax, reduce the disposable income of the borrower. Too much 
debt means a higher debt “tax” and a greater drag on activity – lower lending by 
banks and spending by households and companies. Debt and taxes also affect 
incentives. Set too high, they may dissuade people from working and investing. 
These effects are often captured in a relationship called the Laffer curve. Higher tax 
rates may boost tax revenues, but only up to a point. After that point, the tax take 
could actually fall due to the disincentive effects of higher tax.2
 In that event, both the 
taxpayer and the tax collector are worse off. The same applies to debt. There is a 
 
2
 Laffer, A (2004), “The Laffer Curve, Past, Present and Future”, available at www.heritage.org. 
5
debt Laffer curve. If a debt overhang is sufficiently severe, the interest burden 
weakens debtor incentives to repay. In the event borrowers and lenders find 
themselves on the wrong side of the debt Laffer curve, both are worse off.3
 
Non-economists sometimes have a different word for an overhang. Conveniently, this 
is found by simply transposing the syllables. What we face today may be called a 
debt overhang, but what it will feel like is a debt hangover. Like a hangover, it will 
slow activity in the period ahead. If it is severe enough, it may diminish incentives to 
work and invest. Too wild a financial party risks borrowers finding themselves on the 
wrong side of the Laffer curve the morning after. 
The McKinsey Global Institute has recently looked at the debt accumulated by ten 
developed economies over the past decade, including the UK and US.4
 Since 2000, 
gross debt in these ten economies has increased by around $40 trillion, a rise of 60%. 
The sectoral contributions to this rise in debt are roughly equally split between 
households, companies, the financial sector and governments. 
The accumulation of debt has perhaps been greatest within the financial system. 
Among UK and US banks, leverage has increased dramatically over the past century. 
The ratio of assets to equity rose from single digits at the start of 20th century to over 
twenty by its end.5
 Despite recent capital raising, banks’ leverage remains high 
absolutely and relative to the past, at between 20 and 50 times equity (Chart 9). 
Among households, debt-to-income ratios have risen materially over the past twenty 
years. In the UK, household debt-to-income ratios rose from around 100% in 1988 to 
a peak of around 170% in 2008. In the US over the same period, the household debtto-income ratio rose from 80% to 135%. From different starting points, similar trends 
are evident in Spain, Canada and South Korea. Most households in these countries 
still have significant net wealth, however; in the UK, total assets are five times 
household debt. 
 
3
 Krugman, P (1989), “Market-based Debt Reduction Schemes”, in J Frenkel (ed.), Analytics of 
International Debt, IMF. . 4
 McKinsey’s Global Institute (2010), “Debt and Deleveraging: the Global Credit Bubble and its 
Economic Consequences”. 
5
 Haldane A G (2009), “Banking on the State”, available at www.bankofengland.co.uk. 
6
Those debt trends are repeated in parts of the corporate sector. Among UK 
companies, debt as a fraction of companies’ total financial liabilities has risen from 
around 20% in 1988 to around 34% today. In certain sectors, the run-up in debt has 
been more dramatic – for example, among US and UK commercial property 
companies whose leverage has more than doubled in the past decade. It is also true of 
some companies subject to leveraged buy-outs including, of course, Liverpool 
Football Club. 
Finally among sovereigns, the picture up until recently has been benign with public 
debt flat relative to GDP. But the crisis means that picture is set to change 
dramatically. Among the G7 countries, the IMF forecast that public debt ratios will 
rise from around 80% of GDP in 2007 to around 125% by 2014. In the UK and US, 
public debt ratios are forecast by the IMF to double, mirroring the pattern following 
past financial crises.6
Taking together the debt position of the financial sector, households, companies and 
sovereigns paints a sobering picture. Total debt ratios relative to GDP rose 
significantly in all ten countries studied by McKinsey’s, from an average of around 
200% in 1990 to over 330% by 2008.7
 Over the same period, UK debt ratios more 
than doubled, from just over 200% to around 450% of GDP. 
To date, servicing these debts has been cushioned by policymakers’ actions. 
Government debt and equity have substituted for private debt and augmented private 
equity to support impaired balance sheets, especially among financial firms: a third of 
capital raised by banks since the crisis began has come courtesy of government. And 
through monetary measures, interest costs have been lowered dramatically: debt 
servicing has fallen, often dramatically, across many sectors. 
These extraordinary policy measures have acted like a painkiller for debt problems. 
But painkillers offer only temporary relief. Loans from government to repair balance 
sheets need ultimately to be repaid. And monetary stimulus will need ultimately to be 
 
6
 Reinhart, C M and K Rogoff (2009), This Time is Different – Eight Centuries of Financial Folly, 
Princeton University Press. 
7
 Data are not available for all ten countries, so period averages are based on slightly different samples. 
7
withdrawn. Public policy can act as a balm for debt problems, not a long-run cure. 
So how severe might the hangover be once the painkillers have worn off? 
As a thought experiment, consider the impact on debt servicing costs of long-term 
interest rates reverting to a more normal level (say, 5%) assuming debt levels 
remained as they are. At current mortgage spreads, UK households’ long-term debt 
servicing costs would almost double relative to income, rising to over 13%.8
 In other 
words, income gearing would be close to levels reached in the early 1990s recession. 
Aggregate numbers may under-estimate the burden on deeply indebted households. 
Around a third of UK households have debt servicing costs which exceed 20% of 
income. For those households, debt servicing appears not to have fallen in the past 
two years, with higher mortgage spreads offsetting the effects of lower interest rates 
(Chart 10). 
For companies the picture is similar. A normalisation of long-term interest rates 
would increase UK companies’ debt servicing costs from 17% of profits currently to 
around 33%. That is significantly below the levels reached in the early 1990s, when 
debt servicing peaked at 58% of profits. But the distribution of debt across companies 
again paints a less promising picture. In 2007, around 25% of UK firms made 
insufficient profits to cover their interest payments (Chart 11). This is a long tail. 
For sovereigns, debt servicing costs among the G7 economies are currently low, at 
around 3% of GDP. But were medium-term interest rates to normalise, against a 
backdrop of rising debt ratios, the combined effect would be significant. Over the 
medium term, debt servicing relative to GDP in the G7 economies would double. 
Finally, banks’ refinancing burden in the next few years has grown as a result of a 
shortening of debt maturities during the crisis. The average maturity of US banks’ 
rated debt is estimated by Moody’s to have fallen from 6.7 years to 3.2 years since 
2005. Among UK banks, it has fallen from 6.8 years to 4.3 years. The resulting nearterm refinancing schedule is estimated by Moody’s at $7 trillion over the next three 
 
8
 Bank of England Financial Stability Report, December 2009. 
8
years. Among UK banks, it is in excess of £1 trillion between now and 2014. As 
hangovers go, this one is large and will linger. 
Dealing with Today’s Debt Hangover 
The road to balance sheet repair is likely to be long and winding for both the real 
economy and financial system. Adjustment needs to be neither too fast nor too slow. 
Too fast and lending and spending fall, jeopardising today’s recovery. Too slow and 
balance sheet fragilities persist, jeopardising tomorrow’s stability. So what principles 
should guide the transition? Let me highlight two drawn from the past. 
The first comes from monetary policy. Over the past 30 years, many economies have 
sought to bring down inflation – so-called disinflation. The optimal rate of 
disinflation is usually felt to be gradual to limit damage to short-run growth. But 
there is an important exception to this gradualist rule. If a downward inflation 
surprise comes along, it is optimal to pocket this windfall as this accelerates the path 
to low inflation without harming growth. Disinflation is “opportunistic”.9
The same general principles apply to the repair of indebted balance sheets. Take 
banks. Over the medium-term, their capital ratios are likely to need to rise. If this is 
achieved too fast, by constraining lending, it poses a risk to growth. As with 
disinflation, there is a strong case for gradualism. But if a positive profit surprise 
comes along, this windfall should be pocketed, front-loading the path to higher capital 
without harming lending and growth - an opportunistic approach to stabilisation. 
Global banks have recently received just such a profit windfall, as full-year results for 
the main banks are beginning to attest. There is a strong case for banks, in the UK 
and internationally, pocketing this windfall rather than distributing it to either staff or 
shareholders. This would allow banks’ balance sheets to be repaired while supporting 
lending to the real economy. It is prudential opportunism. 
 
9
 Orphanides, A and Wilcox, D W (1996), “The Opportunistic Approach to Disinflation”, Board of 
Governors of the Federal Reserve System. 
9
So far during this crisis, there has been little evidence of such prudential opportunism. 
Among global banks, net income fell by over 20% between 2006 and 2007. Over the 
same period, dividends grew by 20%. In 2008, global banks made losses totalling 
$60 billion, but on average still made dividend payouts of over $60 billion. 
Although it sounds peculiar, this behaviour appears to be deeply rooted. Table 1 
summarises payouts to shareholders by companies in the UK and US since 1965, both 
banks and non-banks. Five features are notable: 
• First, payout ratios to shareholders from banks’ profits have consistently been 
high. Since the mid-1960s, the payout ratio has generally exceeded 50%. At 
times in the distant past it has been higher still: the average payout ratio to 
Bank of Scotland shareholders over the period 1800 to 1995 was around 
70%.10
• Second, there is little evidence of payout ratios being higher for financial than 
for non-financial companies. Such high payouts are themselves something of 
a puzzle because, at least in theory, the payout ratio ought not to affect the 
value of a firm.11 This may be a collective action problem, with firms fearful 
of sending an adverse market signal through lower payouts. 
 
• Third, for both financials and non-financial firms, the flow of dividend income 
is much less volatile than firms’ stream of profits. In other words, there is 
evidence of firms systematically smoothing dividend payouts to 
shareholders.12 This may also reflect a problem of collective action. 
• Fourth, the profits stream of the financial sector is significantly more volatile 
than for non-financial companies. That appears largely to be the result of 
large-scale, one-off losses by banks. This is as we would expect, given their 
greater leverage. 
 
10 Cameron, A (1995), “Bank of Scotland 1695-1995”, Mainstream Publishing. 
11 Modigliani F and Miller M (1961), “Dividend Policy, Growth and the Valuation of Shares”, Journal 
of Business, 411-33.. 12 For some of the earliest evidence, see Lintner, J (1956), “Distribution of incomes of corporations 
among dividends, retained earnings and taxes”, American Economic Review, 97-113. 
10
• Fifth, as a result, the dividend payout ratio for banks is more volatile than for 
non-banks. 
This behaviour is unlikely to support banking stability. It risks profits being 
distributed as dividends when they are most needed to augment capital ratios and 
boost confidence. In 1996, the Chief Executive of a famous company observed: “We 
are an old-fashioned business, not a quoted plc, and we don’t pay dividends to 
shareholders”. The chief executive? Peter Robinson. The company? Liverpool FC. 
Perhaps banks should have heeded the message. 
If they had, this crisis might have felt rather different. If UK banks had reduced 
dividend payouts ratios by a third between 2000 to 2007, £20 billion of extra capital 
would have been generated.13 Had payouts to staff been trimmed by 10%, a further 
£50 billion in capital would have been saved. And if banks had been restricted from 
paying dividends in the event of an annual loss, £15 billion would have been added to 
the pot. In other words, three modest changes in payout behaviour would have 
generated more capital than was supplied by the UK government during the crisis. 
Opportunistic behaviour is also needed to repair banks’ liquidity positions. Reversing 
the fall in the maturity of banks’ balance sheets will require a front-loaded terming out 
of their debt liabilities. There is some evidence of banks doing so. But given the 
scale of the refinancing mountain, this will be an uphill struggle. 
There is also some evidence of companies tapping capital markets opportunistically to 
help repair their balance sheets. Corporate bond issuance by non-financial companies 
during 2009 was around $1.2 trillion globally, the highest on record. Manchester 
United have recently become one of the first firms to do so in 2010. Surveys of 
households suggest some of the windfall of lower interest costs is being used to repay 
debt. Opportunistic repayment or refinancing is the financial stability equivalent of 
repairing the roof while the sun is shining and it is important it continues. 
 
The second lesson comes from past debt crises, in particular among developing 
countries. If a borrower has a large debt overhang, there is a case for restructuring the 
 
13 Bank of England Financial Stability Report (op.cit.). 
11
claim from debt into equity – a debt-for-equity swap.14 In some cases, such swaps are 
no more than recognition that an impaired debt is, in substance, an equity claim. But 
more than that, debt-for-equity swaps can potentially benefit both lender and 
borrowers by airlifting a debtor to the safer side of the debt Laffer curve.15 
A number of global banks have begun putting such a strategy into practice. Around a 
hundred debt exchanges were carried out by global banks during 2009, typically 
converting debt instruments into equity or retiring debt below face value, so reducing 
leverage in the financial sector. That these swaps were voluntary and market-based is 
evidence of their benefit to both lender and borrower. 
Debt-for-equity swaps could be used to tackle the debt overhang in other sectors. 
There are already some examples of voluntary debt-for-equity exchanges among 
companies. For example, Chelsea Football Club recently announced a large-scale 
swap, which accounts for their plummit down the debt league table. In principle, 
mortgage contracts could also be adapted to lessen the burden on over-indebted 
households by allowing lenders to convert their loan into an equity stake, as 
suggested, for example, by the charity Shelter.16
Preventing Tomorrow’s Debt Hangover 
Once today’s debt hangover is solved, how is tomorrow’s to be prevented? As debt 
crises have been with us for a millennium, it is fanciful to think they could be 
eliminated. But could the party be moderated in frequency and scale? In seeking new 
ways to tackle this old problem, two possible avenues would be the re-orientation of 
regulatory policy on the one hand and better designed debt contracts on the other. 
First, regulatory policy. Since at least the 1970s, public policy has not sought actively 
to moderate fluctuations in the credit cycle in most G7 economies. Moderating the 
business cycle was believed sufficient to hold credit in check. The experience of the 
past decade, in the UK and elsewhere, has called that into question. In the UK, credit 
 
14 Krugman (op.cit). 
15 For example, Zingales, L (2008), “Plan B”, Economists’ Voice. 16 See, for example, 
http://scotland.shelter.org.uk/getadvice/advice_topics/paying_for_a_home/mortgage_arrears/mortgage_
to_shared_equity 
12
grew at three times the rate of money spending over that period, sowing the seeds of 
the credit crisis, while inflation and growth remained remarkably stable. 
In tackling this problem, one option is a re-orientation of regulatory policy towards 
curbing the cycle in credit supply. For example, regulatory requirements on banks 
could be raised to lean against a credit boom and lowered in the teeth of a credit 
downturn. In the public houses for credit, intake would be monitored, opening hours 
restricted and the Happy Hour abolished. 
This approach has become known in policy circles as macro-prudential policy.17 
Although the name is new, it is in the time-honoured policy tradition of “removing the 
punchbowl” from the party as it is getting started – a fitting analogy if the aim is to 
prevent a debt hangover. Had such a party-pooper been in place over the past decade, 
today’s debt headache would plausibly have been less acute. 
Regulation might also be used to lean against the collective tendency of banks to 
payout high even when profits are low. Among regulators internationally, there are 
proposals to introduce capital conservation rules, requiring banks to distribute less of 
their profits in adverse states of the world. By ensuring prudent profit retention on a 
collective basis, these rules ought to be in the long-term interests of banks and their 
shareholders, as well as the authorities. 
A second, complementary, approach for containing or cushioning fluctuations in the 
credit cycle would be to rethink the design of contracts – if you like, lowering the 
alcohol content of debt instruments. Some debt contracts are ill-suited to the needs of 
their customers, with the debt servicing burden rising at just the point borrowers are 
least able to pay. For example, the “teaser rate” mortgages offered to households in 
the US generated an automatic and correlated spike in income gearing as rates were 
reset. This amplified repayment and default risk. 
A better-designed debt contract would automatically adjust repayment terms when the 
borrower found the going getting tough. Debt would become, in the language of 
economics, state-contingent – contingent on the borrower’s state of financial health. 
 
17 Bank of England (2009), “The Role of Macro-Prudential Policy”, available at 
www.bankofengland.co.uk. 
13
By cushioning fluctuations, these instruments have the potential to stabilise 
automatically debt dynamics. And by averting costly default, they potentially benefit 
both creditors and debtors. 
There has been recent interest among banks in issuing state-contingent instruments – 
so-called contingent capital. These instruments convert into equity in the event of a 
pre-defined stress trigger being breached. So these instruments offer repayment 
insurance to banks at the point it is most valued. They are, in effect, a contractually 
pre-committed form of debt-equity swap. 
The design of contingent capital needs further consideration - for example, the 
definition and calibration of the trigger thresholds. Nonetheless, experience to date 
offers encouragement. The contingent capital instruments issued at the end of last 
year by Lloyds Banking Group have risen in price since being issued. If contingent 
capital became more widespread, banks’ capital ratios would be automatically 
stabilised over the cycle, lowering the chances of future banking crises. 
As several academics have argued, the same basic principles could be applied to the 
debt contracts issued by households, companies and even sovereigns. Take a typical 
mortgage contract. A rise in the value of a property relative to the loan gives the 
borrower equity against which they can borrow. This provides an incentive to tradeup, raising house prices and generating further equity. This amplifier operates 
symmetrically, as falling collateral values reduce refinancing options and drive down 
prices. Economists call this effect the financial accelerator.18 It adds to cyclicality in 
credit provision and asset prices. 
US economist Robert Shiller has suggested it might be possible to devise mortgage 
contracts that slow, or even reverse, this financial accelerator19. Instead of being fixed 
in money terms, imagine a mortgage whose value rose with house prices. So the 
repayment burden would rise automatically with asset prices to slow a credit boom 
and fall in a recession to reduce the chances of mortgage default. Mortgages would 
operate like a contractually pre-committed debt-equity swap between households and 
 
18 Bernanke, B and Gertler, M (1989), “Agency costs, net worth and business fluctuations”, American 
Economic Review, 14-31. 19 Shiller, R J (1993), Macro Markets, Oxford University Press. 
14
banks. They would automatically stabilise household loan-to-value ratios. By 
reducing the amplitude of the credit cycle, they ought to benefit both borrower and 
lender. 
Governments cannot issue equity. But this does not prevent them issuing debt with 
equity-like characteristics. For example, Robert Shiller has also suggested 
governments should issue GDP bonds, with coupons which vary with GDP. These 
would lower the public debt servicing burden at the point in the cycle when public 
deficits are likely to be largest. In this way, they would help smooth public 
expenditure and taxation over time. To date, GDP bonds have only been issued by 
some emerging markets. But in principle, they could serve as a quasi-automatic 
stabiliser for any country whose public debt experiences cyclical or crisis-related 
fluctuations.20 Given recent events, perhaps their day is nearing. 
Conclusion 
It is said that the longest journey begins with a single step. Events of the past twelve 
months have been a first step – and a big one. But they are just the start of the 
journey for the financial system and economy as balance sheets are repaired. This 
adjustment needs to be fast enough to repair balance sheets, but not so fast as to risk a 
setback for the financial system or real economy. What a hangover requires is neither 
a day in bed nor a night on the tiles. Having taken one big step forward, we should 
guard against taking two steps back. 

It is good to be back in Yorkshire. I say back because I grew up around 10 miles 
north of here. When I left over 20 years ago, Leeds looked and felt very different to 
today. Nowhere is the contrast greater than in the financial sector. In 1995, almost 
74,000 people were employed in financial and related business services, accounting 
for around 20% of employment in Leeds. By 2009 this had risen to over 116,000, or 
around 30% of employment. Today, Leeds has a legitimate claim to be the UK’s 
second largest financial centre. 
A Short History of Banking in Yorkshire 
The foundations for this success were laid much earlier. The history of banking in 
Yorkshire dates back over 250 years.1
 Pease and Co of Hull, established in 1754, are 
thought to be Yorkshire’s oldest private bank. In the same decade, banks were 
founded in Leeds and Bradford. By the end of the 18th century, Yorkshire had a wellestablished network of over 40 banks in around 16 regional towns and cities. 
In the first two decades of the 19th century, private banking in Yorkshire continued to 
thrive, spreading to around 30 towns and cities. The financial crisis of 1825 brought 
an end to this rapid growth. The crisis itself was interesting for its parallels with 
today. It was sourced in sub-prime lending in (in this case South) America. As fears 
of loan losses rose, runs began on banks throughout England, with more than a dozen 
institutions failing in Yorkshire alone. 
The crisis brought reform of the banking industry in the form of the Joint Stock 
Banking Act of 1826. This removed the Bank of England’s monopoly on joint stock 
banking. This had long been a bone of contention among country banks, not least in 
Yorkshire. The Bank of England itself opened its first branch in Yorkshire - here in 
Leeds, in fact - the following year in 1827. This was acknowledgement of Leeds’ 
emerging importance as a financial centre. Yet the reception it received was, to say 
the least, somewhat mixed. 
 
1
 W.C.E Hartley (1975), “Banking in Yorkshire”, Dalesman Books. The author completed this work 
while a Houblon-Norman fellow at the Bank of England. 
3
Local newspapers and businessmen attacked the Bank’s attempt to spread its 
“pestilential branches” into the regions.2
 Bank of England notes were shunned by 
local shopkeepers. The landlady of the Esk Inn near Whitby did so with the words: 
“I’ll ha’ nought to do with them things, I know nought about them; now if it 
had been a Simpson I would ha’ changed it with pleasure”3
It is easy to imagine, though probably impossible to print, what she might have made 
of Quantitative Easing. 
In the period since, banking and finance in Yorkshire have grown steadily and 
successfully. Among the success stories would be the Yorkshire Penny Bank. This 
was founded in 1859 in Halifax as a means of saving for the working classes. In 
1874, its first School Transfer Department was opened to encourage saving by school 
children. “Take care of the Pence and the Pounds will take care of themselves” 
intoned the bank’s posters. 
Such was the success of the school scheme that, by the end of the nineteenth century, 
the majority of schoolchildren in Yorkshire had a savings account. The scheme aimed 
to educate children from an early age on the benefits of thrift and financial planning. 
And having been nurtured early, the relationship between bank and customer often 
lasted a lifetime. In 1959, the Yorkshire Penny Bank became Yorkshire Bank which 
today remains one of the UK’s most successful medium-sized banks. 
The mutual or building society sector’s roots in Yorkshire are just as deep.4
 Although 
the first societies appeared in the Midlands in around 1775, within a decade Yorkshire 
had established its first society, the Hill House Bank Building Club in 1785. By the 
end of the century, numbers had swelled to around 50, mostly in the Midlands, 
Lancashire and Yorkshire. 
 
2
 Quoting Joseph Brook, a prominent Huddersfield banker, at a meeting of the Huddersfield Banking 
Corporation in January 1828. 
3
 Hartley (op.cit.). A “Simpson” was a note offered by local bank Simpson, Chapman and Co of 
Whitby. Through a sequence of mergers, this subsequently became part of what is today Barclays 
Bank. 
4
 See E.J. Cleary (1965), ‘The Building Society Movement’, Elek Books. See also S.J. Price (1958), 
‘Building Societies, their origin and history’, Franey and Company. 
4
The early mutuals pooled savings to buy property for members and terminated once 
all members’ housing needs were satisfied – so-called “terminating societies”. In 
1844, “permanent societies” were permitted, which allowed for a revolving set of both 
savers and borrowers. But the mutuality principle remained – “to enable a working 
man to secure himself in the course of a few years a dwelling-house as his own 
freehold property, as a home for himself and family.”5
Numbers of mutuals grew rapidly during the 19th century, reaching around 850 by 
1850 and over 1500 by 1875, with almost 100 in Yorkshire alone. Here in Leeds, the 
Leeds and Holbeck Permanent Building Society was founded in 1875 and, having 
been renamed Leeds Building Society in 2005, it remains in the top 10 building 
societies by size nationally. Consolidation and, latterly, demutualization have 
reduced numbers over the past few decades. Today, there are only a handful of 
independent, indigenous mutuals in the region. But they remain some of the strongest 
in the country. 
The success of Yorkshire’s financial sector over this lengthy period is no fluke. A 
reading of history reveals the same successful recipe being repeated. An awareness 
of, and responsiveness to, the needs of the customer, whether saver or borrower. A 
focus on long-term relationship-building, often starting from the earliest age. A 
recognition of the benefits of mutualising both risks and returns. In short, the 
importance of banks being built, first and foremost, on trust. 
The Crisis of Trust 
From the past, then, to the present. A year ago almost to the day, the investment bank 
Lehman Brothers filed for bankruptcy in the United States. Activity across the globe, 
financial and non-financial, froze. Recently, there are indications of some thawing. 
But a year on, many economies around the world remain mired in recession. What 
explains the severity of this crisis and how much longer can it be expected to last? 
 
5
 S.J. Price (1958), “Building Societies: Their Origin and History”, Franey and Company. 
5
The words “credit crunch” contain the seeds of an explanation. In Latin, credit means 
trust. So credit crunch is, in essence, a breakdown in trust. Between different parties 
at different times, that loss of trust has been the root cause of the devastating impact 
felt globally since the credit crunch began. It also explains why the road to recovery 
in credit, and thus in the real economy, may be long and winding. In essence, events 
of the past two years can be re-told as a story of the progressive breakdown in trust. 
The proximate cause of the crisis was a breakdown of trust between banks and 
households, specifically sub-prime mortgage-holders in the United States. The upshot 
was a loss of credit, and in many cases the homes, of these borrowers from 2006 
onwards. Repossession rates on US mortgages have more than tripled in the past two 
years (Chart 1). UK arrears and default rates, although lower, are rising. Some 
borrowers are now rationed out of the mortgage market. In July 2007, there were 
around 9500 sub-prime mortgage products being advertised in the UK. Today, there 
are virtually none (Chart 2). 
As losses on these mortgages and other toxic assets accumulated, trust among banks
was impaired. This saw a seizing-up of inter-bank money markets from the second 
half of 2007 onwards. Before the crisis, banks required about 10 basis points of 
compensation for making a three-month loan to one another (Chart 3). By September 
2007, that compensation premium had risen tenfold to around 100 basis points. By 
September 2008, it had risen more than thirty-fold from pre-crisis levels. It has since 
fallen back to around 35 basis points. These persistent funding pressures have 
constrained banks’ ability to lend to the real economy. 
Damaged by losses on assets and constrained by funding costs, questions began to be 
raised during 2008 about banks’ future profitability and, in some cases, viability. The 
equity prices of banks tumbled, falling 86% on average for the major UK banks 
between February 2007 and March this year (Chart 4). In money terms, that is a loss 
of market capitalisation of around £300bn, equivalent to 20% of annual UK GDP. 
Underlying these price falls was a generalised loss of trust between banks and 
investors in banks, such as sovereign wealth funds and mutual funds. 
6
What explained this wholesale loss of trust? In the run-up to the crisis, banks’ 
business models were increasingly predicated on making loans for onward sale. 
Loans became tradable securities and long-term relationships gave way to short-term 
transactions. The perils of this practice were well understood by Michael Marks, 
founder of Marks and Spencer and of course one of the region’s greatest-ever 
entrepreneurs: “You either make things or you sell them. Don’t try both”. 
Banks tried both, making loans with an eye to subsequently selling them. This had 
unintended, but in fact entirely predictable, consequences. Without skin in the game, 
banks’ due diligence became slipshod. The quality of tradable loans fell as their 
quantity rose. 
Investors in these securities were not as canny as the landlady of the Esk Inn: they 
purchased them in size even though “they knew nought about them”. By the time the 
penny had dropped for these investors, the pounds had not taken care of themselves. 
Global losses on these securitised assets are now believed to lie anywhere up to £3 
trillion.6
 As losses accumulated, trust in these securities was undermined and with it 
trust in the banks issuing them. 
As credit was cut, trust in the viability of some non-financial companies was 
questioned. Corporate distress began to rise internationally during the course of 2008. 
And as corporate distress rose, in particular after the failure of Lehman Brothers, 
distrust between companies mounted. The stream of trade credit extended between 
firms dried to a trickle at the end 2008 and in the first part of 2009. At that time, a 
survey by the Bank’s regional agents reported that around a quarter of contacts had 
had to turn down potentially profitable orders as a result of tighter trade credit. 
This progressive hardening of the credit arteries also had dramatic effects between 
countries. Cross-border flows of credit have reversed dramatically in the past year. 
“Home bias” by investors – a lack of trust in foreign investments - has returned with a 
vengeance. Cross-border lending by international banks grew by 20% per year 
 
6
 For example, Bank of England Financial Stability Report, June 2009. 
7
between 2003 and 2007. In 2008, it fell 5% (Chart 5). This has had adverse effects 
on UK companies, around a third of whose borrowing comes from foreign lenders. 
Through these successive waves, the world financial system found itself with almost 
every link in the credit chain – in the chain of trust - having been weakened or broken. 
That is evident in surveys of the public’s trust in industry: banking and finance are 
firmly rooted to the foot of the league table of trust, in the UK and internationally 
(Chart 6). That loss of trust is mirrored in aggregate bank lending in the UK, in 
particular to companies, where annual growth has fallen from a peak of 23% in March 
2008 to around zero now (Chart 7). And this has in turn been reflected in the largest 
and most synchronous global economic slowdown since the Great Depression. 
Confidence and Credit 
So how is trust, and thereby credit, to be restored? To date, the answer has been to 
rely on the one sector whose credit has not been seriously questioned - governments 
and central banks. There has been large-scale provision of government and central 
bank credit over the past two years in an attempt to ease pressures and shore up breaks 
in the private sector credit chain. 
These interventions have been unprecedented in size during peacetime. The total 
potentially on the table is in excess of $14 trillion.7
 That is roughly $2000 for every 
man, woman and child on the planet. Half of the world’s 20 largest banks have 
needed direct government support. Central bank balance sheets in the major financial 
centres, including in the UK, have more than doubled in size. And deposit insurance 
schemes have been extended in at least 40 countries around the world. 
Extending public sector credit on this scale relies on the deep pockets and prudence of 
our grandchildren. It can be no more than a stop-gap – a temporary bridge - until 
private sector trust can be restored. So far, the bridge has served its purpose. There 
are signs over recent months from surveys that confidence is returning to banks, non-
 
7
 Bank of England Financial Stability Report, June 2009. 
8
financial companies and consumers. And there have been some signs of a turnaround 
in the housing, equity and some debt markets. 
It has been said that every recession in history has been associated with a collapse in 
confidence.8
 This time’s was plainly no exception. So with confidence turning can 
we anticipate an imminent return of credit to the economy? 
Rising confidence among firms and consumers is a necessary condition for recovery. 
But it is questionable whether it is sufficient. That is because confidence and trust are 
subtly different concepts.9
 Confidence derives from observable, authoritative proof. 
At the time of the failure of Lehman Brothers, people struggled to make sense of the 
state of the economy and financial system. Without a compass, they lost their 
financial bearings. Lacking authoritative proof, confidence collapsed. As the banking 
system has since stabilised, people’s bearings have returned and with them 
confidence. 
Trust is an altogether different animal. It is based on beliefs, not observable proofs. 
It is grounded in perceptions rather than evidence. It is as much a psychological state 
as a financial one.10 A clean balance sheet might instil confidence, but it need not 
repair trust. Because it is a moral judgement, repairing trust can be a slow and 
painstaking business. Moral compasses take rather longer to self-correct than 
magnetic ones. This has implications for the path of recovery in the period ahead. 
Historically, credit has tended to lag the recovery in output in the majority of 
recessions, especially financial recessions. During the previous three major 
recessions in the UK – in the 1970s, 1980s and 1990s – credit to business recovered 
slowly and in some cases only several years after the recovery in activity (Charts 8, 9 
and 10). This is consistent with trust between financial institutions and their 
customers being slower to recover than confidence more generally. 
 
8
 “Animal Spirits”, George Akerlof and Robert J Shiller (2009), Princeton University Press. 9
 For example, see “Restoring Confidence and Trust in UK PLC”, Henley Business School (2009). 10 David Tuckett provides a fascinating account of the crisis and its aftermath using psychological 
theories and evidence (“Addressing the Psychology of Financial Markets”, Institute for Public Policy 
Research, May 2009). 
9
Moreover, unlike the situation today, earlier recessions in the UK were not primarily 
the result of financial factors. International evidence suggests that financial 
recessions have tended on average to be both costlier and lengthier.11 Normal 
recessions have been associated with a recovery in output to its previous peak after 
around 3 ½ quarters. Recovery to peak output following financial recessions has on 
average taken around 5 ½ quarters (Chart 11). 
Like its predecessors, lack of confidence may have “caused” this time’s recession. 
But it is lack of trust – and hence credit – that may shape the recovery. Based on past 
evidence, as the Governor has noted recently, we might anticipate a protracted period 
of repair. 
Repairing Trust – The Low Road to Reform 
So what might be needed, beyond time, to repair trust in the financial system? A raft 
of reforms to the financial system, nationally and internationally, is currently being 
assembled. These measures will aim to strengthen banks’ financial resources, risk 
management practices and governance. They are about bringing regulatory rules into 
the 21st century. 
This is the high road to reform – for example, higher buffers of capital and liquidity 
and higher standards of risk management. If successful, these reforms will help 
cleanse bank balance sheets. It is an open question, however, whether these efforts 
will be sufficient to restore public trust in the financial system. 
One reason why regulation might not be the whole answer is that trust in financial 
regulation is itself one of the casualties of crisis. Regulation is seen by some as part 
of the problem, not the solution. More generally, in repairing public trust, it would be 
preferable if banks were seen to be initiating root and branch reform themselves, 
rather than having it thrust upon them by regulators. 
 
11 “Is the US Sub-Prime Crisis So Different? An International Comparison”, Carmen Reinhart and 
Kenneth Rogoff (2008), NBER Working Paper No.13761. 
10
This would be the low road to reform. Low because it would not require any new or 
complex regulatory apparatus. Low because it would not need international 
negotiation or agreement on the contractual fineprint. Instead, what it would require 
is a self-generated sea-change in the structure and strategy of banking. 
So what changes in structure and strategy might be desirable? Without suggesting 
definitive answers, let me discuss three areas where further debate might be useful: 
banks’ size in relation to the services they provide; banks’ strategy in relation to their 
resilience; and banks’ governance in relation to the incentives this creates. 
• Structure - size versus service provision 
Economies of scale typically arise in the production of goods and services which are 
homogenous and replicable. Henry Ford applied this principle to car manufacture 
through his Ford Motor Company, established almost a century ago. It was a success. 
That model has since served many industries well. 
But manufacturing loans is not the same as manufacturing cars. Loans are neither 
homogenous nor replicable. Making loans relies on bespoke, customer-specific 
information. This information is not obtained by computer algorithm or credit rating 
agency but through a banking relationship, ideally a long-term one. Despite the 
advent of social networking, economies of scale are not something we typically 
associate with long-term relationships. 
So the economics of banking do not suggest that bigger need be better. Indeed, if 
large-scale processing of loans risks economising on the collection of information, 
there might even be diseconomies of scale in banking. The present crisis provides a 
case study. The desire to make loans a tradable commodity led to a loss of 
information, as transactions replaced relationships and quantity trumped quality. 
Within the space of a decade, banks went from monogamy to speed-dating. 
Evidence from a range of countries paints a revealing picture. There is not a scrap of 
evidence of economies of scale or scope in banking – of bigger or broader being 
11
better - beyond a low size threshold.12 At least during this crisis, big banks have if 
anything been found to be less stable than their smaller counterparts, requiring on 
average larger-scale support.13 It could be argued that big business needs big banks to 
supply their needs. But this is not an argument that big businesses themselves 
endorse, at least according to a recent survey by the Association of Corporate 
Treasurers.14
Take Grameen Bank – not a household name in the UK, I realise. This grew out of a 
micro-finance project in Bangladesh which began in 1976.15 Grameen operates as a 
set of local credit cooperatives, often comprising as few as five members. The bank’s 
relationship with its borrowers is bound not by legal contract but by trust. Like the 
Yorkshire Penny Bank a century earlier, it aims to encourage saving by the poor and 
supports the education of the children of borrowers and savers. 
In some respects, Grameen Bank is about as basic and small-scale a set of banking 
arrangements as it is possible to conceive. But its success is only too clear. From the 
most modest of beginnings, Grameen Bank now operates in over 40 countries 
worldwide, with over 2000 branches and over 7.5 million borrowers. Grameen is a 
local bank gone global. 
Henry Ford grew an empire on scale economies, centralisation and replication. For 
the Ford Motor Company, bigger was better. At around the same time, Alfred P 
Sloan of General Motors was following a different business model. Size still 
mattered. But production was decentralised and specialised. The focus was on 
customer needs supported by some of the first-ever market research on their tastes. In 
the end, it was Sloan and General Motors whose strategy was emulated around the 
world. 
 
12 See A. Saunders (1996), ‘Financial Institutions Management: A Modern Perspective’, Irwin 
Professional Publishing. Also see D. Amel, C. Barnes, F. Panetta and C. Salleo (2004), ‘Consolidation 
and efficiency in the financial sector: A review of the international evidence’, Journal of Banking & 
Finance Volume 28, Issue 10. 13 Haldane (2009), ‘Small Lessons From a Big Crisis’. See
http://www.bankofengland.co.uk/publications/speeches/2009/speech397.pdf 14 The Association of Corporate Treasurers (2009), ‘Comments in response to Turner Review: a 
regulatory response to the global banking crisis and the accompanying FSA discussion paper DP 
09/02 – A regulatory response to the global banking crisis’. 15 David Bornstein (1996), ‘The Price of a Dream: The Story of the Grameen Bank”, Oxford 
University Press.
12
In meeting the future needs of the real economy, perhaps there is a case for more 
“Sloans” and fewer “Fords” in banking. Perhaps there is a case for a strategic focus 
on the ‘local’ as much as the ‘global’, for more micro-finance and less macro-finance. 
Perhaps it is time for relationship-banking to make a comeback. 
• Strategy - diversity versus diversification 
Customers require a basket of banking services. The provision of some of these 
services is important for the wider economy; they are a quasi-public good. For 
example, the provision of monetary services – basic banking – has a strong public 
good element. That is why depositor protection, in the form of deposit insurance, is 
stipulated by the state in many countries. Lending to households and companies can 
also be thought to contain a public good element. Other functions performed by 
banks may provide more limited social benefits, though their private benefit may be 
significant – for example, proprietary trading in complex instruments. 
If scale economies in banking are weak, banking services could probably be equally 
well provided by either a financial supermarket or a set of specialist banks. These 
different structures might not, however, be equivalent from a risk perspective. The 
supermarket model potentially does offer some risk benefits - the benefits of 
diversification. Profitable business lines can compensate for temporarily nonprofitable ones. What is lost on the swings may be regained on the roundabouts. 
This is a story about which we have heard much over the past few months, especially 
among the biggest global banks. In the first half of this year, big losses on the 
banking book swings have been more than offset by big gains on the trading book 
roundabouts. Diversification, we were told, was paying dividends – and, indeed, 
bonuses. 
But memories can be deceptively short. Rewind the clock one year. Then, it was 
trading book losses that were eroding confidence in the banks. This in turn prompted 
fears about some banks’ solvency, aggravating the very recession which is now 
13
generating banking book losses. Trading book gains may well be acting as a hedge 
today. But they are hedging a risk they helped create. 
All of this implies that business line diversification can be a double-edged sword. 
During the upswing, banks enjoyed windfall gains from bets at the race-track. This 
boosted their buffers. But when those bets turned sour, these same activities put at 
risk banks’ day job – the provision of loan and deposit services to the real economy. 
9500 sub-prime mortgage products at the height of the boom might well have been 
too many. But zero is surely too few. 
There is a second important downside to diversification. While it might be sensible 
for an individual firm to diversify its business lines to reduce its risk, if this same 
strategy is followed by all banks the end-result may be greater fragility across the 
whole system. Why? Because in their desire to look different than in the past, banks’ 
business strategies may end up looking identical in the present. The financial system 
could then become more prone to herd-like upswings and lemming-like downswings. 
There is more than a hint of this behaviour during the run-up to crisis. Banking 
strategies became a whirligig. Building societies transformed themselves into 
commercial banks. Commercial banks tried their hand at investment banking. 
Investment banks developed in-house hedge funds through large proprietary trading 
desks. Hedge funds competed with traditional investment funds. And to complete the 
circle, these investment funds imported the risk all of the others were shedding. In 
their desire to diversify, individual banks generated a lack of diversity, and thus 
resilience, for the financial system as a whole. 
So recent crisis experience highlights some of the costs of bundling banking services. 
Given that, there is an intellectually defensible case for some unbundling of these 
services. This would reduce the risks of spill-over between privately and socially 
beneficial banking activities. And it would help prevent banks making individually 
rational but collectively calamitous strategic choices. 
There are plenty of examples from the non-financial world, from genetics to geology, 
of diversity improving the robustness of systems. The financial system may be no 
14
different. The resulting financial landscape would, however, look rather different. 
There would be greater specialisation and diversity. The shopping experience for a 
banking customer would be more farmers’ market than supermarket. 
• Governance - stakeholders versus shareholders
In deciding appropriate corporate governance arrangements, economists have tended 
to focus on the relationship between shareholders (as principal) and managers (as 
agent). Principal-agent problems arise when these two parties’ incentives are 
misaligned. Profit-related pay is one means of achieving alignment, with managers 
remunerated in line with shareholder returns. In addressing one incentive problem, 
however, this approach may risk worsening two others – between shareholders and 
depositors on the one hand and between shareholders and the public sector on the 
other. 
Limited liability means that returns to shareholders are capped below at zero, but not 
above. That provides a natural incentive for owners to gamble, pursuing high 
risk/high return strategies from which they import the return upside but export the risk 
downside to depositors or the public sector. During this crisis, the pursuit of those 
strategies has resulted in the public sector picking up the cheque for the downside in 
an effort to reduce risks to depositors. 
A mutual model of corporate governance gives rise to a rather different set of 
incentives. Instead of external shareholders, savers and borrowers become the 
owners, with profits remitted directly to them in the form of higher deposit rates 
and/or lower borrowing rates. For that reason, measured profit margins appear 
consistently lower for building societies than for banks. 
Unlike in most industries, however, low margins for a mutual are very unlikely to 
result in shareholder revolt or a missive to management to gear-up or get-out. They 
are a reflection of corporate governance working for stakeholders, here defined 
broadly to include depositors and borrowers. In this way, mutuality reduces the scope 
for misaligned incentives between shareholders and depositors which might otherwise 
arise in a joint stock bank. 
15
This is not to suggest that mutuality is a panacea. For example, it does not address the 
potential incentive problem between stakeholders and the public sector. Indeed, it 
could even potentially worsen this problem as shareholders and depositors are one and 
the same under mutuality and so must rank equally in the event of a payout from the 
public purse. Moreover, profit margins in the building society sector are currently 
under some pressure as a result of low Bank rate and increased competition for prime 
residential loans. Some building societies have also hit bad loan problems. 
Although painful in the short-term, these margin pressures ought in time to abate. So 
in the heartland of mutuality, I am happy to say that reports of the death of the 
building society sector are greatly exaggerated. Indeed, mutuality may do a better job 
of aligning stakeholder incentives than some alternative forms of corporate 
governance. It is a depressing but telling fact that, of the demutualised former UK 
building societies, none is today in independent ownership. 
Thrift, mutuality and relationship-building have long underpinned banking in 
Yorkshire. These principles went missing in the run-up to the present crisis. The 
costs of that vanishing act are now all too apparent. In rebuilding the financial 
system, to create one which is both stable and better able to meet the needs of the real 
economy, these principles need to be rediscovered. They offer a tried and tested – 
indeed, trusted – roadmap for the period ahead.

It is good to be back in Yorkshire. I say back because I grew up around 10 miles 
north of here. When I left over 20 years ago, Leeds looked and felt very different to 
today. Nowhere is the contrast greater than in the financial sector. In 1995, almost 
74,000 people were employed in financial and related business services, accounting 
for around 20% of employment in Leeds. By 2009 this had risen to over 116,000, or 
around 30% of employment. Today, Leeds has a legitimate claim to be the UK’s 
second largest financial centre. 
A Short History of Banking in Yorkshire 
The foundations for this success were laid much earlier. The history of banking in 
Yorkshire dates back over 250 years.1
 Pease and Co of Hull, established in 1754, are 
thought to be Yorkshire’s oldest private bank. In the same decade, banks were 
founded in Leeds and Bradford. By the end of the 18th century, Yorkshire had a wellestablished network of over 40 banks in around 16 regional towns and cities. 
In the first two decades of the 19th century, private banking in Yorkshire continued to 
thrive, spreading to around 30 towns and cities. The financial crisis of 1825 brought 
an end to this rapid growth. The crisis itself was interesting for its parallels with 
today. It was sourced in sub-prime lending in (in this case South) America. As fears 
of loan losses rose, runs began on banks throughout England, with more than a dozen 
institutions failing in Yorkshire alone. 
The crisis brought reform of the banking industry in the form of the Joint Stock 
Banking Act of 1826. This removed the Bank of England’s monopoly on joint stock 
banking. This had long been a bone of contention among country banks, not least in 
Yorkshire. The Bank of England itself opened its first branch in Yorkshire - here in 
Leeds, in fact - the following year in 1827. This was acknowledgement of Leeds’ 
emerging importance as a financial centre. Yet the reception it received was, to say 
the least, somewhat mixed. 
 
1
 W.C.E Hartley (1975), “Banking in Yorkshire”, Dalesman Books. The author completed this work 
while a Houblon-Norman fellow at the Bank of England. 
3
Local newspapers and businessmen attacked the Bank’s attempt to spread its 
“pestilential branches” into the regions.2
 Bank of England notes were shunned by 
local shopkeepers. The landlady of the Esk Inn near Whitby did so with the words: 
“I’ll ha’ nought to do with them things, I know nought about them; now if it 
had been a Simpson I would ha’ changed it with pleasure”3
It is easy to imagine, though probably impossible to print, what she might have made 
of Quantitative Easing. 
In the period since, banking and finance in Yorkshire have grown steadily and 
successfully. Among the success stories would be the Yorkshire Penny Bank. This 
was founded in 1859 in Halifax as a means of saving for the working classes. In 
1874, its first School Transfer Department was opened to encourage saving by school 
children. “Take care of the Pence and the Pounds will take care of themselves” 
intoned the bank’s posters. 
Such was the success of the school scheme that, by the end of the nineteenth century, 
the majority of schoolchildren in Yorkshire had a savings account. The scheme aimed 
to educate children from an early age on the benefits of thrift and financial planning. 
And having been nurtured early, the relationship between bank and customer often 
lasted a lifetime. In 1959, the Yorkshire Penny Bank became Yorkshire Bank which 
today remains one of the UK’s most successful medium-sized banks. 
The mutual or building society sector’s roots in Yorkshire are just as deep.4
 Although 
the first societies appeared in the Midlands in around 1775, within a decade Yorkshire 
had established its first society, the Hill House Bank Building Club in 1785. By the 
end of the century, numbers had swelled to around 50, mostly in the Midlands, 
Lancashire and Yorkshire. 
 
2
 Quoting Joseph Brook, a prominent Huddersfield banker, at a meeting of the Huddersfield Banking 
Corporation in January 1828. 
3
 Hartley (op.cit.). A “Simpson” was a note offered by local bank Simpson, Chapman and Co of 
Whitby. Through a sequence of mergers, this subsequently became part of what is today Barclays 
Bank. 
4
 See E.J. Cleary (1965), ‘The Building Society Movement’, Elek Books. See also S.J. Price (1958), 
‘Building Societies, their origin and history’, Franey and Company. 
4
The early mutuals pooled savings to buy property for members and terminated once 
all members’ housing needs were satisfied – so-called “terminating societies”. In 
1844, “permanent societies” were permitted, which allowed for a revolving set of both 
savers and borrowers. But the mutuality principle remained – “to enable a working 
man to secure himself in the course of a few years a dwelling-house as his own 
freehold property, as a home for himself and family.”5
Numbers of mutuals grew rapidly during the 19th century, reaching around 850 by 
1850 and over 1500 by 1875, with almost 100 in Yorkshire alone. Here in Leeds, the 
Leeds and Holbeck Permanent Building Society was founded in 1875 and, having 
been renamed Leeds Building Society in 2005, it remains in the top 10 building 
societies by size nationally. Consolidation and, latterly, demutualization have 
reduced numbers over the past few decades. Today, there are only a handful of 
independent, indigenous mutuals in the region. But they remain some of the strongest 
in the country. 
The success of Yorkshire’s financial sector over this lengthy period is no fluke. A 
reading of history reveals the same successful recipe being repeated. An awareness 
of, and responsiveness to, the needs of the customer, whether saver or borrower. A 
focus on long-term relationship-building, often starting from the earliest age. A 
recognition of the benefits of mutualising both risks and returns. In short, the 
importance of banks being built, first and foremost, on trust. 
The Crisis of Trust 
From the past, then, to the present. A year ago almost to the day, the investment bank 
Lehman Brothers filed for bankruptcy in the United States. Activity across the globe, 
financial and non-financial, froze. Recently, there are indications of some thawing. 
But a year on, many economies around the world remain mired in recession. What 
explains the severity of this crisis and how much longer can it be expected to last? 
 
5
 S.J. Price (1958), “Building Societies: Their Origin and History”, Franey and Company. 
5
The words “credit crunch” contain the seeds of an explanation. In Latin, credit means 
trust. So credit crunch is, in essence, a breakdown in trust. Between different parties 
at different times, that loss of trust has been the root cause of the devastating impact 
felt globally since the credit crunch began. It also explains why the road to recovery 
in credit, and thus in the real economy, may be long and winding. In essence, events 
of the past two years can be re-told as a story of the progressive breakdown in trust. 
The proximate cause of the crisis was a breakdown of trust between banks and 
households, specifically sub-prime mortgage-holders in the United States. The upshot 
was a loss of credit, and in many cases the homes, of these borrowers from 2006 
onwards. Repossession rates on US mortgages have more than tripled in the past two 
years (Chart 1). UK arrears and default rates, although lower, are rising. Some 
borrowers are now rationed out of the mortgage market. In July 2007, there were 
around 9500 sub-prime mortgage products being advertised in the UK. Today, there 
are virtually none (Chart 2). 
As losses on these mortgages and other toxic assets accumulated, trust among banks
was impaired. This saw a seizing-up of inter-bank money markets from the second 
half of 2007 onwards. Before the crisis, banks required about 10 basis points of 
compensation for making a three-month loan to one another (Chart 3). By September 
2007, that compensation premium had risen tenfold to around 100 basis points. By 
September 2008, it had risen more than thirty-fold from pre-crisis levels. It has since 
fallen back to around 35 basis points. These persistent funding pressures have 
constrained banks’ ability to lend to the real economy. 
Damaged by losses on assets and constrained by funding costs, questions began to be 
raised during 2008 about banks’ future profitability and, in some cases, viability. The 
equity prices of banks tumbled, falling 86% on average for the major UK banks 
between February 2007 and March this year (Chart 4). In money terms, that is a loss 
of market capitalisation of around £300bn, equivalent to 20% of annual UK GDP. 
Underlying these price falls was a generalised loss of trust between banks and 
investors in banks, such as sovereign wealth funds and mutual funds. 
6
What explained this wholesale loss of trust? In the run-up to the crisis, banks’ 
business models were increasingly predicated on making loans for onward sale. 
Loans became tradable securities and long-term relationships gave way to short-term 
transactions. The perils of this practice were well understood by Michael Marks, 
founder of Marks and Spencer and of course one of the region’s greatest-ever 
entrepreneurs: “You either make things or you sell them. Don’t try both”. 
Banks tried both, making loans with an eye to subsequently selling them. This had 
unintended, but in fact entirely predictable, consequences. Without skin in the game, 
banks’ due diligence became slipshod. The quality of tradable loans fell as their 
quantity rose. 
Investors in these securities were not as canny as the landlady of the Esk Inn: they 
purchased them in size even though “they knew nought about them”. By the time the 
penny had dropped for these investors, the pounds had not taken care of themselves. 
Global losses on these securitised assets are now believed to lie anywhere up to £3 
trillion.6
 As losses accumulated, trust in these securities was undermined and with it 
trust in the banks issuing them. 
As credit was cut, trust in the viability of some non-financial companies was 
questioned. Corporate distress began to rise internationally during the course of 2008. 
And as corporate distress rose, in particular after the failure of Lehman Brothers, 
distrust between companies mounted. The stream of trade credit extended between 
firms dried to a trickle at the end 2008 and in the first part of 2009. At that time, a 
survey by the Bank’s regional agents reported that around a quarter of contacts had 
had to turn down potentially profitable orders as a result of tighter trade credit. 
This progressive hardening of the credit arteries also had dramatic effects between 
countries. Cross-border flows of credit have reversed dramatically in the past year. 
“Home bias” by investors – a lack of trust in foreign investments - has returned with a 
vengeance. Cross-border lending by international banks grew by 20% per year 
 
6
 For example, Bank of England Financial Stability Report, June 2009. 
7
between 2003 and 2007. In 2008, it fell 5% (Chart 5). This has had adverse effects 
on UK companies, around a third of whose borrowing comes from foreign lenders. 
Through these successive waves, the world financial system found itself with almost 
every link in the credit chain – in the chain of trust - having been weakened or broken. 
That is evident in surveys of the public’s trust in industry: banking and finance are 
firmly rooted to the foot of the league table of trust, in the UK and internationally 
(Chart 6). That loss of trust is mirrored in aggregate bank lending in the UK, in 
particular to companies, where annual growth has fallen from a peak of 23% in March 
2008 to around zero now (Chart 7). And this has in turn been reflected in the largest 
and most synchronous global economic slowdown since the Great Depression. 
Confidence and Credit 
So how is trust, and thereby credit, to be restored? To date, the answer has been to 
rely on the one sector whose credit has not been seriously questioned - governments 
and central banks. There has been large-scale provision of government and central 
bank credit over the past two years in an attempt to ease pressures and shore up breaks 
in the private sector credit chain. 
These interventions have been unprecedented in size during peacetime. The total 
potentially on the table is in excess of $14 trillion.7
 That is roughly $2000 for every 
man, woman and child on the planet. Half of the world’s 20 largest banks have 
needed direct government support. Central bank balance sheets in the major financial 
centres, including in the UK, have more than doubled in size. And deposit insurance 
schemes have been extended in at least 40 countries around the world. 
Extending public sector credit on this scale relies on the deep pockets and prudence of 
our grandchildren. It can be no more than a stop-gap – a temporary bridge - until 
private sector trust can be restored. So far, the bridge has served its purpose. There 
are signs over recent months from surveys that confidence is returning to banks, non-
 
7
 Bank of England Financial Stability Report, June 2009. 
8
financial companies and consumers. And there have been some signs of a turnaround 
in the housing, equity and some debt markets. 
It has been said that every recession in history has been associated with a collapse in 
confidence.8
 This time’s was plainly no exception. So with confidence turning can 
we anticipate an imminent return of credit to the economy? 
Rising confidence among firms and consumers is a necessary condition for recovery. 
But it is questionable whether it is sufficient. That is because confidence and trust are 
subtly different concepts.9
 Confidence derives from observable, authoritative proof. 
At the time of the failure of Lehman Brothers, people struggled to make sense of the 
state of the economy and financial system. Without a compass, they lost their 
financial bearings. Lacking authoritative proof, confidence collapsed. As the banking 
system has since stabilised, people’s bearings have returned and with them 
confidence. 
Trust is an altogether different animal. It is based on beliefs, not observable proofs. 
It is grounded in perceptions rather than evidence. It is as much a psychological state 
as a financial one.10 A clean balance sheet might instil confidence, but it need not 
repair trust. Because it is a moral judgement, repairing trust can be a slow and 
painstaking business. Moral compasses take rather longer to self-correct than 
magnetic ones. This has implications for the path of recovery in the period ahead. 
Historically, credit has tended to lag the recovery in output in the majority of 
recessions, especially financial recessions. During the previous three major 
recessions in the UK – in the 1970s, 1980s and 1990s – credit to business recovered 
slowly and in some cases only several years after the recovery in activity (Charts 8, 9 
and 10). This is consistent with trust between financial institutions and their 
customers being slower to recover than confidence more generally. 
 
8
 “Animal Spirits”, George Akerlof and Robert J Shiller (2009), Princeton University Press. 9
 For example, see “Restoring Confidence and Trust in UK PLC”, Henley Business School (2009). 10 David Tuckett provides a fascinating account of the crisis and its aftermath using psychological 
theories and evidence (“Addressing the Psychology of Financial Markets”, Institute for Public Policy 
Research, May 2009). 
9
Moreover, unlike the situation today, earlier recessions in the UK were not primarily 
the result of financial factors. International evidence suggests that financial 
recessions have tended on average to be both costlier and lengthier.11 Normal 
recessions have been associated with a recovery in output to its previous peak after 
around 3 ½ quarters. Recovery to peak output following financial recessions has on 
average taken around 5 ½ quarters (Chart 11). 
Like its predecessors, lack of confidence may have “caused” this time’s recession. 
But it is lack of trust – and hence credit – that may shape the recovery. Based on past 
evidence, as the Governor has noted recently, we might anticipate a protracted period 
of repair. 
Repairing Trust – The Low Road to Reform 
So what might be needed, beyond time, to repair trust in the financial system? A raft 
of reforms to the financial system, nationally and internationally, is currently being 
assembled. These measures will aim to strengthen banks’ financial resources, risk 
management practices and governance. They are about bringing regulatory rules into 
the 21st century. 
This is the high road to reform – for example, higher buffers of capital and liquidity 
and higher standards of risk management. If successful, these reforms will help 
cleanse bank balance sheets. It is an open question, however, whether these efforts 
will be sufficient to restore public trust in the financial system. 
One reason why regulation might not be the whole answer is that trust in financial 
regulation is itself one of the casualties of crisis. Regulation is seen by some as part 
of the problem, not the solution. More generally, in repairing public trust, it would be 
preferable if banks were seen to be initiating root and branch reform themselves, 
rather than having it thrust upon them by regulators. 
 
11 “Is the US Sub-Prime Crisis So Different? An International Comparison”, Carmen Reinhart and 
Kenneth Rogoff (2008), NBER Working Paper No.13761. 
10
This would be the low road to reform. Low because it would not require any new or 
complex regulatory apparatus. Low because it would not need international 
negotiation or agreement on the contractual fineprint. Instead, what it would require 
is a self-generated sea-change in the structure and strategy of banking. 
So what changes in structure and strategy might be desirable? Without suggesting 
definitive answers, let me discuss three areas where further debate might be useful: 
banks’ size in relation to the services they provide; banks’ strategy in relation to their 
resilience; and banks’ governance in relation to the incentives this creates. 
• Structure - size versus service provision 
Economies of scale typically arise in the production of goods and services which are 
homogenous and replicable. Henry Ford applied this principle to car manufacture 
through his Ford Motor Company, established almost a century ago. It was a success. 
That model has since served many industries well. 
But manufacturing loans is not the same as manufacturing cars. Loans are neither 
homogenous nor replicable. Making loans relies on bespoke, customer-specific 
information. This information is not obtained by computer algorithm or credit rating 
agency but through a banking relationship, ideally a long-term one. Despite the 
advent of social networking, economies of scale are not something we typically 
associate with long-term relationships. 
So the economics of banking do not suggest that bigger need be better. Indeed, if 
large-scale processing of loans risks economising on the collection of information, 
there might even be diseconomies of scale in banking. The present crisis provides a 
case study. The desire to make loans a tradable commodity led to a loss of 
information, as transactions replaced relationships and quantity trumped quality. 
Within the space of a decade, banks went from monogamy to speed-dating. 
Evidence from a range of countries paints a revealing picture. There is not a scrap of 
evidence of economies of scale or scope in banking – of bigger or broader being 
11
better - beyond a low size threshold.12 At least during this crisis, big banks have if 
anything been found to be less stable than their smaller counterparts, requiring on 
average larger-scale support.13 It could be argued that big business needs big banks to 
supply their needs. But this is not an argument that big businesses themselves 
endorse, at least according to a recent survey by the Association of Corporate 
Treasurers.14
Take Grameen Bank – not a household name in the UK, I realise. This grew out of a 
micro-finance project in Bangladesh which began in 1976.15 Grameen operates as a 
set of local credit cooperatives, often comprising as few as five members. The bank’s 
relationship with its borrowers is bound not by legal contract but by trust. Like the 
Yorkshire Penny Bank a century earlier, it aims to encourage saving by the poor and 
supports the education of the children of borrowers and savers. 
In some respects, Grameen Bank is about as basic and small-scale a set of banking 
arrangements as it is possible to conceive. But its success is only too clear. From the 
most modest of beginnings, Grameen Bank now operates in over 40 countries 
worldwide, with over 2000 branches and over 7.5 million borrowers. Grameen is a 
local bank gone global. 
Henry Ford grew an empire on scale economies, centralisation and replication. For 
the Ford Motor Company, bigger was better. At around the same time, Alfred P 
Sloan of General Motors was following a different business model. Size still 
mattered. But production was decentralised and specialised. The focus was on 
customer needs supported by some of the first-ever market research on their tastes. In 
the end, it was Sloan and General Motors whose strategy was emulated around the 
world. 
 
12 See A. Saunders (1996), ‘Financial Institutions Management: A Modern Perspective’, Irwin 
Professional Publishing. Also see D. Amel, C. Barnes, F. Panetta and C. Salleo (2004), ‘Consolidation 
and efficiency in the financial sector: A review of the international evidence’, Journal of Banking & 
Finance Volume 28, Issue 10. 13 Haldane (2009), ‘Small Lessons From a Big Crisis’. See
http://www.bankofengland.co.uk/publications/speeches/2009/speech397.pdf 14 The Association of Corporate Treasurers (2009), ‘Comments in response to Turner Review: a 
regulatory response to the global banking crisis and the accompanying FSA discussion paper DP 
09/02 – A regulatory response to the global banking crisis’. 15 David Bornstein (1996), ‘The Price of a Dream: The Story of the Grameen Bank”, Oxford 
University Press.
12
In meeting the future needs of the real economy, perhaps there is a case for more 
“Sloans” and fewer “Fords” in banking. Perhaps there is a case for a strategic focus 
on the ‘local’ as much as the ‘global’, for more micro-finance and less macro-finance. 
Perhaps it is time for relationship-banking to make a comeback. 
• Strategy - diversity versus diversification 
Customers require a basket of banking services. The provision of some of these 
services is important for the wider economy; they are a quasi-public good. For 
example, the provision of monetary services – basic banking – has a strong public 
good element. That is why depositor protection, in the form of deposit insurance, is 
stipulated by the state in many countries. Lending to households and companies can 
also be thought to contain a public good element. Other functions performed by 
banks may provide more limited social benefits, though their private benefit may be 
significant – for example, proprietary trading in complex instruments. 
If scale economies in banking are weak, banking services could probably be equally 
well provided by either a financial supermarket or a set of specialist banks. These 
different structures might not, however, be equivalent from a risk perspective. The 
supermarket model potentially does offer some risk benefits - the benefits of 
diversification. Profitable business lines can compensate for temporarily nonprofitable ones. What is lost on the swings may be regained on the roundabouts. 
This is a story about which we have heard much over the past few months, especially 
among the biggest global banks. In the first half of this year, big losses on the 
banking book swings have been more than offset by big gains on the trading book 
roundabouts. Diversification, we were told, was paying dividends – and, indeed, 
bonuses. 
But memories can be deceptively short. Rewind the clock one year. Then, it was 
trading book losses that were eroding confidence in the banks. This in turn prompted 
fears about some banks’ solvency, aggravating the very recession which is now 
13
generating banking book losses. Trading book gains may well be acting as a hedge 
today. But they are hedging a risk they helped create. 
All of this implies that business line diversification can be a double-edged sword. 
During the upswing, banks enjoyed windfall gains from bets at the race-track. This 
boosted their buffers. But when those bets turned sour, these same activities put at 
risk banks’ day job – the provision of loan and deposit services to the real economy. 
9500 sub-prime mortgage products at the height of the boom might well have been 
too many. But zero is surely too few. 
There is a second important downside to diversification. While it might be sensible 
for an individual firm to diversify its business lines to reduce its risk, if this same 
strategy is followed by all banks the end-result may be greater fragility across the 
whole system. Why? Because in their desire to look different than in the past, banks’ 
business strategies may end up looking identical in the present. The financial system 
could then become more prone to herd-like upswings and lemming-like downswings. 
There is more than a hint of this behaviour during the run-up to crisis. Banking 
strategies became a whirligig. Building societies transformed themselves into 
commercial banks. Commercial banks tried their hand at investment banking. 
Investment banks developed in-house hedge funds through large proprietary trading 
desks. Hedge funds competed with traditional investment funds. And to complete the 
circle, these investment funds imported the risk all of the others were shedding. In 
their desire to diversify, individual banks generated a lack of diversity, and thus 
resilience, for the financial system as a whole. 
So recent crisis experience highlights some of the costs of bundling banking services. 
Given that, there is an intellectually defensible case for some unbundling of these 
services. This would reduce the risks of spill-over between privately and socially 
beneficial banking activities. And it would help prevent banks making individually 
rational but collectively calamitous strategic choices. 
There are plenty of examples from the non-financial world, from genetics to geology, 
of diversity improving the robustness of systems. The financial system may be no 
14
different. The resulting financial landscape would, however, look rather different. 
There would be greater specialisation and diversity. The shopping experience for a 
banking customer would be more farmers’ market than supermarket. 
• Governance - stakeholders versus shareholders
In deciding appropriate corporate governance arrangements, economists have tended 
to focus on the relationship between shareholders (as principal) and managers (as 
agent). Principal-agent problems arise when these two parties’ incentives are 
misaligned. Profit-related pay is one means of achieving alignment, with managers 
remunerated in line with shareholder returns. In addressing one incentive problem, 
however, this approach may risk worsening two others – between shareholders and 
depositors on the one hand and between shareholders and the public sector on the 
other. 
Limited liability means that returns to shareholders are capped below at zero, but not 
above. That provides a natural incentive for owners to gamble, pursuing high 
risk/high return strategies from which they import the return upside but export the risk 
downside to depositors or the public sector. During this crisis, the pursuit of those 
strategies has resulted in the public sector picking up the cheque for the downside in 
an effort to reduce risks to depositors. 
A mutual model of corporate governance gives rise to a rather different set of 
incentives. Instead of external shareholders, savers and borrowers become the 
owners, with profits remitted directly to them in the form of higher deposit rates 
and/or lower borrowing rates. For that reason, measured profit margins appear 
consistently lower for building societies than for banks. 
Unlike in most industries, however, low margins for a mutual are very unlikely to 
result in shareholder revolt or a missive to management to gear-up or get-out. They 
are a reflection of corporate governance working for stakeholders, here defined 
broadly to include depositors and borrowers. In this way, mutuality reduces the scope 
for misaligned incentives between shareholders and depositors which might otherwise 
arise in a joint stock bank. 
15
This is not to suggest that mutuality is a panacea. For example, it does not address the 
potential incentive problem between stakeholders and the public sector. Indeed, it 
could even potentially worsen this problem as shareholders and depositors are one and 
the same under mutuality and so must rank equally in the event of a payout from the 
public purse. Moreover, profit margins in the building society sector are currently 
under some pressure as a result of low Bank rate and increased competition for prime 
residential loans. Some building societies have also hit bad loan problems. 
Although painful in the short-term, these margin pressures ought in time to abate. So 
in the heartland of mutuality, I am happy to say that reports of the death of the 
building society sector are greatly exaggerated. Indeed, mutuality may do a better job 
of aligning stakeholder incentives than some alternative forms of corporate 
governance. It is a depressing but telling fact that, of the demutualised former UK 
building societies, none is today in independent ownership. 
Thrift, mutuality and relationship-building have long underpinned banking in 
Yorkshire. These principles went missing in the run-up to the present crisis. The 
costs of that vanishing act are now all too apparent. In rebuilding the financial 
system, to create one which is both stable and better able to meet the needs of the real 
economy, these principles need to be rediscovered. They offer a tried and tested – 
indeed, trusted – roadmap for the period ahead.

On 16 November 2002, the first official case of Severe Acute Respiratory Syndrome 
(SARS) was recorded in Guangdong Province, China. Panic ensued. Uncertainty 
about its causes and contagious consequences brought many neighbouring economies 
across Asia to a standstill. Hotel occupancy rates in Hong Kong fell from over 80% 
to less than 15%, while among Beijing’s 5-star hotels occupancy rates fell below 2%. 
Media and modern communications fed this frenzy and transmitted it across borders. 
In North America, parents kept their children from school in Toronto, longshoreman 
refused to unload a ship in Tacoma due to concerns about its crew and there was a 
boycott of large numbers of Chinese restaurants across the United States. Dr David 
Baltimore, Nobel prize winner in medicine, commented: “People clearly have reacted 
to it with a level of fear that is incommensurate with the size of the problem”. 
The macroeconomic impact of the SARS outbreak will never be known with any 
certainty. But it is estimated to stand at anything up to $100 billion in 2003 prices. 
Across Asia, growth rates were reduced by SARS by between 1 and 4 percentage 
points. Yet in the final reckoning, morbidity and mortality rates were, by 
epidemiological standards, modest. Only around 8000 people were infected and 
fewer than 1000 died. 
On 15 September 2008, Lehman Brothers filed for Chapter 11 bankruptcy in a New 
York courtroom in the United States. Panic ensued. Uncertainty about its causes and 
contagious consequences brought many financial markets and institutions to a 
standstill. The market for Credit Default Swaps (CDS) froze, as Lehman was 
believed to be counterparty to around $5 trillion of CDS contracts. 
Media and modern communications fed this frenzy and transmitted it across markets. 
Banks hoarded liquidity for fear of lending to infected banks, causing gridlock in term 
money markets, spreads on lower-rated companies’ bonds spiked and there was an 
effective boycott of the remaining large US investment banks. Professor Paul 
Krugman, Nobel prize winner in economics, commented: “Letting Lehman fail 
basically brought the entire world capital market down.” 
3
The macroeconomic impact of Lehman Brothers’ failure will never be known with 
any certainty. IMF forecasts of global growth for 2009 have been revised down by 
over 5 percentage points since Lehman’s failure. Yet in the final reckoning, the direct 
losses from Lehman’s failure seem likely to be relatively modest. Net payouts on 
Lehman’s CDS contracts amounted to only around $5 billion. 
These similarities are striking. An external event strikes. Fear grips the system 
which, in consequence, seizes. The resulting collateral damage is wide and deep. Yet 
the triggering event is, with hindsight, found to have been rather modest. The flap of 
a butterfly’s wing in New York or Guangdong generates a hurricane for the world 
economy. The dynamics appear chaotic, mathematically and metaphorically. 
These similarities are no coincidence. Both events were manifestations of the 
behaviour under stress of a complex, adaptive network. Complex because these 
networks were a cat’s-cradle of interconnections, financial and non-financial. 
Adaptive because behaviour in these networks was driven by interactions between 
optimising, but confused, agents. Seizures in the electricity grid, degradation of ecosystems, the spread of epidemics and the disintegration of the financial system – each 
is essentially a different branch of the same network family tree. 
This paper considers the financial system as a complex adaptive system. It applies 
some of the lessons from other network disciplines – such as ecology, epidemiology, 
biology and engineering – to the financial sphere. Peering through the network lens, 
it provides a rather different account of the structural vulnerabilities that built-up in 
the financial system over the past decade and suggests ways of improving its 
robustness in the period ahead. 
Part 1 provides the diagnosis. Using network theory and evidence, it explains the 
emergence of two characteristics of the financial network over the past decade – 
complexity and homogeneity. Together, these resulted in a financial network: 
• Which was at the same time both robust and fragile – a property exhibited by 
other complex adaptive networks, such as tropical rainforests; 
4
• Whose feedback effects under stress (hoarding of liabilities and fire-sales of 
assets) added to these fragilities – as has been found to be the case in the spread of 
certain diseases; 
• Whose dimensionality and hence complexity amplified materially Knightian 
uncertainties in the pricing of assets – causing seizures in certain financial 
markets; 
• Where financial innovation, in the form of structured products, increased further 
network dimensionality, complexity and uncertainty; and 
• Whose diversity was gradually eroded by institutions’ business and risk 
management strategies, making the whole system less resistant to disturbance – 
mirroring the fortunes of marine eco-systems whose diversity has been steadily 
eroded and whose susceptibility to collapse has thereby increased. 
This evolution in the topology of the network meant that sharp discontinuities in the 
financial system were an accident waiting to happen. The present crisis is the 
materialisation of that accident. 
Given that diagnosis, Part 2 of the paper provides some tentative policy prescriptions. 
The experience of other network disciplines suggests a rather different approach to 
managing the financial network than has been the case in the past, if future systemic 
dislocations are to be averted. Three areas in particular are discussed: 
• Data and Communications: to allow a better understanding of network dynamics 
following a shock and thereby inform public communications. For example, 
learning from epidemiological experience in dealing with SARs, or from 
macroeconomic experience after the Great Depression, putting in place a system 
to map the global financial network and communicate to the public about its 
dynamics; 
• Regulation: to ensure appropriate control of the damaging network consequences 
of the failure of large, interconnected institutions. For example learning from 
experience in epidemiology by seeking actively to vaccinate the “super-spreaders” 
to avert financial contagion; and 
• Restructuring: to ensure the financial network is structured so as to reduce the 
chances of future systemic collapse. For example, learning from experience with 
engineering networks through more widespread implementation of central 
counterparties and intra-system netting arrangements, which reduce the financial 
network’s dimensionality and complexity. 
5
Networks and finance are not complete strangers. There has been growing interest 
among network theorists in applying their techniques to financial phenomena over the 
past few years. For example, network techniques have already been applied 
extensively to the dynamics of payment systems and inter-bank networks.1
 But the 
financial crisis of the past two years provides both a greater body of evidence, and a 
stronger incentive, to apply the lessons from other network disciplines to the pressing 
problems facing financial policymakers today. 
 
1
 Federal Reserve Bank of New York (2007); May, Levin and Sugihara (2008); Allen and Gale 
(2000). 
6
Part 1: Topology of the Financial Network 
In many important respects, the current financial crisis is cut from familiar cloth. Its 
genesis was the over-extension of credit, over-inflation of asset prices and overexuberance of participants. From the South Sea bubble to the sub-prime crisis, this 
roll-call of excesses is familiar. Gerald Corrigan, ex-President of the New York Fed, 
said ahead of the crisis: 
“In recent years the pace of change and innovation in financial markets and 
institutions here and around the world has increased enormously as have the speed, 
volume and value of financial transactions. The period has also seen a greatly 
heightened degree of aggressive competition in the financial sector. All of this is 
taking place in the context of a legal and a regulatory framework which is 
increasingly outdated and ill-equipped to meet the challenges of the day. This has led 
to…concern that the fragility of the system has increased, in part because the degree 
of operational, liquidity and credit interdependency has risen sharply”.2
 
Corrigan was speaking in January 1987. The crisis foretold was the October 1987 
stock market crash. Plus ça change. 
Yet in some more fundamental respects this time’s crisis feels different – larger 
probably, more discontinuous, complex and interconnected certainly. There are 
already numerous accounts of why that might be. Here, I argue that these knife-edge 
dynamics can essentially be explained by two structural features of the financial 
network. These have developed over many years but at particular pace over the past 
decade. They are complexity on the one hand, and homogeneity on the other. 
In essence, the financial network has over time become progressively more complex 
and less diverse. Why? And what have been the consequences? 
In the 1987 film Wall Street, the financial sector mantra was “greed is good”. The 
stock market crash of the same year put paid to that doctrine, at least temporarily. By 
the early part of this century, both the circumstances and the individuals had changed. 
So too had the mantra. It had become the rather gentler “diversification is desirable”. 
Risk-taking became less Gordon Gekko and more Merton Miller. 
 
2
 Corrigan (1987). 
7
Diversification came care of two complementary business strategies. The first was 
“originate and distribute”. Risk became a commodity. As such it could be bundled, 
sliced, diced and then re-bundled for onward sale. Credit became, in the jargon, 
structured. Securitisation was one vehicle for achieving this. Derivatives, such as 
CDS, were another. As these marketable instruments passed between participants, the 
network chain lengthened. 
In principle, these instruments delivered a Pareto-improving reallocation of risk. Risk 
would flow to those best able to bear it. They had deep pockets which they sought to 
line with higher yield. For the system as a whole, this sounded like the land of milk 
and honey. For a risk shared was a risk halved – perhaps more than halved, given the 
magic of diversification. The network chain, meanwhile, just kept on growing. 
The second strategy was diversification of business lines. Firms migrated activity to 
where returns looked largest. As each new day dawned – leveraged loans yesterday, 
CDOs today, proprietary trading tomorrow – the whole sector was drawn to the new 
source of sunlight. Through competitive forces, finance engaged in a frantic game of 
follow-the-leader, played for real money. 
From an individual firm perspective, these strategies indeed looked like sensible 
attempts to purge risk through diversification: more eggs were being placed in the 
basket. Viewed across the system as a whole, however, it is clear now that these 
strategies generated the opposite result: the greater the number of eggs, the greater 
the fragility of the basket - and the greater the probability of bad eggs. 
Securitisation increased the dimensionality, and thus complexity, of the financial 
network. Nodes grew in size and interconnections between them multiplied. The 
financial cat’s-cradle became dense and opaque. As a result, the precise source and 
location of underlying claims became anyone’s guess. Follow-the-leader became 
blind-man’s buff. In short, diversification strategies by individual firms generated 
heightened uncertainty across the system as a whole. 
8
Meanwhile, a strategy of changing the way they had looked in the past led to many 
firms looking the same as each other in the present. Banks’ balance sheets, like 
Tolstoy’s happy families, grew all alike. So too did their risk management strategies. 
Financial firms looked alike and responded alike. In short, diversification strategies 
by individual firms generated a lack of diversity across the system as a whole. 
So what emerged during this century was a financial system exhibiting both greater 
complexity and less diversity. Up until 2007, many participants in financial markets 
would have viewed that network evolution as the inevitable by-product of technical 
progress in finance. Until then, complexity plus homogeneity equalled stability. 
But in just about every non-financial discipline - from ecologists to engineers, from 
geneticists to geologists - this evolution would have set alarm bells ringing. Based on 
their experience, complexity plus homogeneity did not spell stability; it spelt 
fragility. In understanding why, it is useful to explore some of the wider lessons from 
those disciplines, taking in turn the effects of complexity and diversity on stability. 
Complexity and Stability 
Tropical rainforests are a complex adaptive system. In the immediate post-war 
period, these eco-systems were often used as a case-study when demonstrating why 
complex systems tended to exhibit greater stability.3
 In Elton’s (1958) words, this 
was because there are “always enough enemies and parasites available to turn on any 
species that starts being unusually numerous”. Complexity strengthened selfregulatory forces in systems, so improving robustness. This was the prevailing 
ecological wisdom up until the early 1970s. 
That conventional wisdom has since been turned on its head. From the 1970s 
onwards, orthodoxy was altered by a combination of enriched mathematical models 
and practical experience.4
 Counter-examples emerged, with some simple eco-systems 
– savannas and grasslands – found to exhibit high robustness and some complex ecosystems proving vulnerable to attack. Perhaps tellingly, large-scale clearance of 
 
3
 For example, Voute (1946) and Elton (1958). 
4
 For example, May (1974). 
9
tropical rainforests highlighted their inherent fragility. Not for nothing did rainforests 
become known as a “non-renewable” resource from the early 1970s. 
Finance appears to be following in ecologists’ footsteps, albeit with a generational 
lag. Until recently, mathematical models of finance pointed to the stabilising effects 
of financial network completeness.5
 Connectivity meant risk dispersion. Real-world 
experience appeared to confirm that logic. Between 1997 and 2007, buffeted by oil 
prices shocks, wars and dotcom mania, the financial system stood tall; it appeared 
self-regulating and self-repairing. Echoes of 1950s ecology were loud and long. 
The past 18 months have revealed a system which has shown itself to be neither selfregulating nor self-repairing. Like the rainforests, when faced with a big shock, the 
financial system has at times risked becoming non-renewable. Many of the reasons 
for this have a parallel in other disciplines. In particular, in making sense of recent 
financial network dynamics, four mechanisms appear to have been important: 
connectivity; feedback; uncertainty; and innovation. 
(a) Connectivity and Stability 
Over the past 30 years, a great deal has been established about the links between 
network connectivity and robustness. These lessons span a range of disciplines 
including physics, biology, engineering and epidemiology. There are perhaps three 
key robustness results from this literature which are relevant to the financial system. 
Perhaps the key one concerns the “robust-yet-fragile” property of connected 
networks.6
 The intuition behind this result is beguilingly simple, but its implications 
profound. In a nutshell, interconnected networks exhibit a knife-edge, or tipping 
point, property. Within a certain range, connections serve as a shock-absorber. The 
system acts as a mutual insurance device with disturbances dispersed and dissipated. 
Connectivity engenders robustness. Risk-sharing – diversification – prevails. 
 
5
 For example, Allen and Gale (op.cit). 
6
 For example, May and Anderson (1991), Albert et al (2004), Kinney et al (2005), Watts (2002). 
10
But beyond a certain range, the system can flip the wrong side of the knife-edge. 
Interconnections serve as shock-amplifiers, not dampeners, as losses cascade. The 
system acts not as a mutual insurance device but as a mutual incendiary device. 
Risk-spreading – fragility - prevails. The extent of the systemic dislocation is often 
disproportionate to the size of the initial shock. Even a modest piece of news might 
be sufficient to take the system beyond its tipping point. This same basic logic has 
latterly been applied to financial systems, using mathematical models and simulated 
data.7
 
These knife-edge dynamics match closely the behaviour of the financial system in the 
recent past. A lengthy period of seeming robustness (the Golden Decade from 1997 
to 2007) was punctuated by an acute period of financial fragility (the period since). 
The shock causing this tipping point to be reached – the sub-prime crisis – was by 
global financial standards rather modest. The robust-yet-fragile property of networks 
helps make sense of these non-linear financial dynamics. Though they looked and felt 
like chaos, these dynamics were in fact manifestations of a new network order. 
The second key robustness result concerns the “long-tailed distribution” of connected 
networks. The degree of a node measures the number of links to other nodes. So the 
degree distribution could be thought of as a histogram of the number of links for each 
node. For a network whose links are randomly configured, this degree distribution 
would be symmetric and bell-shaped; it would have a fat middle and thin tails. 
But many real-world networks do not exhibit these properties, including the internet, 
biological food webs and epidemiology networks.8
 Instead these networks have been 
found to have a thin middle and long, fat tails. There is a larger than expected number 
of nodes with both a smaller and a larger number of links than average. Some 
financial networks, such as payment systems, have also been found to exhibit long 
tails.9
 
7
 For example, at the Bank of England by Nier et al (2008) and Gai and Kapadia (2008); and by 
Battiston et al (2009) and Gallegati et al (2008). 
8
 May (2006). 
9
 Pröpper et al (2008). 
11
Long tails have been shown to have important implications for network robustness. 
In particular, long-tailed distributions have been shown to be more robust to random 
disturbances, but more susceptible to targeted attacks.10 Why? Because a targeted 
attack on a hub risks bringing the heart of the system to a standstill, whereas random 
attacks are most likely to fall on the periphery. 
This result carries important policy implications. Long periods of apparent 
robustness, where peripheral nodes are subject to random shocks, should offer little 
comfort or assurance of network health. It is only when the hub – a large or 
connected financial institution - is subject to stress that network dynamics will be 
properly unearthed. When large financial institutions came under stress during this 
crisis, these adverse system-wide network dynamics revealed themselves. 
The third result is the well-known “small world” property of connected networks.11 
The origin of this was a chain letter experiment by Stanley Milgram in 1967. This 
showed that the average path length (number of links) between any two individuals 
was around six – hence “six degrees of separation”. Although networks tend to 
exhibit local clustering or neighbourhoods, certain key nodes can introduce short-cuts 
connecting otherwise detached local communities. 
This small world property has again been found across a range of physical networks, 
including the World Wide Web and forest fires.12 Its implications for network 
robustness are subtle. In general, however, it will tend to increase the likelihood of 
local disturbances having global effects – so-called “long hops”. That could occur 
between different institutions or between different nation states. Either way, a small 
world is more likely to turn a local problem into a global one. 
So what evidence do we have on these three characteristics in real financial networks? 
Charts 1-3 look at the evolution in the international financial network. In particular, 
they look at cross-border stocks of external assets and liabilities in 18 countries at 
 
10 May and Anderson (1991), Porterie et al (2008) 
11 Watts and Strogatz (1998). 
12 On the former see Albert et al (2000); on the latter see Porterie et al (2008). 
12
three dates: 1985, 1995 and 2005. These data can be used to gauge the scale and 
evolution of interconnectivity within the global financial network. 
In Charts 1-3, the nodes are scaled in proportion to total external financial stocks, 
while the thickness of the links between nodes is proportional to bilateral external 
financial stocks relative to GDP.13 Table 1, meanwhile, provides some summary 
statistics for the international financial network, in particular measures of the skew 
and fat-tailedness in the degree distribution and its average path length. 
Three key points emerge. First, it is clear that the scale and interconnectivity of the 
international financial network has increased significantly over the past two decades. 
Nodes have ballooned, increasing roughly 14-fold. And links have become both fatter 
and more frequent, increasing roughly 6-fold. The network has become markedly 
more dense and complex. And what is true between countries is also likely to have 
been true between institutions within countries. 
Second, the international financial network exhibits a long-tail. Measures of skew 
and kurtosis suggest significant asymmetry in the network’s degree distribution. 
Global finance appears to comprise a relatively small number of financial hubs with 
multiple spokes. 
Third, the average path length of the international financial network has also shrunk 
over the past twenty years. Between the largest nation states, there are fewer than 1.4 
degrees of separation. Were the network extended beyond the 18 countries in the 
sample, the evolution of this “small world” property would be clearer still. 
So based on evidence from a sampled international financial network, the past twenty 
years have resulted in a financial system with high and rising degrees of 
interconnection, a long-tailed degree distribution and small world properties. That is 
an unholy trinity. From a stability perspective, it translates into a robust-yet-fragile 
system, susceptible to a loss of confidence in the key financial hubs and with rapid 
 
13 Specifically, nodes are scaled by (Total External Assets + Total External Liabilities) for each node, 
and links between nodes i and j by (Total External Assetsij + Total External Liabilitiesij )/(GDPi + 
GDPj). The data are developed and analysed in Kubulec and Sa (2008). 
13
international transmission of disturbances. That is not the worst description of 
financial events over the past decade – and in particular over the past 18 months. 
(b) Feedback and Stability 
In epidemiology, the impact of a disease depends crucially on such structural 
parameters as the mortality rate once infected and the transmission rate across 
agents.14 The first is largely fixed and biological. But the second is likely to be 
variable and sociological. In other words, agents’ responses to infection, or indeed 
the fear of infection, are often crucial in determining its rate of transmission. 
In practice, these behavioural responses typically take one of two forms: “hide” or 
“flight”. For example, the response to the SARS epidemic in the 21st century was a 
“hide” response, with people self-quarantining by staying at home and with flight, in 
this case literally, prohibited. But the response to yellow fever in North America in 
the 19th century was “flight”, with half the population of Memphis fleeing in 1878.15
Either response is rational from an individual perspective. Both responses have the 
aim of removing that individual from circulation with other, potentially infectious, 
agents. But the implications of these responses for infection rates across the system 
are potentially very different. Hide responses tend to contain infection locally, thus 
protecting the system globally. This was the SARS experience. Flight, by contrast, 
tends to propagate infection globally. This was the yellow fever experience, as 
incidence of the disease followed the railroad line out of Memphis.16
During this financial crisis, faced with fears about infection, similar sets of 
behavioural responses by financial institutions have occurred. Only the names are 
different. The “hiding” has taken the form of hoarding, typically of liquidity. And 
the “flight” from infected cities has taken the form of flight from infected assets, as 
 
14 May (2006), Newman (2002). 
15 Epstein et al (2008) provide a range of examples. 
16 Wheelis (2006) provides an excellent example of the role of human (in this particular case literal) 
flight in transmitting the Plague to Europe in the 14th Century. Transmission of the Plague was 
reputedly the result of Genoese traders fleeing the Crimean city of Caffa after the Mongol army had 
catapulted infected corpses over the city walls. 
14
institutions have sold toxic assets. Unlike in an epidemiological context, however, 
both behavioural responses have aggravated stresses in the financial system. How so? 
Banks entered the crisis with a large portfolio of risky assets. As risk materialised, 
banks rationally sought to protect themselves from infection from other banks by 
hoarding liquidity rather than on-lending it. The result has been enduring stress in 
money markets. Banks’ mutual interdependence in inter-bank networks meant that 
individually-rational actions generated a collectively worse funding position for all. 
That, in turn, contributed to the second behavioural response. Unable easily to fund 
their asset portfolio, some financial firms instead opted for flight through sales of 
assets. These acted like the railroad out of Memphis, placing downward pressure on 
asset prices and thereby spreading the infection to other institutions. Others’ 
immunity to infection was simultaneously being lowered by widespread marking of 
assets to market. In escaping the plague, asset flight served to propagate it. 
These behavioural dynamics – panic hoarding of liabilities, distress sales of assets – 
have been defining features of this crisis. Placing these responses in a network 
framework clarifies the individual rationalities, but collective externalities, that drove 
these actions. These rational responses by banks to fear of infection added to the 
fragility of an already robust-yet-fragile financial network. 
(c) Uncertainty and Stability 
A related, but separate, behavioural response to fear of infection is felt in the pricing 
of financial instruments. Networks generate chains of claims. At times of stress, 
these chains can amplify uncertainties about true counterparty exposures. Who is 
really at the end of the chain – Warren Buffett or Bernard Madoff? Through their 
impact on counterparty uncertainty, networks have important consequences for 
dynamics and pricing in financial markets. 
To illustrate, consider the case of pricing in the CDS market – an inherently complex, 
high dimension market. In particular, consider Bank A seeking insurance from Bank 
B against the failure of Entity C. Bank A faces counterparty risk on Bank B. If that 
15
were the end of the story, network uncertainty would not much matter. Bank A could 
monitor Bank B’s creditworthiness, if necessary directly, and price the insurance 
accordingly. 
But what if Bank B itself has n counterparties? And what if each of these n
counterparties itself has n counterparties? Knowing your ultimate counterparty’s risk 
then becomes like solving a high-dimension Sudoku puzzle. Links in the chain, like 
cells in the puzzle, are unknown - and determining your true risk position is thereby 
problematic. 
For Bank A, not knowing the links in the chain means that judging the default 
prospects of Bank B becomes a lottery. Indeed, in some ways it is worse than a 
lottery, whose odds are at least known. In this example, Bank A faces uncertainty in 
the Knightian sense, as distinct from risk, about the true network structure. 
Counterparty risk is not just unknown; it is almost unknowable. And the higher the 
dimensionality of the network, the greater that uncertainty. 
It is possible to formalise this intuition with some simple numerical examples.17 
Consider two states of the world, pre-crisis and crisis. And consider the impact of 
network complexity on CDS pricing. Once we introduce Knightian uncertainty, asset 
prices are no longer determinate; they are defined by a range rather than a point. So 
the range of equilibrium CDS spreads can be taken as a metric of the uncertainty, and 
hence distortion, arising from different network structures. 
Chart 4 plots a pre-crisis world where it is assumed that counterparty default 
probabilities, and the uncertainty around them, are low. Subject to those assumptions, 
it illustrates how the range of CDS spreads is affected by Bank B’s number of 
counterparties. Larger numbers of counterparties are marginally beneficial. There is 
a “law of large numbers” benefit. Broadly-speaking, however, network 
dimensionality has no material bearing on CDS pricing. 
 
17 The following is based on work in progress at the Bank on asset pricing under network Knightian 
uncertainty, by Sebastiano Daros and Kemal Ercevik. 
16
Chart 5 simulates a crisis world in which the default probability of Bank B has risen 
and so too the uncertainty around that probability. The difference is striking. Pricing 
uncertainty now increases with the dimensionality of the web. Extra counterparties 
add to, rather than subtract from, pricing distortions. There is a “law of large 
numbers” cost. That uncertainty cost, or Knightian distortion, is roughly proportional 
the dimension of the network. 
It is difficult not to draw comparisons with Lehman’s experience. Lehman had large 
CDS counterparty exposures relative to its balance sheet and hundreds of 
counterparties. AIG was similarly situated. It is little wonder participants took fright 
as both institutions came under stress, fearful not so much of direct counterparty risk, 
but of indirect counterparty risks emanating from elsewhere in the network. The 
network chain was so complex that spotting the weakest link became impossible. 
This added yet a further layer of fragility to the financial system. 
(d) Innovation and Stability 
A fourth dimension to complexity in network chains derives from the effects of 
financial innovation. Over the past decade, this often took a particular form – 
structured credit - with risk decomposed and then reconstituted like the meat in an 
increasingly exotic sausage. The result was a complex interlocking set of claims. 
With each restructuring of ingredients, the web branched and the dimensionality of 
the network multiplied. 
Chart 6 shows some of the interlocking networks of structured products that emerged. 
I will not attempt to describe this chart; it would take too long and, even if I had the 
time, I doubt I would have the ability. These were the self-same constraints – time, 
complexity - which faced investors in these products. Due diligence was the casualty. 
End-investors in these instruments were no more likely to know the name of the 
companies in their portfolios than the name of the cow or pig in their exotic hot dog. 
To illustrate, consider an investor conducting due diligence on a set of financial 
claims: RMBS, ABS CDOs and CDO2
. How many pages of documentation would a 
diligent investor need to read to understand these products? Table 2 provides the 
17
answer. For simpler products, this is just about feasible – for example, around 200 
pages, on average, for an RMBS investor. But an investor in a CDO2
 would need to 
read in excess of 1 billion pages to understand fully the ingredients. 
With a PhD in mathematics under one arm and a Diploma in speed-reading under the 
other, this task would have tried the patience of even the most diligent investor. With 
no time to read the small-print, the instruments were instead devoured whole. Food 
poisoning and a lengthy loss of appetite have been the predictable consequences. 
Though it had aimed to dampen institutional risk, innovation in financial instruments 
served to amplify further network fragility. 
 
Diversity and Stability 
A final dimension to network robustness concerns the effects of diversity. The oceans 
provide a rich and lengthy test-bed of the links between diversity and robustness. 
Over the past millennium, studies of coastal eco-systems reveal some dramatic 
patterns.18 For around 800 years, between the years 1000-1800AD, fish stocks and 
species numbers were seemingly stable and robust. Since then, almost 40% of fish 
species across the world’s major coastal eco-systems have “collapsed”, defined here 
as a fall in population of greater than 90%. That is systemic by any metric. 
There appear to be many environmental reasons for this collapse, some natural, others 
man-made. But the distribution of this collapse across eco-systems is revealing. For 
species-rich – that is, diverse – eco-systems the rate of collapse has been as low as 
10%; for species-poor eco-systems, as high as 60%. Diverse coastal eco-systems 
have proved to be markedly more robust, measured over century spans. 
Results for large marine eco-systems suggest a similar picture. Over the period 1950-
2003, the incidence of collapsed fisheries declines exponentially with speciesdiversity.19 Diversity also appears to increase the resilience of fisheries – that is, their 
 
18 The results here are based on Worm et al (2006). 
19 Worm et al (op.cit.). 
18
capacity to recover – in the event of collapse. These results reappear throughout 
marine eco-systems, “in coral reefs in Jamaica and on rocky shores in Panama”.20 
And they do not appear to be unique to marine eco-systems. For example, similar 
effects of diversity have been found in studies of the resilience of crops to pathogen 
outbreaks; in the robustness of savannas and grassland to drought; and in morbidity 
and mortality rates among humans facing disease and infection.21 Diversity of the 
gene pool, it seems, improves durability. 
The financial system has mirrored the fortunes of the fisheries, for many of the same 
reasons. Since the start of 2007, 23 of the largest European and US banks have seen 
their market capitalisation fall by 90% or more – the fisheries equivalent of collapse. 
But what took marine eco-systems two hundred years to achieve has been delivered 
by financial engineers in two. In explaining the collapse in fish and finance, lack of 
diversity seems to be a common denominator. 
Within the financial sector, diversity appears to have been reduced for two separate, 
but related, reasons: the pursuit of return; and the management of risk. The pursuit 
of yield resulted in a return on equity race among all types of financial firm. As they 
collectively migrated to high-yield activities, business strategies came to be replicated 
across the financial sector. Imitation became the sincerest form of flattery. 
So savings cooperatives transformed themselves into private commercial banks. 
Commercial banks ventured into investment banking. Investment banks developed 
in-house hedge funds through large proprietary trading desks. Funds of hedge funds 
competed with traditional investment funds. And investment funds – pension, money 
market mutual, insurance - imported the risk the others were shedding. 
Cumulative returns earned by, on the face of it, very different financial models 
illustrate this story (Chart 7). Looking across global banks, large complex financial 
institutions (LCFIs), insurance companies and hedge funds, cumulative returns have 
exhibited a remarkably similar pattern, both in the run-up to crisis and in the 
 
20 Levin and Lubchenco (2008). 
21 For example, Tilman (1999) and Clay (2004). 
19
subsequent run-down. Rolling averages of pairwise correlations across sectors 
averaged in excess of 0.9 throughout the period 2004-2007. At the height of the 
credit boom, financial imitation appears to have turned into near-cloning. Flattery 
gave way to fat-cattery. 
What was true across financial sectors was also true within them. For example, hedge 
fund strategies rejoice in such oblique names as “convertible arbitrage” and 
“dedicated short bias”. The average pairwise correlation between these different 
funds’ strategies was roughly zero at the turn of the century. By 2008, it had risen to 
around 0.35. Far from daring to be different, hedge funds seem increasingly to have 
hunted as a pack. 
Management of the risks resulting from these strategies amplified this homogeneity. 
Basel II provided a prescriptive rule-book ensuring a level playing field. Ratings 
were hard-wired into regulation. Risk models blossomed, with Value-at-Risk (VaR) 
and stress-testing providing seductively precise outputs. Like blossom, these models 
looked and acted alike - and may yet prove similarly ephemeral. The level playing 
field resulted in everyone playing the same game at the same time, often with the 
same ball. 
Through these channels, financial sector balance sheets became homogenised. 
Finance became a monoculture. In consequence, the financial system became, like 
plants, animals and oceans before it, less disease-resistant. When environmental 
factors changed for the worse, the homogeneity of the financial eco-system increased 
materially its probability of collapse. 
So where does this leave us? With a financial system exhibiting, for individually 
quite rational reasons, increasing complexity and homogeneity. A network which, in 
consequence, was robust-yet-fragile. A network predisposed to tipping points and 
discontinuities, even for small shocks. A network which, like Tolstoy’s unhappy 
families, could be unhappy in quite different ways. A network mostly self-repairing, 
but occasionally self-destructing. A network which, like the little girl with the curl, 
when the going was good was very, very good – but when it turned bad was horrid. 
20
Part 2: Improving Network Stability 
This is a gloomy prognosis: a financial system teetering between triumph and 
disaster. Unlike Kipling, policymakers in practice are unlikely to treat those two 
imposters just the same. Recent events have rather illustrated that. Public 
interventions in the financial system during this crisis – through liquidity injections, 
capital injections or public sector guarantees – already total in excess of £5 trillion.22 
So what could be done to protect the financial network from future such dynamics? 
And are there lessons from other network disciplines which might help inform these 
efforts? Let me highlight three areas where improvements in the robustness of the 
financial network seem feasible: mapping; regulating; and restructuring. 
(a) Mapping the Network 
The SARS episode may be remembered by historians as an overblown economic 
reaction to a small health risk – that was Nobel Laureate Dr David Baltimore’s 
prognosis. But there is an alternative reading of the runes, one which offers some 
lessons, and not a little hope, for financial policymakers. 
In 2000, the World Health Organisation (WHO) established the Global Outbreak 
Alert and Response Network (GOARN). This brings together over 120 international 
institutions and networks to share resources to better identify and manage outbreaks. 
In the case of SARS, the speed and scale of response was striking. 
On 12 March 2003, less than two weeks after the Hong Kong outbreak, the WHO 
issued a global health alert. On 15 March, a “general travel advisory” was issued. By 
17 March, a network of scientists from 11 laboratories in 9 countries was established 
to devise diagnostic tests, analyse samples and share results in real time. This allowed 
national agencies to promulgate information quickly and widely, with governments in 
 
22 For example, Bank of England Financial Stability Report, October 2008. 
21
Thailand, Malaysia, China, Singapore and Canada each imposing some combination 
of travel bans, quarantining and public health notices.23
These measures appear to have contributed both to the rapid subsidence of SARSrelated fears and uncertainties among the general public and to containing the spread 
of the disease. Since April 2004, there have been no reported cases of SARS. The 
global information infrastructure of GOARN is widely acknowledged as having 
helped nip the SARS crisis in the bud. 
There are important lessons here for the financial system. At present, risk 
measurement in financial systems is atomistic. Risks are evaluated node by node. In 
a network, this approach gives little sense of risks to the nodes, much less to the 
overall system. It risks leaving policymakers navigating in dense fog when assessing 
the dynamics of the financial system following failure. The market repercussions of 
Lehman’s failure were in part the result of such restricted visibility. 
What more might be done to prevent a repeat? Part of the answer lies in improved 
data, part in improved analysis of that data, and part in improved communication of 
the results. On data, in some real-world physical networks, data is collected on 
virtually all nodes and links. For example, in modelling the US electricity grid, data 
are collected on all major power stations (nodes) and power lines (links).24 As these 
total 14,000 and 20,000 respectively, this is a large-dimension network. 
Data from physical networks such as the power grid are relatively easy to collect. For 
many other large-dimension networks, sampling techniques are typically required. 
These typically take one of three forms: node sampling; link sampling; and 
“snowball” sampling.25 There are lessons for the financial system from all three. 
To date, sampling of nodes has been the dominant means of assessing risk within the 
financial system, typically for a sub-set of the nodes such as banks. Where non-bank 
financial intermediaries are an important part of the network, sampling of nodes has 
 
23 For example, Smith (2006) and McKercher and Chon (2004). 
24 For example, Kinney et al (2005). 
25 Lee, Kim and Jeong (2006). 
22
shown itself deficient. For example, little was known about the activities of off 
balance sheet vehicles – SIVs and conduits - ahead of crisis. More fundamentally, 
this approach provides little information on the links between nodes. These are 
central to understanding network dynamics. Imagine assessing the robustness of the 
electricity grid with data on power stations but not on the power lines connecting 
them. 
Sampling of links has historically been little deployed when analysing the financial 
system. Some data exist on the degree of linkage between financial firms – for 
example, from regulatory returns on large exposures. This has been used to construct 
rough approximations of inter-bank networks.26 But these data are typically partial 
and lack timeliness. They are weak foundations for understanding the financial 
network. 
That takes us to snowballing – that is, constructing a picture of the network by 
working outwards from the links to one of the nodes. As a way of understanding the 
financial web, there are attractions to this approach. It is agnostic about which are the 
key nodes and important links. Network boundaries are uncovered by following the 
money, rather than by using institutional labels or national or regulatory boundaries. 
Applied in practice, this approach might have helped identify some of the key nodal 
sources of risk ahead of financial crisis. In early 2007, it is doubtful whether many of 
the world’s largest financial institutions were more than two or three degrees of 
separation from AIG. And in 1998, it is unlikely that many of the world’s largest 
banks were more than one or two degrees of separation from LTCM. Rolling the 
snowball might have identified these financial black holes before they swallowed too 
many planets. 
There have been a number of recent policy proposals in this general area. For 
example, the de Larosiere Report (2009) calls for a European and, ultimately, global 
initiative to create an international register of claims between financial institutions. A 
similar initiative following the LDC debt crisis resulted in the Bank for International 
 
26 Propper et al (2008). 
23
Settlements (BIS) developing international banking statistics. These are now an 
essential source of international financial network data. There is a need for similar 
ambition now in fashioning international flow of funds and balance sheet data. 
Even with these data, policymakers and practitioners need to invest in new means of 
analysis. Node-by-node diagnostics, such as VaR, have shown themselves during this 
crisis to offer a poor guide to institutional robustness. Fortunately, network theorists 
have identified some of the key summary statistics determining system robustness.27 
This includes degree distributions and average path lengths. In time, network 
diagnostics such as these may displace atomised metrics such as VaR in the armoury 
of financial policymakers. 
To these static diagnostics could be added dynamic summary statistics of network 
resilience, such as simulated responses to nodal failure or stress. Stress-testing to date 
has focussed on institutional, idiosyncratic risk. It needs instead to focus on systemwide, systematic risk.28 Advances in computing power mean that technology is no 
longer a constraint. In studies of the electricity grid, simulations of hundreds of 
thousands of observations are common. Finance can piggy-back on these efforts. 
After data and analysis comes, crucially, communication. Network information is a 
classic public good. Not only is it in no-one’s individual interest to collect it; nor is it 
remotely within anyone’s compass. Aggregate data are a job for the authorities. And 
having been collected, these results need then to be disseminated. This is important 
both ex-ante as a means of better pricing and managing risk, and ex-post as a means 
of containing that risk. 
In a world of 24/7 media, public communications during crisis become crucial. That 
was the lesson from SARS – and may yet be the enduring lesson from Lehman. 
From mid-September to mid-October 2008, the financial crisis did not just dominate 
the news; it was the news. Only a hermit could have failed to have their perceptions 
shaped by this tale of woe. As woe became the popular narrative, depressed 
expectations may have become self-fulfilling. 
 
27 Newman (2002). 
28 Haldane (2009). 
24
In their recent book, Animal Spirits, George Akerlof and Robert Shiller emphasise the 
role of popular psychology – “stories” - in shaping people’s perceptions and actions. 
Depression is a psychological state as well as an economic one. Perhaps the best 
explanation we have about events following the Lehman crisis is that these two states 
merged. Adroit communications by the authorities, like counselling, might help headoff future bouts of clinical depression in the financial system. 
 
This is undoubtedly an ambitious agenda. But experience after the Great Depression 
suggests grounds for optimism. That crisis brought about a revolution in thinking 
about macroeconomic theory and macroeconomic policy. In many respects, it marked 
the birth of modern macroeconomic models – in the form of IS/LM analysis – and 
modern macroeconomic policy – in the form of activist monetary and fiscal policy. 
Though less heralded, it also resulted in a revolution in macroeconomic data. Despite 
attempts in the 1920s and 1930s, it was from the 1940s onwards that national 
accounts data emerged for the main developed economies. This was largely a 
response to the evolution in macroeconomic thinking and policy-making following 
the Great Depression. Crisis experience led theory which in turn led data. That is the 
evolutionary path finance now needs to be on. 
(b) Regulating the Network 
The first diagnosed case of Human Immuno-Deficiency Virus (HIV) in the United 
States came in June 1981. The first diagnosed case of HIV in Australia came in 
November 1982. In the early 1980s, rates of HIV and AIDS incidence in the US and 
Australia were roughly similar on a per capita basis. But from the mid-1980s 
onwards, things changed. By 1994, rates of incidence in the US were six times those 
in Australia. By 2003, the per capita prevalence of HIV in the US was ten times that 
in Australia.29 What explains these differences? 
 
29 Bowtell (2005, 2007). 
25
The short answer appears to be government policy. In the US, the policy stance since 
the early 1980s has been largely theological. The preventative response has taken the 
form of moralising about sexual abstinence and monogamy. Since the mid-1990s, 
the US government has invested in the less contentious areas of HIV/AIDS treatment. 
But as recently as 2007, the US administration remained opposed to the provision of 
condoms or needle and syringe programmes to prevent the spread of HIV/AIDS. 
Australian policy since the early 1980s has, by contrast, been grounded in biology 
rather than theology. It has been systematic, with policy evidence-based and 
preventative. Education and prophylactic measures have been widely available. But 
there have been targeted initiatives for high-risk groups – for example, sex workers 
and drug users – through subsidised needle and syringe exchanges and free condoms. 
The results of this programme are clear in the statistics. 
There are perhaps two clear lessons from this experience. First, the importance of 
targeting high-risk, high-infection individuals – the “super-spreaders”. This principle 
has an impeccable epidemiological pedigree.30 For randomly distributed networks, 
targeted treatment has no value. But for networks exhibiting long tails – which is 
most of them, certainly including finance - targeted vaccination programmes offer a 
much more effective means of curtailing epidemics. 
Not for nothing is epidemiology the origin of the 80/20 principle.31 For a number of 
diseases, including SARS and measles, the distribution of infection rates suggest 20% 
of the population is responsible for 80% of the spread. Similar patterns have been 
found in the transmission of HIV/AIDS, foot and mouth and computer viruses on the 
internet. In each of these cases, the right response has been shown to be targeted 
vaccination of the super-spreaders. 
The second lesson concerns the importance of a system-wide approach to the 
management of network problems. The Australian HIV/AIDS programme was 
system-wide, tackling both the causes and consequences of the disease and its spread. 
Fisheries management provides a second revealing case study. Concerns about the 
 
30 May and Anderson (1991). 
31 May and Anderson (op.cit.), May (2005). 
26
collapse of fisheries came to a head during the 1970s and 1980s, leading to the 
imposition of fishing quotas for various species. The effect of quotas was, at best, 
mixed. 
Recently, there has been a growing recognition of what went wrong. In setting 
quotas, no account was taken of interactions between species and the surrounding 
eco-system. During this century, fisheries management has pursued a different 
strategy – Ecosystem-Based Fishery Management (EBFM).32 EBFM takes as its 
starting point the management of the eco-system. It develops system-level standards 
and single-species targets are calibrated to ecosystem-wide objectives. The EBFM 
approach is already being implemented in Alaska, California and the Antarctic. 
Existing regulatory rules for financial institutions have echoes of fisheries 
management in the 1970s. Risk quotas are calibrated and applied node by node, 
species by species. This approach takes no account of individual nodes’ system-wide 
importance – for example, arising from their connectivity to other nodes in the 
network or their scale of operations. 
 
Charts 8 and 9 illustrate the problem. They plot the relationship between global 
banks’ capital ratios and their size, where size is used here as a rough proxy for 
connectivity and scale. Chart 8 shows there is essentially no relationship between 
banks’ systemic importance and their Basel capital ratios. There has been no targeted 
vaccination of the super-spreaders of financial contagion. Chart 9 uses leverage ratios 
rather than risk-weighted Basel capital ratios. It suggests that, if anything, the superspreaders may historically have had lower capital buffers. 
One potential explanation of these findings is that large banks have benefited from the 
diversification benefits – those words again – of Basel II. Another is that financial 
markets have allowed these banks lower capital buffers because of the implicit 
promise of government support. Chart 10 offers support for the latter hypothesis. It 
suggests a positive relationship between bank size and pre-crisis expectations of 
 
32 For example, Pikitch et al (2004). 
27
official sector support.33 Size matters. Historically, the safety net was perceived to be 
fur-lined for those above a certain size. 
This evidence is discouraging from a systemic risk perspective. It suggests incentives 
to generate and propagate risks may have been strongest among those posing greatest 
systemic threat. Basel vaccinated the naturally immune at the expense of the 
contagious: the celibate were inoculated, the promiscuous intoxicated. Latterly, this 
defect has begun to be addressed. For example, the US and Swiss authorities have 
announced plans to introduce tighter regulatory requirements for systemic institutions. 
There is further to go internationally. Work is needed to give systemic regulation 
practical effect. A number of calibration devices have been proposed.34 With richer 
data on network topology, calibrated simulation models could help gauge financial 
institutions’ marginal contribution to systemic risk. This is standard practice in 
management of the electricity grid and eco-systems. Finance needs to catch up. 
(c) Restructuring the Network 
In Herbert Simon’s The Architecture of Complexity, he tells the parable of two 
watchmakers, Hora and Tempus.35 Both produce watches composed of 1000 parts. 
Both watches are, in this sense, equally complex. They are also of equal quality and 
sell at the same price. But Hora’s business prospers, while Tempus’s founders. 
Why? 
The answer lies in the structure of complex systems. Hora’s watches are designed as 
ten sub-assemblies each comprising ten elements, which are combined into ten larger 
sub-assemblies, ten of which then constitute a whole watch. Tempus, by contrast, 
assembles his watches part by part. The result is that, whenever Tempus is 
interrupted – in Simon’s parable by a telephone call ordering more watches – his work 
 
33 Proxied by Fitch ratings agency’s support ratings for institutions. 
34 Including measures of banks’ Conditional VaR or CoVaR (NYU Stern School of Business (2009), 
and Brunnermeier et al (2009)). These are statistical measures of an institution’s VaR conditional on 
other institutions in the network simultaneously facing stress. 
35 Simon (1962). 
28
is lost and he must start again. Hora suffers the same fate much less frequently, due 
to the sub-assembly structure of his watches. 
The differences in the robustness of these equally complex structures are dramatic. If 
the probability of interruption is 0.01, Hora will complete 9 watches for every 10 
attempts. By contrast, Tempus completes 44 watches for every million attempts. The 
probability of horological collapse is lowered from 0.999956 to 0.1. 
The secret of the structure of Hora’s complex watches is that they are “hierarchical”, 
with separate and separable sub-structures. Simon discusses how a number of other 
networks, both social and physical, exhibit this hierarchical structure. This is no 
evolutionary accident. For many networks, hierarchy emerges naturally. It is the 
product of a process of Darwinian selection in which it is only the hierarchical 
structures that survive to maturity. Hora’s business thrives, Tempus’s dies. 
In other networks, hierarchy is the result not of natural evolution but human 
intervention. For example, the optimal distribution of trees has been shown to 
comprise contiguous patches separated by firebreaks.36 The firebreaks created by 
man generate hierarchy in this system. The same man-made firebreaks are present in 
epidemiological networks, such as the imposition of travel bans following the SARS 
outbreak in Asia or the prohibition of animal movement during the foot and mouth 
epidemic in the UK.37 
All of this has relevance to the future structure and design of the financial network. 
What is second nature to the watch-maker needs to become second nature to the 
watchdog. Four topical examples can be used to illustrate the importance of these 
structural issues for financial network design. 
 
First, the past decade has seen an explosion in the dimensionality, and thus 
complexity, of the financial web. Among others things, that has exacerbated the 
system’s robust-yet-fragile characteristics and uncertainty about counterparty pricing 
 
36 Carlson and Doyle (1999). 
37 Kelling et al (2003). 
29
within the network. Both have been much in evidence recently. Yet there are 
structural means of addressing these combined problems at a stroke. 
The stroke is infrastructure. Central counterparties (CCPs) are intended to deal with 
precisely these problems. They interpose themselves between every trade. In this 
way, a high-dimension web is instantly compressed to a sequence of bilateral 
relationships with the central counterparty - a simple hub-and-spokes. The lengthy 
network chain is condensed to a single link. Provided that link is secure – the hub’s 
resilience is beyond question – counterparty uncertainty is effectively eliminated. 
Table 3 simulates the benefits of introducing a CCP in reducing counterparty 
uncertainty. As in the earlier example, Knightian uncertainty is measured by the size 
of the range of CDS spreads. In all cases, moving to a central counterparty (n = 1) 
results in a material reduction in uncertainty around spreads. These benefits are 
predicated on the CCP “super-spreader” itself being impregnable to attack. 
There have been various initiatives over the recent past to introduce central 
counterparties for the clearing of certain financial instruments, including CDS 
products over the past 18 months.38 This is welcome. But the debate needs not to end 
there. A much broad range of over-the-counter financial instruments, both cash and 
derivatives, could potentially benefit from the introduction of a central counterparty. 
Central counterparties are of course not new. Clearing houses date from the early 19th
century. But, latterly, the question often most asked of central counterparties has been 
“Why”? Experience during the crisis means we now know why. From a network 
resilience perspective, it is important that in future the central counterparty question 
becomes not “Why?” but “Why not”? 
Second, financial innovation has created strings of gross claims between financial 
entities which far exceed their capital bases. Lehman had gross CDS exposures 
around eight times its balance sheet. These gross intra-system claims have grown 
rapidly over the past decade, fuelled by off balance sheet activity. CDS growth has 
 
38 President’s Working Group (2008). 
30
outpaced Moore’s Law – the more than doubling of microchip capacity every two or 
so years. In the CDS market, what were 1000-piece watches in 2000 would by 2007 
have become more than 64,000 piece. 
Intra-system claims on this scale increase network fragility. When one node 
collapses, the ripple across the system risks developing into a tsunami – as Lehman’s 
experience attests. Herbert Simon recognised just this problem. Hierarchical 
networks are, in his words, decomposable with intra-system interactions constrained. 
The financial system has recently evolved in the opposite direction, with intra-system 
interactions growing and decomposability of the system thereby reduced. 
Policy initiatives may be able to help. For example, infrastructure could be developed 
to “net off” gross claims within the financial system. Attempts have already been 
made to do this in the CDS market, by tearing-up redundant claims among 
participants. This has reduced outstanding CDS claims by as much as 30%. The 
same netting principle could potentially be applied to a wider range of contracts and 
counterparties, to improve the decomposability and hence robustness of the system.39 
Third, financial innovation in the form of structured credit also had the consequence 
of creating a network structure which was non-hierarchical. Financial engineers 
created products in which elements of a loan portfolio were reassigned to a higherorder sub-assembly. In this way, an automatic dependence was created among almost 
every sub-structure. By contract design, the overall financial system became 
impossible to decompose into separable sub-structures. 
Such a structure is in fact worse even than Tempus’s complex production line. 
Structured credit was equivalent to taking one part randomly from each of 1000 
watches and reassembling the pieces. No watchmaker in their right mind would 
expect the resulting timepiece to keep time for too long. Such was the CDO story. 
However sensible structuring of credit may have seemed for individual firms, it is 
difficult to conceive of a network which could have been less structurally robust. 
 
39 For example, as proposed in King (2008). 
31
Darwinian evolution is currently in the process of naturally deselecting CDOs. But 
there is a strong public policy case for the authorities intervening more aggressively 
when next financial innovation spawns species with undesirable physiological 
features. 
Finally, the business strategies of financial firms have over the past decade created a 
network structure which is much less easily decomposable. Under the old financial 
order, mutuals were a sub-structure, as were commercial banks, investment banks and 
investment funds. In some cases that was by choice. In other cases it was the result 
of regulatory design: for the larger part of the past century, the Glass-Steagall Act in 
the US prohibited inter-breeding between commercial and investment banking. 
Deregulation swept away banking segregation and, with it, decomposability of the 
financial network. The upshot was a predictable lack of network robustness. That is 
one reason why Glass-Steagall is now back on the international policy agenda. It may 
be the wrong or too narrow an answer. But it asks the right question: can network 
structure be altered to improve network robustness? Answering that question is a 
mighty task for the current generation of policymakers. Using network resilience as a 
metric for success would help ensure it was a productive one. 
Conclusion 
Through history, there are many examples of human flight on an enormous scale to 
avoid the effects of pestilence and plague. From yellow fever and cholera in the 19th
century to polio and influenza in the 20th. In these cases, human flight fed contagion 
and contagion fed human catastrophe. The 21st century offered a different model. 
During the SARS epidemic, human flight was prohibited and contagion contained. 
In the present financial crisis the flight is of capital, not humans. Yet the scale and 
contagious consequences may be no less damaging. This financial epidemic may 
endure in the memories long after SARS has been forgotten. But in halting the spread 
of future financial epidemics, it is important that the lessons from SARS and from 
other non-financial networks are not forgotten. 

By any historical standard, the financial crisis of the past 18 months has been 
extraordinary. Some have suggested it is the worst since the early 1970s; others, the 
worst since the Great Depression; others still, the worst in human history. Time will 
tell. 
Risk managers are of course known for their pessimistic streak. Back in August 2007, 
the Chief Financial Officer of Goldman Sachs, David Viniar, commented to the 
Financial Times: 
“We are seeing things that were 25-standard deviation moves, several days in a row” 
To provide some context, assuming a normal distribution, a 7.26-sigma daily loss 
would be expected to occur once every 13.7 billion or so years. That is roughly the 
estimated age of the universe. 
A 25-sigma event would be expected to occur once every 6 x 10124 lives of the 
universe. That is quite a lot of human histories. When I tried to calculate the 
probability of a 25-sigma event occurring on several successive days, the lights 
visibly dimmed over London and, in a scene reminiscent of that Little Britain sketch, 
the computer said “No”. Suffice to say, time is very unlikely to tell whether Mr 
Viniar’s empirical observation proves correct. 
Fortunately, there is a simpler explanation – the model was wrong. Of course, all 
models are wrong. The only model that is not wrong is reality and reality is not, by 
definition, a model. But risk management models have during this crisis proved 
themselves wrong in a more fundamental sense. They failed Keynes’ test – that it is 
better to be roughly right than precisely wrong. With hindsight, these models were 
both very precise and very wrong. 
For that reason, 2008 might well be remembered as the year stress-testing failed. 
Failed those institutions who invested in it in the hope it would transform their 
3
management of risk. Failed the authorities who had relied – perhaps over-relied – on 
the signal it provided about financial firms’ risk management capabilities. And, 
perhaps most important of all, failed the financial system as a whole by contributing, 
first, to the decade of credit boom and, latterly, the credit bust. 
That is a stark conclusion. But it is a conclusion which is hard to escape. When 
tested against real stress, large parts of the financial system seized-up and a number of 
financial institutions failed. Against that backdrop, now is as good a time as any for 
candour about what went wrong. That is the purpose of my comments today: to 
diagnose some of market failures or frictions in stress-testing practices highlighted by 
the crisis; and, more speculatively, to suggest some practical ways in which stresstesting might deliver answers which are “roughly right”. 
The Golden Decade 
To understand the recent failures in risk management, some history is instructive. 
Prior to the current financial crisis, the previous two low tide marks for the financial 
system and risk management were the stock market crash of October 1987 and the 
failure of the hedge fund LTCM in September 1998. Both prompted a sea-change in 
risk management practices and technologies. 
The October 1987 crash in many respects marked the birth of Value at Risk (VaR) as 
a key risk management tool in financial firms. By 1989, Dennis Weatherstone, JP 
Morgan’s then-chairman, called for a “4:15 Report”, which combined all of the firm’s 
data on market risk in one place. That report should contain information sufficient to 
answer the question “How much could JPM lose if tomorrow turns out to be a 
relatively bad day?” 
With this as the top-down edict, it is perhaps unsurprising that JP Morgan were an 
early-developer and early-adopter of VaR. By 1996, they had published their 
methodology and the detail of the parameterisation of their risk models. In 1998 
RiskMetrics Group, an independent for-profit business, spun off the JP Morgan 
methodology and began offering consultancy services to the risk management 
community. And from 1997 onwards, VaR came to take a degree of prominence 
4
within the regulatory community, with first the US SEC and subsequently the Basel 
Committee on Banking Supervision (BCBS) giving further impetus to VaR, including 
through the design and implementation of Basel II. 
By 2006, when Philippe Jorion published his famous textbook on VaR, relatively few 
would have disputed the claim in its title - “Value at Risk: The New Benchmark for 
Managing Financial Risk”. The message was clear: the technological frontier of risk 
management had been shifted outwards decisively. A cursory search suggests that 
there have been more than 200 books published on VaR since the October 1987 crash, 
or roughly one a month. 
The date of birth of stress-testing is harder to trace. Early mention is made of it in a 
technical note by RiskMetrics in 1996. But it is clear that stress-testing was given 
considerable impetus by the failure of LTCM more than a decade after the October 
1987 crash. Unlike VaR, which had private sector origins, the official sector appears 
to have been at least as much a driver behind the adoption of stress-testing. By 2001, 
under the auspices of its Financial Sector Assessment Programme (FSAP), the IMF 
was publishing details of its stress-testing methodology and experience. Today, the 
same cursory search reveals over 250 articles on stress-testing in the past ten years, or 
more than one a fortnight. We were experiencing a second wave of technological 
revolution in risk management. 
This technological transformation contributed to what was, with hindsight, an 
extraordinary period of growth and success for the financial system and financial 
markets – a Golden Decade. Between October 1998 and June 2007, banks’ share 
prices increased almost 60% and their balance sheets rose more than threefold. In 
some markets growth was little short of explosive, with the rise in volumes 
outstanding in the CDS market making Moore’s Law look positively sluggish. 
And why was this credit boom not destined to end in bust? Because this time was 
different. At the same time as returns were being boosted by bigger balance sheets 
and financed by higher leverage, risk was being held in check by a shift in the 
technological frontier of risk management. A new era had dawned, one with 
simultaneously higher return and lower risk. This miracle came care of a compelling 
5
combination of cavalier risk-takers and roundhead risk-managers. Or so ran the 
rhetoric. 
With hindsight, this Golden Decade and its aftermath has all the hallmarks of, in 
Charles Kindleberger’s words, Manias, Panics and Crashes. Enthusiasm about return 
gave way to hubris and a collective blind eye was turned to the resulting risk. This 
was a latter-day version of the Hans Christian Andersen fairy-tale, “The Emperor’s 
New Clothes”. In a classic collective delusion, the Emperor’s new clothes, you will 
recall, were admired by all. Conferences like this one became catwalks for banks and 
the authorities alike, parading their new garments through the streets in all their 
finery. Risk modelling became high fashion for the pointy-heads, haute-couture for 
the anoraks. 
The past two years have rather changed all that. The sub-prime market has played the 
role of the child in the fairytale, naively but honestly shifting everyone’s perceptions 
about how threadbare the financial system had become. The madness of crowds, as 
Charles Mackay so vividly put it, became visible to all. The resulting unravelling of 
the Golden Decade has been little short of remarkable. 
Asset prices have collapsed – for example, world equity prices have lost more than 
three-quarters of their gains during the Golden Decade. Prices of banks’ shares have 
fared even worse, losing almost 60% of their value and are now lower than at the start 
of the Golden Decade. In the face of these falls, risk management systems across 
virtually all institutions have been found badly wanting. A survey of 500 risk 
managers by KPMG in October last year found that 92% intended to review their risk 
management practices. 
Estimated losses within the financial sector since the start of the crisis lie anywhere 
between a large number and an unthinkably large one. Today, managers of risk – the 
authorities just as much as banks – find themselves struggling to preserve their 
dignity, with risk management systems a combination of sack-cloth and fig-leaf. This 
year, stress-testing conferences like this one are more doghouse than catwalk. 
6
Diagnosing the Market Failures 
So what were the failures, specifically of stress-testing and other risk management 
tools, that contributed to this credit boom and subsequent bust? It is useful to try and 
identify the micro-economic friction – the market failure – that was the root cause of 
these risk management problems. Doing so better enables both financial institutions 
and the authorities to pinpoint what needs to change and how. These market failures 
fall roughly into three categories: 
• disaster myopia; 
• network externalities; and 
• misaligned incentives. 
All three have impeccable microeconomic credentials and potentially disastrous 
macroeconomic consequences. 
Disaster Myopia 
In a nutshell, disaster myopia refers to agents’ propensity to underestimate the 
probability of adverse outcomes, in particular small probability events from the 
distant past. That makes it sound like a rather unworthy informational failure. In fact, 
it is well-established in cognitive psychology that economic agents have a tendency to 
base decision rules around rough heuristics or rules of thumb.1
 The longer the period 
since an event occurred, the lower the subjective probability attached to it by agents 
(the so-called “availability heuristic”). And below a certain bound, this subjective 
probability will effectively be set at zero (the “threshold heuristic”). 
If the period of stability is sufficiently long – a Golden Decade perhaps? - this 
subjective approach to evaluating probabilities looks increasingly like a fully-rational, 
Bayesian approach to updating probabilities. As time passes, convincing the crowds 
that you are not naked becomes progressively easier. It is perhaps no coincidence that 
the last three truly systemic crises – October 1987, August 1998, and the credit crunch 
 
1
 For example, Kahneman, Slovic and Tversky (1982). 
7
which commenced in 2007 – were roughly separated by a decade. Perhaps ten years 
is the threshold heuristic for risk managers. 
Models of disaster myopia have been used to explain a number of phenomena, 
including the tendency for drivers to slow down having witnessed an accident and 
then speed up once the accident has become more distant in their memory, and for 
people to under-insure against low frequency natural hazards such as earthquakes and 
floods. In the context of financial crises, disaster myopia has been used to explain the 
LDC debt crisis, the US savings and loans debacle and various commercial property 
crises.2
 The credit crunch of the past 18 months is but the latest in a long line of 
myopia-induced disasters. 
Such disaster myopia is not of course confined to the private sector. The official 
sector is just as likely to succumb to cognitive biases borne of long periods of 
stability. With hindsight, the stress-tests required by the authorities over the past few 
years were too heavily influenced by behaviour during the Golden Decade. Many risk 
management models developed within the private sector during the Golden Decade 
were, in effect, pre-programmed to induce disaster myopia. These models were often 
data hungry. Improvements in data and IT technology were able to feed these beasts 
with vast, high-frequency datasets. This provided, in the statistical jargon, ample 
degrees of freedom for modellers, enabling them to devise risk frameworks which, on 
the face of it, were very precisely calibrated in-sample. 
And there’s the rub. The sample in question was, with hindsight, most unusual from a 
macroeconomic perspective. The distribution of outcomes for both macroeconomic 
and financial variables during the Golden Decade differed very materially from 
historical distributions. Charts 1-8 illustrate this small sample problem. They look at 
the distribution of a set of macroeconomic and financial variables, comparing the 
Golden Decade with a sample stretching back in some cases to the 17th century. Even 
visually, these distributions plainly suggest that the Golden Era distributions have a 
much smaller variance and slimmer tails. More formally, Table 1 looks at the first 
four moments of these variables, comparing the Golden Era with the full sample. 
 
2
 For example, Guttentag and Herring (1986a) and Herring (1999). 
8
For the macro time-series, the differences in variability are striking. The long-run 
standard deviation of UK GDP growth has on average been 4 times greater than 
during the Golden Decade; for unemployment 5 times greater; for inflation 7 times 
greater; and for earnings 12 times greater. Put differently, as part of the Basel II 
regime the FSA require banks to simulate the effects of a 1-in-25 year stress. In 2007, 
the worst such GDP growth outcome over the preceding 25 year period was -1.4%; 
the average 1-in-25 year stress over the full sample is -3.8%. 
For financial time-series, small sample problems are even more acute, especially for 
events in the tail of the distribution. Measures of kurtosis – the fatness of the tails – 
of UK house price inflation are 6 times larger over the full sample than over the 
Golden Decade; for UK bond yields 10 times larger; and for UK equity returns 16 
times larger. To bring these stylised facts to life, consider the distribution of equity 
returns in Chart 7. If we assumed the Golden Era distribution was the true one, the 
three worst monthly returns in history – the bursting of the South Sea bubble in 
September and October 1720, and Black Monday in October 1987 – would have been 
respectively 12.7, 6.9 and 6.5-sigma events. All three would have appeared to be 
once in a lifetime – of the universe – events. 
Underestimation of risk, whether variances or tail outcomes, has consequences for the 
risks facing both individual firms and for the system as a whole. As an example of 
the former, Chart 9 plots some unconditional 90th percentile VaRs for a selection of 
UK banks, based on their equity returns up until end-July 2007 and then extended to 
include the present crisis.3
 These unconditional VaRs for UK banks increase, on 
average, by almost 60% once the sample is extended; and for some banks these risk 
measures more than double. 
For the system as a whole, one way of illustrating the consequences of 
underestimating risk is to translate it into “fair value” insurance premia. For example, 
consider a financial firm offering insurance against moves in future equity prices by 
writing put options in mid-2007. Pricing of this insurance is assumed to be based on 
 
3
 The banks themselves have been anonymised to protect the innocent. 
9
the distribution of equity returns during the Golden Decade. If the “true” distribution 
of returns were its long-run average, by how much would this insurance have been 
under-priced in 2007? 
Chart 10 provides some answers for a selection of strike prices for the option. The 
degree of under-pricing of risk is large and is larger for options designed to protect 
against tail risks (lower strike prices). For at-the-money options on UK equities, the 
insurance premium would have been under-priced by around 45%; for options well 
out-of-the-money – say, 50% below equity prices at the time – the mis-pricing would 
have been nearer 90%. This is risk under-pricing on a dramatic scale. 
These examples are no more than illustrative. But they help illustrate that the 
quantitative consequences of disaster myopia were material ahead of crisis and may 
have contributed importantly to the price of risk being set too low. And that, in turn, 
helped sow the seeds of the credit boom. 
Network Externalities 
Any asset portfolio is, in essence, a financial network. So the balance sheet of a large 
financial institution is a network, with nodes defined by the assets and links defined 
by the correlations among those assets. The financial system is similarly a network, 
with nodes defined by the financial institutions and links defined by the financial 
interconnections between these institutions. 
Evaluating risk within these networks is a complex science; indeed, it is the science 
of complexity.4
 When assessing nodal risk, it is not enough to know your 
counterparty; you need to know your counterparty’s counterparty too. In other 
words, there are network externalities.5
 In financial networks, these externalities are 
often referred to as contagion or spillovers. There have been many examples of such 
spillover during this crisis, with Lehman Brothers’ failure a particularly painful one. 
 
4
 Gell-Mann (1994). 
5
 Morris and Shin (2008). 
10
That is why there have been recent calls to calibrate regulatory requirements to these 
risk externalities.6
 
These network uncertainties make it tremendously difficult for risk managers to 
identify and price, and hence manage, balance sheet risk. Consider first evaluating 
risks across the portfolio of an individual firm. There is evidence that firms find 
aggregation of risks across their balance sheet extremely difficult to execute.7
 To the 
extent this is done at all, it requires firms to make assumptions about correlations 
between asset prices. But at times of stress, asset correlation matrices are unlikely to 
be stable and correlations invariably head towards one. So pre-crisis measures of 
balance sheet risk are likely to be significant under-estimates. Chart 11 looks at asset 
correlations over the past few years. Note their instability and abrupt upward shift 
during crisis. 
These risk externalities will tend to be amplified when aggregated across the network 
as a whole. This generates further underestimation of institutional risks. Consider 
again those 90th percentile VaR measures for UK banks. But instead of looking at 
unconditional VaR, consider now conditional VaRs (CoVaRs) – that is to say, VaRs 
conditional on other institutions in the network simultaneously facing stress.8
 As 
Chart 12 shows, this raises the median risk facing UK banks by around 40%; and for 
some banks, risk estimates almost double. For a financial firm leveraged 20+ times, 
those risk revisions could be the difference between success and failure. 
Network risk externalities of this type impose formidable informational demands on 
banks. For example, understanding the full consequences of Lehman’s failure would 
have required information on the entire topology of the financial network. This is 
unrealistic even for the authorities, much less an individual firm. Absent that 
knowledge, the financial system was seized by network uncertainty. If this 
informational failure is not easily rectified by the actions of individual firms, there is a 
 
6
 See, for example, Brunnermeier, Crockett, Goodhart, Persaud and Shin (2009) and NYU Stern School 
of Business (2008). 
7
 For example, a survey of stress-testing by the CGFS in 2005 found that only a small minority of 
firms considered the effects of multiple shocks on their balance sheet. 
8
 Following the methodology of Adrian and Brunnermeier (2008). 
11
case for the authorities attempting to provide that missing informational public good, 
however difficult that might be in practice. 
Misaligned Incentives 
Finally, and perhaps most contentiously, incentives and governance. Principal-agent 
problems crop up in all aspects of economics. But it is questionable whether there is 
any event in recent history where these agency problems have been exposed so 
frequently and extensively as during the current financial crisis. It is easy to see why. 
Financial innovation lengthened the informational chain from ultimate borrower to 
end-investor. The resulting game of Chinese whispers meant that, by the time 
information had reached investors at the end of the chain, it was seriously impaired. 
In the narrower context of stress-testing, these principal-agent problems appear to 
have operated at two distinct levels. First, internally, through the relationship 
between risk managers and the risk-takers within financial firms; and second, 
externally, in the relationship between financial firms and the authorities. The former 
principal-agent problem has been rather less discussed, but appears to have been 
potent during the credit boom. 
Decision-making within firms is an arm-wrestle between risk and return, between risk 
managers and risk-takers. When returns are high and risks appear low, this armwrestle can become one-sided. Power switches from back to front offices and risk 
managers become the poor relation.9
 And what is true within individual firms is then 
amplified by behaviour across the system as a whole, as firms conduct their own armwrestle with competitors for higher returns on equity. The Bank’s market intelligence 
suggested this “keeping up with the Jones’s” was a potent force within financial firms 
during the upswing. 
The second principal-agent problem, between firms and the authorities, is different in 
kind but similar in consequence. It arises because of a familiar public policy problem 
– time-consistency. If the ex-post failure of an institution risks destabilising the 
 
9
 The KPMG survey of risk managers in October 2008 pointed to a similar conclusion, as does the 
FSA consultation paper on stress-testing published in December 2008. 
12
system, any ex-ante pre-commitment by the authorities to let it fail will lack 
credibility. This is simply a variant of the old adage that if you owe the bank a small 
amount it is your problem, a large amount it is theirs. These days, if a bank owes a 
small amount it is their problem, a large amount it is the authorities. 
This time-consistency problem weakens incentives for banks to consider for 
themselves large-scale risks to their balance sheet which might induce failure. The 
safety net becomes a comfort blanket, the backstop a balm. And the greater the risk 
these institutions themselves pose in the event of failure, the weaker the incentives to 
manage risk. These are topsy-turvy incentives from a public policy perspective, with 
risk management discipline weakest among those whom society would wish it to be 
strongest. 
And the evidence? A few years ago, ahead of the present crisis, the Bank of England 
and the FSA commenced a series of seminars with financial firms, exploring their 
stress-testing practices. The first meeting of that group sticks in my mind. We had 
asked firms to tell us the sorts of stress which they routinely used for their stress-tests. 
A quick survey suggested these were very modest stresses. We asked why. Perhaps 
disaster myopia – disappointing, but perhaps unsurprising? Or network externalities – 
we understood how difficult these were to capture? 
No. There was a much simpler explanation according to one of those present. There 
was absolutely no incentive for individuals or teams to run severe stress tests and 
show these to management. First, because if there were such a severe shock, they 
would very likely lose their bonus and possibly their jobs. Second, because in that 
event the authorities would have to step-in anyway to save a bank and others suffering 
a similar plight. 
All of the other assembled bankers began subjecting their shoes to intense scrutiny. 
The unspoken words had been spoken. The officials in the room were aghast. Did 
banks not understand that the official sector would not underwrite banks mismanaging their risks? 
Yet history now tells us that the unnamed banker was spot-on. His was a brilliant 
13
articulation of the internal and external incentive problem within banks. When the big 
one came, his bonus went and the government duly rode to the rescue. The timeconsistency problem, and its associated negative consequences for risk management, 
was real ahead of crisis. Events since will have done nothing to lessen this problem, 
as successively larger waves of institutions have been supported by the authorities. 
More recently, the Bank and FSA have been engaged in some practical work with 
banks, running stress-tests through their models on common scenarios. When asked 
to assess the consequences of a macro stress-test, the like of which we are currently 
experiencing, some banks have found it problematic. In defence, they have suggested 
that such an exercise was only conducted annually as part of their Basel II 
preparations and as such new stress tests would take months to conduct. 
This too was revealing. If even the most obvious stress-test took many weeks to 
prepare and assess, how could these tests meaningfully be used to manage risk? The 
short answer, I think, is that stress-testing was not being meaningfully used to manage 
risk. Rather, it was being used to manage regulation. Stress-testing was not so much 
regulatory arbitrage as regulatory camouflage. 
Prescribing Some Solutions 
Each of these market failures has been exposed by events over the past 18 months. 
When risks materialised outside of calibrated distributions, risk models provided little 
guidance in identifying, pricing and hence managing them. This failure is not of 
purely academic interest. The breakdown of risk models is itself likely to have 
contributed importantly to crisis dynamics. Why? 
First, the potential losses arising from under-pricing of risk are large. Consider the 
earlier example of a disaster-myopic writer of deep out-of-the-money put options on 
UK equities, priced using distributions drawn from the Golden Decade. Let’s say 
that, in June 2007, a five-year put had been written on the FTSE-100 with a strike 
price 40% below the prevailing market price. Today, that put would be at-the-money. 
14
Hedging that position would crystallise a loss roughly 60 times the income received 
from having written the option in the first place.10 
 
This example is far from hypothetical. These are essentially the same trades 
undertaken by a number of insurance companies and other investors ahead of crisis. 
In the go-go years, the insurance premia from them yielded a steady income stream. 
But when risk shifted, many insurers have suffered large-scale losses as premia have 
adjusted and investors have scrambled to hedge. The large US insurer AIG has so far 
suffered gross losses totalling over $60bn on CDS contracts alone. Losses by the 
monoline insurers have also totalled in excess of $60bn. 
Second, the breakdown of these models had the consequence of turning risk into 
uncertainty, in the Knightian sense.11 Once the models broke down, how were assets 
to be priced? Practitioners have a devil of a job pricing assets in the face of such 
uncertainty. So too do academics, though some attempts have been made.12 The 
theory of asset pricing under Knightian uncertainty throws up at least two striking 
results. First, in the face of such uncertainty, asset prices are not precisely determined 
but instead lie in a range. This indeterminacy in prices is larger the greater is 
uncertainty and the greater agents’ aversion to it. Second, asset prices exhibit a 
downward bias relative to fundamentals. Uncertainty gives the appearance of 
“pessimistic” expectations. 
Both of these theoretical predictions match pretty closely the moments of many asset 
prices in the world today. Many appear to lack a clear compass relative to 
fundamentals. Most are excessively volatile. Among investors, pessimism is the new 
optimism, with talk of a lost decade in succession to the Golden one. Risk models – 
or the failure thereof – have played their part in generating these foggy outcomes. 
That is the diagnosis. What of the prescription? In their recent consultation paper, 
the Financial Services Authority has outlined some very good proposals for 
 
10 Roughly half of that loss represents under-estimation of the distribution of returns back in 2007. 11 Knight (1921). 
12 For example, Epstein and Wang (1994). 
15
improving stress-testing practices among financial institutions.13 Based on my 
reading of the identified failures in stress-testing, let me put forward a complementary 
“five-point plan”. 
• First, setting the stress scenario. The key elements here are devising a 
multi-factor risk scenario that is sufficiently extreme to constitute a tail event. 
Arguably, designing such a scenario is better delegated to the authorities than 
to individual firms, in part because they ought to be more immune to disaster 
myopia. In its Financial Stability Report (FSR), the Bank describes such 
stress scenarios. In future, the Bank aims to be able to offer through the FSR 
some greater clarity to financial firms about the sorts of vulnerability scenario 
it thinks they could use as one (and only one) input to their stress-testing 
machinery. This might include both solvency and liquidity-type scenarios. As 
the FSA have proposed, banks should also test to destruction their balance 
sheets through “reverse” stress tests, in order to identify potential areas of 
balance sheet weakness. 
• Second, regular evaluation of common stress scenarios. Having banks 
conduct regular evaluations of their positions relative to a set of common 
scenarios (provided by the authorities) would be an improvement on current 
practices in several respects. First, it would allow some degree of 
benchmarking of results across institutions; second, it would allow a degree 
of benchmarking, and hence peer review, of models; and third, it would 
hopefully help in ensuring stress-testing exercises form an input to 
management decisions and are not an annual regulatory ritual. Comparing 
these bottom-up exercises with top-down evaluations conducted by the 
authorities – the like of which have appeared in recent Bank FSRs - can also 
help in benchmarking results and models. 
• Third, an assessment of the second-round effects of stress. The results of 
these common stress evaluations should be the starting point, not the end 
point. These common stress tests need to be made dynamic, so that the second 
 
13 FSA (2008). See also Counterparty Risk Management Group (2008) for other useful suggestions. 
16
and subsequent round interactions, and their consequences for system-wide 
risk, can be evaluated. This calls for an iterative approach to stress-testing in 
which banks’ first-round results and management actions influence secondround stresses facing firms – for example, the effects of asset sales and 
liquidity hoarding. In effect, what we would then have is a hybrid stress testcum-war game. This will better enable firms to assess the spillover and 
contagion consequences of their own and others’ actions, so helping 
internalise to some degree the network externality problems which have been 
prevalent through this crisis. This dynamic, collective approach to stresstesting has already been attempted in one or two countries; it would be 
desirable if it became standard practice more widely. The recent Geneva 
report on financial regulation proposes greater use of such systemic stress 
testing. From the authorities’ side, the Bank is developing a framework which 
will enable us to capture such network effects – for example, the effects of 
liquidity contagion and asset price disposals – on other firms in the network. 
The results from that framework could be used alongside firm-specific results 
to gauge network risks.14
• Fourth, translation of results into firms’ liquidity and capital planning. 
The results from these exercises need to influence management outcomes if 
they are to be useful; the internal incentive problem needs to be overcome. 
So there should be a presumption that the results of these dynamic stress tests 
are taken, for example, to banks’ risk committees. And banks’ executives 
should periodically be asked how they intend to respond to these findings, 
including how effective their defensive responses are likely to be when the 
stress is system-wide and how the results affect liquidity and capital planning 
decisions. 
• Fifth, transparency to regulators and financial markets. The bank-specific 
results ought to inform regulatory decisions about firms’ capital and liquidity 
buffers. Indeed, there is a case for having these results set out regularly in 
firms’ public reports. This would hopefully help exert a degree of market 
 
14 Aikman, Alessandri, Eklund, Gai, Kapadia, Martin, Mora, Sterne and Willison (2008). 
17
discipline over management choices, as has been proposed by the Treasury 
Committee.15 Existing disclosures by banks are a patchwork of different 
practices which make cross-firm comparisons of risk nigh on impossible. 
Having a standardised, published set of such stress-testing results would help 
improve financial markets’ understanding and hence pricing of bank-specific 
risk – a particular problem during this crisis – thereby helping address the 
external incentive problem. 
Working alongside the other Tripartite authorities, the Bank would be interested in 
exploring with financial firms the feasibility and desirability of putting this five-point 
plan into practice. This plan is about making stress-testing more robust but also more 
relevant. It is about providing that missing informational public good. In the armwrestle with management, it is about supplying power to the elbow of risk-managers. 
Conclusion 
Let me conclude. As after the previous two episodes of systemic failure, in October 
1987 and August 1998, a third wave of technological transformation in the standards 
of risk management is now needed as a matter of priority. Firms themselves admit as 
much. That calls for a new agenda. I have outlined some elements of such an agenda, 
to address some of the failures exposed by the crisis. These measures involve a 
greater degree of engagement both between risk managers and senior management 
within firms, and between financial firms and the authorities. They would also 
involve much greater transparency to the wider world about risk metrics and 
accompanying management actions. These measures would not prevent a next time –
nor should they – but they might help make risk management roughly right.